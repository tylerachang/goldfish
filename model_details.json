{"hmn_latn_5mb": {"language_name": "Hmong", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hmn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cqd", "hea", "hma", "hmc", "hmd", "hme", "hmg", "hmh", "hmi", "hmj", "hml", "hmm", "hmp", "hmq", "hms", "hmw", "hmy", "hmz", "hnj", "hrm", "huj", "mmr", "muq", "mww", "sfm"], "language_byte_premium": 1.18983824, "dataset_raw_mb": 5.95127299809694, "dataset_scaled_mb": 5.001749648, "dataset_tokens": 1492480, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001749648, "tokenizer_training_raw_mb": 5.95127299809694, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "46079", "model_parameters": "36989952", "train_compute_flops": 1128834379284480.0, "train_compute_hours": 0.10672615949598721, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4"], "dataset_readme_str": "* 54.54885%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 45.45115%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4"]}, "uzb_latn_5mb": {"language_name": "Uzbek", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "uzb", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.229496706, "dataset_raw_mb": 6.148287998301212, "dataset_scaled_mb": 5.000654307, "dataset_tokens": 1364992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000654307, "tokenizer_training_raw_mb": 6.148287998301212, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1032101497405440.0, "train_compute_hours": 0.09758050520924161, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 76.42069%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.51914%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.42511%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.63507%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bul_cyrl_5mb": {"language_name": "Bulgarian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bul", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.812315686, "dataset_raw_mb": 9.064979004783739, "dataset_scaled_mb": 5.00187637, "dataset_tokens": 1181696, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00187637, "tokenizer_training_raw_mb": 9.064979004783739, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 894110898585600.0, "train_compute_hours": 0.08453412132082037, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tlh_latn_5mb": {"language_name": "Klingon", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tlh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.143065822, "dataset_raw_mb": 5.7154570007764285, "dataset_scaled_mb": 5.000111884, "dataset_tokens": 1486336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000111884, "tokenizer_training_raw_mb": 5.7154570007764285, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41713", "model_parameters": "34892800", "train_compute_flops": 1124185622446080.0, "train_compute_hours": 0.1062866406676294, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 93.86315%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.89344%: [Tatoeba](https://tatoeba.org/en/)\n* 1.24341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "uzn_latn_5mb": {"language_name": "Northern Uzbek", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "uzn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 1.645764497, "dataset_raw_mb": 8.229647225316617, "dataset_scaled_mb": 5.000501129, "dataset_tokens": 1856512, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000501129, "tokenizer_training_raw_mb": 8.229647225316617, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1404002044477440.0, "train_compute_hours": 0.13274201147786707, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "kor_hang_5mb": {"language_name": "Korean", "language_script": "Hangul", "dataset_category": "5mb", "language_iso6393": "kor", "language_iso15924": "hang", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.293311204, "dataset_raw_mb": 6.469145000114325, "dataset_scaled_mb": 5.002001823, "dataset_tokens": 1224192, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002001823, "tokenizer_training_raw_mb": 6.469145000114325, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 925838664007680.0, "train_compute_hours": 0.08753383732436248, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mad_latn_5mb": {"language_name": "Madurese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mad", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.285380683, "dataset_raw_mb": 6.427054997373185, "dataset_scaled_mb": 5.000117928, "dataset_tokens": 1423872, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000117928, "tokenizer_training_raw_mb": 6.427054997373185, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1076225947729920.0, "train_compute_hours": 0.1017522714217379, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 45.95275%: [IndoNLP](https://huggingface.co/indonlp)\n* 25.66820%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13860%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.75067%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 8.48839%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/)\n* 0.00140%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["IndoNLP", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data", "Tatoeba"], "dataset_links": ["https://huggingface.co/indonlp", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/"]}, "hye_armn_5mb": {"language_name": "Armenian", "language_script": "Armenian", "dataset_category": "5mb", "language_iso6393": "hye", "language_iso15924": "armn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.724226492, "dataset_raw_mb": 8.624681990589414, "dataset_scaled_mb": 5.002058622, "dataset_tokens": 1075200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002058622, "tokenizer_training_raw_mb": 8.624681990589414, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 813532446720000.0, "train_compute_hours": 0.07691579496261819, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 52.58958%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 24.27630%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.13412%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "uig_latn_5mb": {"language_name": "Uighur", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "uig", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.191355626, "dataset_raw_mb": 5.9577166906843635, "dataset_scaled_mb": 5.000787809, "dataset_tokens": 1075712, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000787809, "tokenizer_training_raw_mb": 5.9577166906843635, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 812796393553920.0, "train_compute_hours": 0.07684620448146154, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 97.79454%: [eBible](https://ebible.org/find/)\n* 2.20546%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["eBible", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "WikiMatrix"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "afb_arab_5mb": {"language_name": "Gulf Arabic", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "afb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.374141124, "dataset_raw_mb": 6.871620630343368, "dataset_scaled_mb": 5.000665878, "dataset_tokens": 1185280, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000665878, "tokenizer_training_raw_mb": 6.871620630343368, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 896396537364480.0, "train_compute_hours": 0.0847502180780963, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.98461%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/)\n* 0.01539%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "ADD", "AraBench", "DART", "Habibi", "QADI", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/"]}, "pes_arab_5mb": {"language_name": "Iranian Persian", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "pes", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.59732627, "dataset_raw_mb": 7.986632004903771, "dataset_scaled_mb": 5.00000041, "dataset_tokens": 1141760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00000041, "tokenizer_training_raw_mb": 7.986632004903771, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "35490", "model_parameters": "31747072", "train_compute_flops": 863196665610240.0, "train_compute_hours": 0.08161132111224088, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ful_latn_5mb": {"language_name": "Fulah", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ful", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ffm", "fub", "fuc", "fue", "fuf", "fuh", "fui", "fuq", "fuv"], "language_byte_premium": 1.258383084, "dataset_raw_mb": 6.293326998640647, "dataset_scaled_mb": 5.00112174, "dataset_tokens": 1678336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00112174, "tokenizer_training_raw_mb": 6.293326998640647, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1269188096163840.0, "train_compute_hours": 0.11999596545549034, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "cawoylel/FulaSpeechCorpora", "allenai/MADLAD-400"], "dataset_readme_str": "* 66.44435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.91335%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.61833%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.98971%: [Fula Speech Corpora](https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora)\n* 0.75836%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.27589%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Fula Speech Corpora", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "srp_latn_5mb": {"language_name": "Serbian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "srp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.826257686, "dataset_raw_mb": 4.132323995283018, "dataset_scaled_mb": 5.00125332, "dataset_tokens": 1075200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00125332, "tokenizer_training_raw_mb": 4.132323995283018, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 813532446720000.0, "train_compute_hours": 0.07691579496261819, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.93765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.75254%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30981%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "hun_latn_5mb": {"language_name": "Hungarian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01995535, "dataset_raw_mb": 5.101951005138881, "dataset_scaled_mb": 5.002131716, "dataset_tokens": 1011200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002131716, "tokenizer_training_raw_mb": 5.101951005138881, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 764681760276480.0, "train_compute_hours": 0.07229718460795811, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "plt_latn_5mb": {"language_name": "Plateau Malagasy", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "plt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.151182211, "dataset_raw_mb": 5.757179054577043, "dataset_scaled_mb": 5.001101476, "dataset_tokens": 1370624, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001101476, "tokenizer_training_raw_mb": 5.757179054577043, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1035936721797120.0, "train_compute_hours": 0.09794310824263681, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 95.80021%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.19979%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TeDDi](https://github.com/MorphDiv/TeDDi_sample)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "TeDDi"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/MorphDiv/TeDDi_sample"]}, "mam_latn_5mb": {"language_name": "Mam", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.34382757, "dataset_raw_mb": 6.720509000272154, "dataset_scaled_mb": 5.001020332, "dataset_tokens": 1960448, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001020332, "tokenizer_training_raw_mb": 6.720509000272154, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "37222", "model_parameters": "32795648", "train_compute_flops": 1482217378283520.0, "train_compute_hours": 0.14013691576498735, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 47.02182%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.57200%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm)\n* 17.40618%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://ebible.org/find/"]}, "lfn_latn_5mb": {"language_name": "Lingua Franca Nova", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lfn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.301456056, "dataset_raw_mb": 6.507462099918302, "dataset_scaled_mb": 5.000139705, "dataset_tokens": 1562112, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000139705, "tokenizer_training_raw_mb": 6.507462099918302, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1181520290119680.0, "train_compute_hours": 0.11170737288404248, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 50.98661%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 49.01339%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "hif_latn_5mb": {"language_name": "Fiji Hindi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hif", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.281123121, "dataset_raw_mb": 6.406133999940805, "dataset_scaled_mb": 5.000404641, "dataset_tokens": 1656320, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000404641, "tokenizer_training_raw_mb": 6.406133999940805, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1252801228308480.0, "train_compute_hours": 0.11844666158552902, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 74.06143%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.82293%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.11449%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00116%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "lat_latn_5mb": {"language_name": "Latin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.87538342, "dataset_raw_mb": 4.378511999569768, "dataset_scaled_mb": 5.001821944, "dataset_tokens": 996352, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001821944, "tokenizer_training_raw_mb": 4.378511999569768, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 753176087101440.0, "train_compute_hours": 0.07120937550777251, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.83255%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.81393%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.54910%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.52148%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.21200%: [eBible](https://ebible.org/find/)\n* 0.07094%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "shn_mymr_5mb": {"language_name": "Shan", "language_script": "Burmese", "dataset_category": "5mb", "language_iso6393": "shn", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.819448606, "dataset_raw_mb": 14.099282001662335, "dataset_scaled_mb": 5.000723181, "dataset_tokens": 991744, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000723181, "tokenizer_training_raw_mb": 14.099282001662335, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 749263383429120.0, "train_compute_hours": 0.07083944716057135, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 67.30922%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 32.69078%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "bar_latn_5mb": {"language_name": "Bavarian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bar", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129561954, "dataset_raw_mb": 5.647988680188332, "dataset_scaled_mb": 5.000158389, "dataset_tokens": 1490944, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000158389, "tokenizer_training_raw_mb": 5.647988680188332, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1128098326118400.0, "train_compute_hours": 0.10665656901483056, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 64.07962%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 35.91237%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00802%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "rmc_latn_5mb": {"language_name": "Carpathian Romani", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "rmc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["rom"], "language_code_individuals": [], "language_byte_premium": 1.024618866, "dataset_raw_mb": 5.123123999888502, "dataset_scaled_mb": 5.000028957, "dataset_tokens": 1112576, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000028957, "tokenizer_training_raw_mb": 5.123123999888502, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "29661", "model_parameters": "28601344", "train_compute_flops": 840688934584320.0, "train_compute_hours": 0.07948331745160844, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 54.85237%: [eBible](https://ebible.org/find/)\n* 45.14763%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["eBible", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "oss_cyrl_5mb": {"language_name": "Ossetian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "oss", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.846676453, "dataset_raw_mb": 9.235655001104208, "dataset_scaled_mb": 5.001230717, "dataset_tokens": 1863680, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001230717, "tokenizer_training_raw_mb": 9.235655001104208, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1410122907648000.0, "train_compute_hours": 0.1333207112685382, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 71.94447%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.35828%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.22885%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.45171%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.01669%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tur_latn_5mb": {"language_name": "Turkish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.044489573, "dataset_raw_mb": 5.2228220001205585, "dataset_scaled_mb": 5.000358199, "dataset_tokens": 991232, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000358199, "tokenizer_training_raw_mb": 5.2228220001205585, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 749999436595200.0, "train_compute_hours": 0.07090903764172801, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tbz_latn_5mb": {"language_name": "Ditammari", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tbz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.330093026, "dataset_raw_mb": 6.651780000111317, "dataset_scaled_mb": 5.000988555, "dataset_tokens": 1835008, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000988555, "tokenizer_training_raw_mb": 6.651780000111317, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "31342", "model_parameters": "29649920", "train_compute_flops": 1388428709068800.0, "train_compute_hours": 0.13126962340286838, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 53.71611%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 46.28389%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "gaz_latn_5mb": {"language_name": "West Central Oromo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "gaz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["orm"], "language_code_individuals": [], "language_byte_premium": 1.332217666, "dataset_raw_mb": 6.662594259524123, "dataset_scaled_mb": 5.001130393, "dataset_tokens": 1680384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001130393, "tokenizer_training_raw_mb": 6.662594259524123, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1270737681776640.0, "train_compute_hours": 0.1201424717316096, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 96.08435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.91565%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "udm_cyrl_5mb": {"language_name": "Udmurt", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "udm", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740439775, "dataset_raw_mb": 8.705619001018775, "dataset_scaled_mb": 5.001965093, "dataset_tokens": 1106432, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001965093, "tokenizer_training_raw_mb": 8.705619001018775, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 836040177745920.0, "train_compute_hours": 0.07904379862325063, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.17663%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.77909%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 4.96507%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.07691%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00230%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "iku_cans_5mb": {"language_name": "Inuktitut", "language_script": "Unified Canadian Aboriginal Syllabics", "dataset_category": "5mb", "language_iso6393": "iku", "language_iso15924": "cans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ike", "ikt"], "language_byte_premium": 2.158751414, "dataset_raw_mb": 10.7950139986933, "dataset_scaled_mb": 5.000582248, "dataset_tokens": 978944, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000582248, "tokenizer_training_raw_mb": 10.7950139986933, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 740701922918400.0, "train_compute_hours": 0.07002999998501237, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 50.41059%: [Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0](https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60)\n* 42.46450%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.06461%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.06031%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0", "Glot500", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "srd_latn_5mb": {"language_name": "Sardinian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "srd", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["sdc", "sdn", "src", "sro"], "language_byte_premium": 1.109372772, "dataset_raw_mb": 5.548026464915091, "dataset_scaled_mb": 5.001047984, "dataset_tokens": 1436160, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001047984, "tokenizer_training_raw_mb": 5.548026464915091, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1085523461406720.0, "train_compute_hours": 0.10263130907845354, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 44.15271%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.42496%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.42165%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00068%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "fry_latn_5mb": {"language_name": "Western Frisian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fry", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23134294, "dataset_raw_mb": 6.159053000541573, "dataset_scaled_mb": 5.001898984, "dataset_tokens": 1459200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001898984, "tokenizer_training_raw_mb": 6.159053000541573, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1103382435594240.0, "train_compute_hours": 0.10431979391072815, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.10455%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.78619%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.66588%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.44227%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00110%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zza_latn_5mb": {"language_name": "Zaza", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "zza", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["diq", "kiu"], "language_byte_premium": 1.199780688, "dataset_raw_mb": 5.9994429993721035, "dataset_scaled_mb": 5.000449715, "dataset_tokens": 1659392, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000449715, "tokenizer_training_raw_mb": 5.9994429993721035, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1254428293201920.0, "train_compute_hours": 0.11860049317545426, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 35.78443%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.44703%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 19.35033%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.39981%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01839%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "slk_latn_5mb": {"language_name": "Slovak", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "slk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.041506091, "dataset_raw_mb": 5.208210994868475, "dataset_scaled_mb": 5.000653419, "dataset_tokens": 1126912, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000653419, "tokenizer_training_raw_mb": 5.208210994868475, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 851536033873920.0, "train_compute_hours": 0.08050886138444335, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "chv_cyrl_5mb": {"language_name": "Chuvash", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "chv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.796969484, "dataset_raw_mb": 8.987129997126447, "dataset_scaled_mb": 5.001270237, "dataset_tokens": 1680384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001270237, "tokenizer_training_raw_mb": 8.987129997126447, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1270737681776640.0, "train_compute_hours": 0.1201424717316096, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.68880%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.72870%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.16329%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.06333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.35083%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00505%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "TIL", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kea_latn_5mb": {"language_name": "Kabuverdianu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kea", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.781652324, "dataset_raw_mb": 3.9086358696611647, "dataset_scaled_mb": 5.000478793, "dataset_tokens": 1060864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000478793, "tokenizer_training_raw_mb": 3.9086358696611647, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 802685347430400.0, "train_compute_hours": 0.07589025102978328, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.94529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.05471%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "vls_latn_5mb": {"language_name": "Vlaams", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "vls", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.208907177, "dataset_raw_mb": 6.046328179349033, "dataset_scaled_mb": 5.001482574, "dataset_tokens": 1482240, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001482574, "tokenizer_training_raw_mb": 6.046328179349033, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1121086451220480.0, "train_compute_hours": 0.10599362811539084, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 59.71361%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.28639%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nld_latn_5mb": {"language_name": "Dutch", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nld", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.051605721, "dataset_raw_mb": 5.258426004698754, "dataset_scaled_mb": 5.000377898, "dataset_tokens": 1130496, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000377898, "tokenizer_training_raw_mb": 5.258426004698754, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 855371258265600.0, "train_compute_hours": 0.08087146441783856, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kat_latn_5mb": {"language_name": "Georgian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.200493595, "dataset_raw_mb": 6.002775535456571, "dataset_scaled_mb": 5.000256195, "dataset_tokens": 1290240, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000256195, "tokenizer_training_raw_mb": 6.002775535456571, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 976238936064000.0, "train_compute_hours": 0.09229895395514183, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "fas_arab_5mb": {"language_name": "Persian", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "fas", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pes", "prs"], "language_byte_premium": 1.590775443, "dataset_raw_mb": 7.956011009594346, "dataset_scaled_mb": 5.001341355, "dataset_tokens": 1269248, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001341355, "tokenizer_training_raw_mb": 7.956011009594346, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "37693", "model_parameters": "32795648", "train_compute_flops": 959929547489280.0, "train_compute_hours": 0.09075697539898649, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "cos_latn_5mb": {"language_name": "Corsican", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176702158, "dataset_raw_mb": 5.884453999148789, "dataset_scaled_mb": 5.00080157, "dataset_tokens": 1615360, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00080157, "tokenizer_training_raw_mb": 5.884453999148789, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1221809516052480.0, "train_compute_hours": 0.11551653606314358, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 60.95981%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.95148%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.08866%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00005%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "afr_latn_5mb": {"language_name": "Afrikaans", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "afr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.03722598, "dataset_raw_mb": 5.187721998315809, "dataset_scaled_mb": 5.001534958, "dataset_tokens": 1250816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001534958, "tokenizer_training_raw_mb": 5.187721998315809, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 945983276974080.0, "train_compute_hours": 0.08943841891391302, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.06252%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.34080%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 13.07209%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.39061%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.13398%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "Earthlings", "Mburisano_Covid", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://repo.sadilar.org/handle/20.500.12185/536", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bho_deva_5mb": {"language_name": "Bhojpuri", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "bho", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.515275311, "dataset_raw_mb": 12.578658000256988, "dataset_scaled_mb": 5.000907036, "dataset_tokens": 1354752, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000907036, "tokenizer_training_raw_mb": 12.578658000256988, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1024353569341440.0, "train_compute_hours": 0.09684797382864524, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.96562%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 39.06917%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.96521%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "prs_arab_5mb": {"language_name": "Dari", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "prs", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.66404538, "dataset_raw_mb": 8.321556249276538, "dataset_scaled_mb": 5.000798866, "dataset_tokens": 1207808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000798866, "tokenizer_training_raw_mb": 8.321556249276538, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "23971", "model_parameters": "25455616", "train_compute_flops": 913441979105280.0, "train_compute_hours": 0.0863617871154083, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.28229%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.71771%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tico-19.github.io/"]}, "glk_arab_5mb": {"language_name": "Gilaki", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "glk", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.682310821, "dataset_raw_mb": 8.413202239676671, "dataset_scaled_mb": 5.000979685, "dataset_tokens": 1339392, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000979685, "tokenizer_training_raw_mb": 8.413202239676671, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "38897", "model_parameters": "32795648", "train_compute_flops": 1013428990771200.0, "train_compute_hours": 0.09581510458200437, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 50.16168%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 48.65632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.13696%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "dov_latn_5mb": {"language_name": "Dombe", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "dov", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.989047107, "dataset_raw_mb": 4.947458000006056, "dataset_scaled_mb": 5.002247077, "dataset_tokens": 958464, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002247077, "tokenizer_training_raw_mb": 4.947458000006056, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 725206066790400.0, "train_compute_hours": 0.06856493722381964, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 89.36384%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.63616%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "din_latn_5mb": {"language_name": "Dinka", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "din", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dib", "dik", "dip", "diw", "dks"], "language_byte_premium": 1.241578901, "dataset_raw_mb": 6.209275000439497, "dataset_scaled_mb": 5.001111887, "dataset_tokens": 1803264, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001111887, "tokenizer_training_raw_mb": 6.209275000439497, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1363712818544640.0, "train_compute_hours": 0.12893284829876597, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 73.62398%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.66747%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.00269%: [eBible](https://ebible.org/find/)\n* 2.70586%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "frr_latn_5mb": {"language_name": "Northern Frisian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "frr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.169852216, "dataset_raw_mb": 5.850973344724618, "dataset_scaled_mb": 5.001463659, "dataset_tokens": 1413632, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001463659, "tokenizer_training_raw_mb": 5.850973344724618, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1068478019665920.0, "train_compute_hours": 0.10101974004114153, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 51.70479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 48.23694%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.05826%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "bqc_latn_5mb": {"language_name": "Boko (Benin)", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bqc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.976139974, "dataset_raw_mb": 4.881389385017295, "dataset_scaled_mb": 5.000706369, "dataset_tokens": 1332736, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000706369, "tokenizer_training_raw_mb": 4.881389385017295, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "15113", "model_parameters": "21261312", "train_compute_flops": 1007966701486080.0, "train_compute_hours": 0.09529866995868394, "dataset_hugging_face": [], "dataset_readme_str": "* 51.76714%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["eBible"], "dataset_links": ["https://ebible.org/find/"]}, "bsb_latn_5mb": {"language_name": "Brunei Bisaya", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bsb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.308722105, "dataset_raw_mb": 6.544316070174027, "dataset_scaled_mb": 5.00053911, "dataset_tokens": 1444864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00053911, "tokenizer_training_raw_mb": 6.544316070174027, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1092535336304640.0, "train_compute_hours": 0.10329424997789324, "dataset_hugging_face": [], "dataset_readme_str": "", "dataset_names": [], "dataset_links": []}, "run_latn_5mb": {"language_name": "Rundi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "run", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119232514, "dataset_raw_mb": 5.596256290053791, "dataset_scaled_mb": 5.000083736, "dataset_tokens": 1390080, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000083736, "tokenizer_training_raw_mb": 5.596256290053791, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1051355098644480.0, "train_compute_hours": 0.09940084569002357, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 82.69665%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.93706%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.36629%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "bak_cyrl_5mb": {"language_name": "Bashkir", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bak", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.27194034, "dataset_raw_mb": 11.361659998976062, "dataset_scaled_mb": 5.00086195, "dataset_tokens": 1554944, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00086195, "tokenizer_training_raw_mb": 11.361659998976062, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1175399426949120.0, "train_compute_hours": 0.11112867309337135, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 29.93711%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 29.35888%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.91103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.93919%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.89615%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Languages of Russia", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "orm_latn_5mb": {"language_name": "Oromo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "orm", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gax", "gaz", "hae", "orc"], "language_byte_premium": 1.264405846, "dataset_raw_mb": 6.323794001700249, "dataset_scaled_mb": 5.001395732, "dataset_tokens": 1502208, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001395732, "tokenizer_training_raw_mb": 6.323794001700249, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1135923733463040.0, "train_compute_hours": 0.10739642570923288, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "statmt/cc100", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 32.67081%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 27.67571%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.09955%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.69511%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.12785%: [eBible](https://ebible.org/find/)\n* 0.73097%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CC100", "CCNet", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "nso_latn_5mb": {"language_name": "Pedi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115696353, "dataset_raw_mb": 5.579642001067108, "dataset_scaled_mb": 5.001039921, "dataset_tokens": 1531392, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001039921, "tokenizer_training_raw_mb": 5.579642001067108, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1158276505927680.0, "train_compute_hours": 0.10950977874225339, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 46.87464%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 28.49201%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.91655%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.71679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "mlg_latn_5mb": {"language_name": "Malagasy", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mlg", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bhr", "bmm", "bzc", "msh", "plt", "skg", "tdx", "tkg", "txy", "xmv", "xmw"], "language_byte_premium": 1.266754162, "dataset_raw_mb": 6.335684000211657, "dataset_scaled_mb": 5.001510309, "dataset_tokens": 1525760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001510309, "tokenizer_training_raw_mb": 6.335684000211657, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1154441281536000.0, "train_compute_hours": 0.1091471757088582, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.72851%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.99233%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.22403%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.55221%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.26037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.24255%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TeDDi", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ven_latn_5mb": {"language_name": "Venda", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ven", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300531185, "dataset_raw_mb": 6.505036001111046, "dataset_scaled_mb": 5.00183008, "dataset_tokens": 1419264, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00183008, "tokenizer_training_raw_mb": 6.505036001111046, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41399", "model_parameters": "34892800", "train_compute_flops": 1073862829670400.0, "train_compute_hours": 0.10152884935065601, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.17655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71722%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 9.05878%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.04745%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lim_latn_5mb": {"language_name": "Limburgan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lim", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.997886276, "dataset_raw_mb": 4.990215144828327, "dataset_scaled_mb": 5.000785425, "dataset_tokens": 1457152, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000785425, "tokenizer_training_raw_mb": 4.990215144828327, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1101832849981440.0, "train_compute_hours": 0.10417328763460888, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 45.81871%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.97133%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.20996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tam_taml_5mb": {"language_name": "Tamil", "language_script": "Tamil", "dataset_category": "5mb", "language_iso6393": "tam", "language_iso15924": "taml", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.729099655, "dataset_raw_mb": 13.651773001823079, "dataset_scaled_mb": 5.002299193, "dataset_tokens": 1067008, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002299193, "tokenizer_training_raw_mb": 13.651773001823079, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 807334104268800.0, "train_compute_hours": 0.07632976985814109, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 36.08815%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.39717%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 19.80765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.41716%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.28987%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "OSCAR", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "fij_latn_5mb": {"language_name": "Fijian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.210673556, "dataset_raw_mb": 6.0535210004239, "dataset_scaled_mb": 5.000126558, "dataset_tokens": 1543168, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000126558, "tokenizer_training_raw_mb": 6.0535210004239, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "45580", "model_parameters": "36989952", "train_compute_flops": 1166915445719040.0, "train_compute_hours": 0.11032655123161833, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.40494%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 43.45086%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.42740%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.71680%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "bbc_latn_5mb": {"language_name": "Batak Toba", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bbc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.332303573, "dataset_raw_mb": 6.663180001639353, "dataset_scaled_mb": 5.001247566, "dataset_tokens": 1442304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001247566, "tokenizer_training_raw_mb": 6.663180001639353, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1090172218245120.0, "train_compute_hours": 0.10307082790681135, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 98.53788%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.46212%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/indonlp"]}, "lez_cyrl_5mb": {"language_name": "Lezghian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "lez", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.829893752, "dataset_raw_mb": 9.151782730185367, "dataset_scaled_mb": 5.001264538, "dataset_tokens": 1280512, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001264538, "tokenizer_training_raw_mb": 9.151782730185367, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 967754954833920.0, "train_compute_hours": 0.09149683209338881, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.87415%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 21.89044%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.23541%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nob_latn_5mb": {"language_name": "Norwegian Bokm\u00e5l", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nob", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 0.9976190383, "dataset_raw_mb": 4.988522996481718, "dataset_scaled_mb": 5.000428826, "dataset_tokens": 1070592, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000428826, "tokenizer_training_raw_mb": 4.988522996481718, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 809619743047680.0, "train_compute_hours": 0.07654586661541703, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "twi_latn_5mb": {"language_name": "Twi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "twi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aka"], "language_code_individuals": [], "language_byte_premium": 1.032409918, "dataset_raw_mb": 5.162951779990222, "dataset_scaled_mb": 5.000873868, "dataset_tokens": 1560576, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000873868, "tokenizer_training_raw_mb": 5.162951779990222, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1180784236953600.0, "train_compute_hours": 0.11163778240288583, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 77.35013%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.40427%: [eBible](https://ebible.org/find/)\n* 7.58195%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.66365%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "sah_cyrl_5mb": {"language_name": "Yakut", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "sah", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.881497141, "dataset_raw_mb": 9.407672998632902, "dataset_scaled_mb": 5.000099545, "dataset_tokens": 1221120, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000099545, "tokenizer_training_raw_mb": 9.407672998632902, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 922816972062720.0, "train_compute_hours": 0.0872481500859299, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 39.11470%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.57614%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 19.21725%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 5.94791%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.13671%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00728%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fon_latn_5mb": {"language_name": "Fon", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fon", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.541191249, "dataset_raw_mb": 7.7066670007918745, "dataset_scaled_mb": 5.000461173, "dataset_tokens": 1781760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000461173, "tokenizer_training_raw_mb": 7.7066670007918745, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1348139483136000.0, "train_compute_hours": 0.12746046022376728, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 81.63858%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.44701%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.91441%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [FFR](https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "FFR", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset", "https://wortschatz.uni-leipzig.de/en/download"]}, "tel_latn_5mb": {"language_name": "Telugu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tel", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.276370327, "dataset_raw_mb": 6.383482000255341, "dataset_scaled_mb": 5.001277345, "dataset_tokens": 1368576, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001277345, "tokenizer_training_raw_mb": 6.383482000255341, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1034387136184320.0, "train_compute_hours": 0.09779660196651753, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ast_latn_5mb": {"language_name": "Asturian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ast", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.748880208, "dataset_raw_mb": 8.745806060628144, "dataset_scaled_mb": 5.000803383, "dataset_tokens": 2150400, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000803383, "tokenizer_training_raw_mb": 8.745806060628144, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1627064893440000.0, "train_compute_hours": 0.15383158992523638, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.70018%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.64321%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.37355%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.28306%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pan_guru_5mb": {"language_name": "Panjabi", "language_script": "Gurmukhi", "dataset_category": "5mb", "language_iso6393": "pan", "language_iso15924": "guru", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.220967698, "dataset_raw_mb": 11.107244009008866, "dataset_scaled_mb": 5.001083095, "dataset_tokens": 1149440, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001083095, "tokenizer_training_raw_mb": 11.107244009008866, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "40049", "model_parameters": "33844224", "train_compute_flops": 868581475614720.0, "train_compute_hours": 0.08212043042175535, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.26136%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.38636%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.73324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.48906%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12998%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kas_deva_5mb": {"language_name": "Kashmiri", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "kas", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.525878379, "dataset_raw_mb": 12.632241351028743, "dataset_scaled_mb": 5.001128105, "dataset_tokens": 1443840, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001128105, "tokenizer_training_raw_mb": 12.632241351028743, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1092457857024000.0, "train_compute_hours": 0.10328692466408729, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "swe_latn_5mb": {"language_name": "Swedish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "swe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.020981292, "dataset_raw_mb": 5.106990995461914, "dataset_scaled_mb": 5.002041698, "dataset_tokens": 1081856, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002041698, "tokenizer_training_raw_mb": 5.106990995461914, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 817445150392320.0, "train_compute_hours": 0.07728572330981935, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "azb_arab_5mb": {"language_name": "South Azerbaijani", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "azb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.490422093, "dataset_raw_mb": 7.4540841847746755, "dataset_scaled_mb": 5.001324269, "dataset_tokens": 1311744, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001324269, "tokenizer_training_raw_mb": 7.4540841847746755, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 991812271472640.0, "train_compute_hours": 0.09377134203014052, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 46.02648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 42.37928%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.52167%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "OSCAR", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "ltz_latn_5mb": {"language_name": "Luxembourgish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ltz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.225348993, "dataset_raw_mb": 6.127229001133866, "dataset_scaled_mb": 5.000395019, "dataset_tokens": 1438720, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000395019, "tokenizer_training_raw_mb": 6.127229001133866, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1087886579466240.0, "train_compute_hours": 0.10285473114953543, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.34921%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.85023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.94131%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.85925%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "div_thaa_5mb": {"language_name": "Dhivehi", "language_script": "Thaana", "dataset_category": "5mb", "language_iso6393": "div", "language_iso15924": "thaa", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.997934845, "dataset_raw_mb": 9.994345998045999, "dataset_scaled_mb": 5.002338301, "dataset_tokens": 972288, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002338301, "tokenizer_training_raw_mb": 9.994345998045999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 735239633633280.0, "train_compute_hours": 0.06951356536169194, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.19489%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.14021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.85897%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.61771%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18821%: [eBible](https://ebible.org/find/)\n* 0.00002%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "yor_latn_5mb": {"language_name": "Yoruba", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "yor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.374948177, "dataset_raw_mb": 6.876290997449906, "dataset_scaled_mb": 5.001127397, "dataset_tokens": 1615872, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001127397, "tokenizer_training_raw_mb": 6.876290997449906, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1222623048499200.0, "train_compute_hours": 0.1155934518581062, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.33023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.88587%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.42418%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Menyo20K](https://github.com/uds-lsv/menyo-20k_MT), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 2.89553%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.95515%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.49782%: [eBible](https://ebible.org/find/)\n* 0.01122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Menyo20K", "OSCAR", "TeDDi", "W2C", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/uds-lsv/menyo-20k_MT", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "eng_latn_5mb": {"language_name": "English", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "eng", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.0, "dataset_raw_mb": 5.002111, "dataset_scaled_mb": 5.002111, "dataset_tokens": 1115648, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002111, "tokenizer_training_raw_mb": 5.002111, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "44666", "model_parameters": "35941376", "train_compute_flops": 843710626529280.0, "train_compute_hours": 0.07976900469004103, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bpy_beng_5mb": {"language_name": "Bishnupriya", "language_script": "Bengali", "dataset_category": "5mb", "language_iso6393": "bpy", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.330506839, "dataset_raw_mb": 11.652987140656693, "dataset_scaled_mb": 5.000194355, "dataset_tokens": 1127424, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000194355, "tokenizer_training_raw_mb": 11.652987140656693, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 852349566320640.0, "train_compute_hours": 0.08058577717940597, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.33593%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 26.65812%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.00595%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mya_mymr_5mb": {"language_name": "Burmese", "language_script": "Burmese", "dataset_category": "5mb", "language_iso6393": "mya", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 5.0, "dataset_raw_mb": 25.003099999999996, "dataset_scaled_mb": 5.00062, "dataset_tokens": 2134016, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00062, "tokenizer_training_raw_mb": 25.003099999999996, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1614668208537600.0, "train_compute_hours": 0.15265953971628218, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 56.80614%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.62193%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.24461%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 11.75396%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 4.08864%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.48472%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TeDDi", "TICO", "W2C", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kbp_latn_5mb": {"language_name": "Kabiy\u00e8", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kbp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.441209593, "dataset_raw_mb": 7.2072220002762295, "dataset_scaled_mb": 5.000814618, "dataset_tokens": 1736704, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000814618, "tokenizer_training_raw_mb": 7.2072220002762295, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "49952", "model_parameters": "39087104", "train_compute_flops": 1314048599654400.0, "train_compute_hours": 0.12423732214914328, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.53530%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.12198%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.69494%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.64778%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "hin_deva_5mb": {"language_name": "Hindi", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "hin", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.370151128, "dataset_raw_mb": 11.851844003917373, "dataset_scaled_mb": 5.000459196, "dataset_tokens": 1220096, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000459196, "tokenizer_training_raw_mb": 11.851844003917373, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "46432", "model_parameters": "36989952", "train_compute_flops": 922739492782080.0, "train_compute_hours": 0.08724082477212393, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kom_cyrl_5mb": {"language_name": "Komi", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kom", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["koi", "kpv"], "language_byte_premium": 1.614292695, "dataset_raw_mb": 8.073634001300332, "dataset_scaled_mb": 5.001344568, "dataset_tokens": 1335808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001344568, "tokenizer_training_raw_mb": 8.073634001300332, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1009593766379520.0, "train_compute_hours": 0.09545250154860917, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.95323%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 25.40122%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.73519%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.04414%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.86405%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00218%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hat_latn_5mb": {"language_name": "Haitian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9657950702, "dataset_raw_mb": 4.829888000299077, "dataset_scaled_mb": 5.000944972, "dataset_tokens": 1244160, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000944972, "tokenizer_training_raw_mb": 4.829888000299077, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 940675946250240.0, "train_compute_hours": 0.08893663491820451, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 43.18761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CMU_Haitian_Creole](http://www.speech.cs.cmu.edu/haitian/text/), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 35.83733%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39761%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.81760%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.62603%: [eBible](https://ebible.org/find/)\n* 0.13368%: [CMU Haitian Creole](http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht)\n* 0.00014%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CMU_Haitian_Creole", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible", "CMU Haitian Creole"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "http://www.speech.cs.cmu.edu/haitian/text/", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht"]}, "slv_latn_5mb": {"language_name": "Slovenian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "slv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9721500484, "dataset_raw_mb": 4.8614169969435945, "dataset_scaled_mb": 5.000685856, "dataset_tokens": 1051136, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000685856, "tokenizer_training_raw_mb": 4.8614169969435945, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 794201366200320.0, "train_compute_hours": 0.07508812916803026, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "arz_arab_5mb": {"language_name": "Egyptian Arabic", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "arz", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.551170046, "dataset_raw_mb": 7.756174129817305, "dataset_scaled_mb": 5.00020881, "dataset_tokens": 1360384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00020881, "tokenizer_training_raw_mb": 7.756174129817305, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1028188793733120.0, "train_compute_hours": 0.09721057686204045, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 84.18988%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.34266%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.36232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.09533%: [eBible](https://ebible.org/find/)\n* 0.00981%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "ADD", "AraBench", "DART", "Habibi", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "QADI", "Tatoeba", "TeDDi", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "uzn_cyrl_5mb": {"language_name": "Northern Uzbek", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "uzn", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 2.010315146, "dataset_raw_mb": 10.054145065540538, "dataset_scaled_mb": 5.001278076, "dataset_tokens": 1135616, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001278076, "tokenizer_training_raw_mb": 10.054145065540538, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 858547908771840.0, "train_compute_hours": 0.08117180228388306, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "wln_latn_5mb": {"language_name": "Walloon", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "wln", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.218380386, "dataset_raw_mb": 6.091980998013529, "dataset_scaled_mb": 5.000064896, "dataset_tokens": 1670656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000064896, "tokenizer_training_raw_mb": 6.091980998013529, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1263648327598080.0, "train_compute_hours": 0.11947220551836393, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.85998%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 12.46237%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 5.48451%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.19218%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "glg_latn_5mb": {"language_name": "Galician", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "glg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059078607, "dataset_raw_mb": 5.2959739999834206, "dataset_scaled_mb": 5.000548557, "dataset_tokens": 1159680, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000548557, "tokenizer_training_raw_mb": 5.2959739999834206, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 876329403678720.0, "train_compute_hours": 0.08285296180235172, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.77003%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [SLI_GalWeb.1.0](https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.22997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "SLI_GalWeb.1.0", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaa_latn_5mb": {"language_name": "Kara-Kalpak", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kaa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22736123, "dataset_raw_mb": 6.137416001247963, "dataset_scaled_mb": 5.00049688, "dataset_tokens": 1245184, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00049688, "tokenizer_training_raw_mb": 6.137416001247963, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 942148052582400.0, "train_compute_hours": 0.08907581588051783, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 97.52670%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.65677%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81649%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "hsb_latn_5mb": {"language_name": "Upper Sorbian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hsb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.124658236, "dataset_raw_mb": 5.625282614587801, "dataset_scaled_mb": 5.001770702, "dataset_tokens": 1250816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001770702, "tokenizer_training_raw_mb": 5.625282614587801, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 945983276974080.0, "train_compute_hours": 0.08943841891391302, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 52.92169%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.06296%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.86926%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14609%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hil_latn_5mb": {"language_name": "Hiligaynon", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.351057958, "dataset_raw_mb": 6.757401001803257, "dataset_scaled_mb": 5.001562636, "dataset_tokens": 1474560, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001562636, "tokenizer_training_raw_mb": 6.757401001803257, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "42119", "model_parameters": "34892800", "train_compute_flops": 1115701641216000.0, "train_compute_hours": 0.10548451880587638, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 99.96778%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.01707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/)\n* 0.01288%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00227%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/"]}, "jav_latn_5mb": {"language_name": "Javanese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "jav", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.146845795, "dataset_raw_mb": 5.735445001973892, "dataset_scaled_mb": 5.001060323, "dataset_tokens": 1295360, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001060323, "tokenizer_training_raw_mb": 5.735445001973892, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 979415586570240.0, "train_compute_hours": 0.09259929182118634, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.56951%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 38.81475%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.78232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 6.69194%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.05217%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.08930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "isl_latn_5mb": {"language_name": "Icelandic", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "isl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.154319145, "dataset_raw_mb": 5.773714004519799, "dataset_scaled_mb": 5.00183509, "dataset_tokens": 1242624, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00183509, "tokenizer_training_raw_mb": 5.773714004519799, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 939784934522880.0, "train_compute_hours": 0.08885239380943594, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 80.12926%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.11564%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 8.86357%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.85697%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03456%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Ndc without informant codes", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "tet_latn_5mb": {"language_name": "Tetum", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tet", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.400532559, "dataset_raw_mb": 7.002840000183093, "dataset_scaled_mb": 5.000126527, "dataset_tokens": 1519616, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000126527, "tokenizer_training_raw_mb": 7.002840000183093, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "44711", "model_parameters": "35941376", "train_compute_flops": 1149792524697600.0, "train_compute_hours": 0.10870765688050037, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 98.26624%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.01934%: [eBible](https://ebible.org/find/)\n* 0.71376%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00066%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "xal_cyrl_5mb": {"language_name": "Kalmyk", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "xal", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.723396561, "dataset_raw_mb": 8.619039999347313, "dataset_scaled_mb": 5.001193686, "dataset_tokens": 1236992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001193686, "tokenizer_training_raw_mb": 8.619039999347313, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 935949710131200.0, "train_compute_hours": 0.08848979077604073, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.88105%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 21.64138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.15243%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23428%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09086%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Tatoeba"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://tatoeba.org/en/"]}, "uig_arab_5mb": {"language_name": "Uighur", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "uig", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.308625158, "dataset_raw_mb": 11.547056000394855, "dataset_scaled_mb": 5.001702403, "dataset_tokens": 1388544, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001702403, "tokenizer_training_raw_mb": 11.547056000394855, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1050619045478400.0, "train_compute_hours": 0.09933125520886692, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 65.05923%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.33662%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.64713%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.70495%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 2.67723%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.57484%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "WikiMatrix", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "bem_latn_5mb": {"language_name": "Bemba", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bem", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155237492, "dataset_raw_mb": 5.777824245532927, "dataset_scaled_mb": 5.001416839, "dataset_tokens": 1502720, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001416839, "tokenizer_training_raw_mb": 5.777824245532927, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1136582307348480.0, "train_compute_hours": 0.10745869087658358, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.99219%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.00781%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "kur_latn_5mb": {"language_name": "Kurdish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kur", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.288322619, "dataset_raw_mb": 6.442143997291418, "dataset_scaled_mb": 5.000412088, "dataset_tokens": 1465344, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000412088, "tokenizer_training_raw_mb": 6.442143997291418, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1108031192432640.0, "train_compute_hours": 0.10475931273908597, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 71.61672%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.40823%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.33884%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.36208%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.27236%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00178%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "BLOOM", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "TICO", "W2C", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "ekk_latn_5mb": {"language_name": "Standard Estonian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ekk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["est"], "language_code_individuals": [], "language_byte_premium": 0.993591004, "dataset_raw_mb": 4.969392829644432, "dataset_scaled_mb": 5.001447084, "dataset_tokens": 1071104, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001447084, "tokenizer_training_raw_mb": 4.969392829644432, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 810433275494400.0, "train_compute_hours": 0.07662278241037965, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.24897%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.75103%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/"]}, "bik_latn_5mb": {"language_name": "Bikol", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bik", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bcl", "bln", "bto", "cts", "fbl", "lbl", "rbl", "ubl"], "language_byte_premium": 1.272069912, "dataset_raw_mb": 6.3622129986771805, "dataset_scaled_mb": 5.001464887, "dataset_tokens": 1467904, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001464887, "tokenizer_training_raw_mb": 6.3622129986771805, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1110239351930880.0, "train_compute_hours": 0.10496808418255593, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 43.21685%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.79360%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.98955%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "pnb_arab_5mb": {"language_name": "Western Panjabi", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "pnb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["lah"], "language_code_individuals": [], "language_byte_premium": 1.414617177, "dataset_raw_mb": 7.074940764993295, "dataset_scaled_mb": 5.001311224, "dataset_tokens": 1317376, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001311224, "tokenizer_training_raw_mb": 7.074940764993295, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "39221", "model_parameters": "33844224", "train_compute_flops": 995647495864320.0, "train_compute_hours": 0.09413394506353572, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.15181%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.73099%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.11696%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00023%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lvs_latn_5mb": {"language_name": "Standard Latvian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lvs", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.206980036, "dataset_raw_mb": 6.03691485055743, "dataset_scaled_mb": 5.001669183, "dataset_tokens": 1250816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001669183, "tokenizer_training_raw_mb": 6.03691485055743, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 945983276974080.0, "train_compute_hours": 0.08943841891391302, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "mai_deva_5mb": {"language_name": "Maithili", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "mai", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.38980971, "dataset_raw_mb": 11.951686997902039, "dataset_scaled_mb": 5.001104041, "dataset_tokens": 1388032, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001104041, "tokenizer_training_raw_mb": 11.951686997902039, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1049805513031680.0, "train_compute_hours": 0.0992543394139043, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.00767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.01673%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.36867%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.75652%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.83462%: [eBible](https://ebible.org/find/)\n* 0.01578%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fur_latn_5mb": {"language_name": "Friulian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.067269167, "dataset_raw_mb": 5.336636025486508, "dataset_scaled_mb": 5.0002719, "dataset_tokens": 1356800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.0002719, "tokenizer_training_raw_mb": 5.336636025486508, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1025903154954240.0, "train_compute_hours": 0.09699448010476452, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 72.56517%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 15.82188%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.61295%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pcm_latn_5mb": {"language_name": "Nigerian Pidgin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pcm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.954503897, "dataset_raw_mb": 4.774616074763351, "dataset_scaled_mb": 5.002196523, "dataset_tokens": 1119744, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002196523, "tokenizer_training_raw_mb": 4.774616074763351, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "33821", "model_parameters": "30698496", "train_compute_flops": 846809797754880.0, "train_compute_hours": 0.08006201724227957, "dataset_hugging_face": ["castorini/afriberta-corpus", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 48.40160%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 48.06214%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 3.19131%: [eBible](https://ebible.org/find/)\n* 0.34495%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["AfriBERTa", "Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/castorini/afriberta-corpus", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "pus_arab_5mb": {"language_name": "Pushto", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "pus", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pbt", "pbu", "pst"], "language_byte_premium": 1.585665259, "dataset_raw_mb": 7.930212997222756, "dataset_scaled_mb": 5.001189849, "dataset_tokens": 1271296, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001189849, "tokenizer_training_raw_mb": 7.930212997222756, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "44041", "model_parameters": "35941376", "train_compute_flops": 961479133102080.0, "train_compute_hours": 0.09090348167510576, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.56138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95008%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.68420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.73176%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/)\n* 4.07251%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00007%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Tatoeba", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "mgh_latn_5mb": {"language_name": "Makhuwa-Meetto", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mgh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113355123, "dataset_raw_mb": 5.567544000378204, "dataset_scaled_mb": 5.000690153, "dataset_tokens": 1097728, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000690153, "tokenizer_training_raw_mb": 5.567544000378204, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 830577888460800.0, "train_compute_hours": 0.07852736399993018, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 85.38638%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.61362%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "yid_hebr_5mb": {"language_name": "Yiddish", "language_script": "Hebrew", "dataset_category": "5mb", "language_iso6393": "yid", "language_iso15924": "hebr", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ydd", "yih"], "language_byte_premium": 1.546105069, "dataset_raw_mb": 7.730952999208191, "dataset_scaled_mb": 5.000276601, "dataset_tokens": 1005056, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000276601, "tokenizer_training_raw_mb": 7.730952999208191, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 760033003438080.0, "train_compute_hours": 0.0718576657796003, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.13900%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.11646%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.12670%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.66901%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.94883%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "bos_cyrl_5mb": {"language_name": "Bosnian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bos", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.148940187, "dataset_raw_mb": 5.746857001686719, "dataset_scaled_mb": 5.00187657, "dataset_tokens": 1246208, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00187657, "tokenizer_training_raw_mb": 5.746857001686719, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 942225531863040.0, "train_compute_hours": 0.08908314119432378, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "zho_hans_5mb": {"language_name": "Chinese", "language_script": "Han Simplified", "dataset_category": "5mb", "language_iso6393": "zho", "language_iso15924": "hans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.9359663918, "dataset_raw_mb": 4.679860999229239, "dataset_scaled_mb": 5.000031027, "dataset_tokens": 1085952, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000031027, "tokenizer_training_raw_mb": 4.679860999229239, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 820544321617920.0, "train_compute_hours": 0.0775787358620579, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ell_grek_5mb": {"language_name": "Modern Greek", "language_script": "Greek", "dataset_category": "5mb", "language_iso6393": "ell", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.967262375, "dataset_raw_mb": 9.839772995764818, "dataset_scaled_mb": 5.001759359, "dataset_tokens": 1269760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001759359, "tokenizer_training_raw_mb": 9.839772995764818, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 960743079936000.0, "train_compute_hours": 0.09083389119394909, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tdx_latn_5mb": {"language_name": "Tandroy-Mahafaly Malagasy", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tdx", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.000930264, "dataset_raw_mb": 5.006158000307794, "dataset_scaled_mb": 5.00150528, "dataset_tokens": 1250304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00150528, "tokenizer_training_raw_mb": 5.006158000307794, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41557", "model_parameters": "34892800", "train_compute_flops": 945324703088640.0, "train_compute_hours": 0.08937615374656234, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 71.99496%: [eBible](https://ebible.org/find/)\n* 28.00504%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["eBible", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mdf_cyrl_5mb": {"language_name": "Moksha", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mdf", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.712267834, "dataset_raw_mb": 8.563704998480363, "dataset_scaled_mb": 5.001381693, "dataset_tokens": 1070592, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001381693, "tokenizer_training_raw_mb": 8.563704998480363, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 809619743047680.0, "train_compute_hours": 0.07654586661541703, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 79.42795%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39365%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.17561%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00279%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Languages of Russia", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "lub_latn_5mb": {"language_name": "Luba-Katanga", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lub", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.296565957, "dataset_raw_mb": 6.483475998659233, "dataset_scaled_mb": 5.000498404, "dataset_tokens": 1353728, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000498404, "tokenizer_training_raw_mb": 6.483475998659233, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41962", "model_parameters": "34892800", "train_compute_flops": 1024276090060800.0, "train_compute_hours": 0.09684064851483928, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "hau_latn_5mb": {"language_name": "Hausa", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hau", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176529442, "dataset_raw_mb": 5.8834790010206115, "dataset_scaled_mb": 5.000706987, "dataset_tokens": 1437184, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000706987, "tokenizer_training_raw_mb": 5.8834790010206115, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1086995567738880.0, "train_compute_hours": 0.10277049004076684, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 31.25887%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.98045%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.12632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/)\n* 8.31691%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 3.07782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23963%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "TeDDi", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "umb_latn_5mb": {"language_name": "Umbundu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "umb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.167261104, "dataset_raw_mb": 5.836331435531031, "dataset_scaled_mb": 5.000022202, "dataset_tokens": 1521152, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000022202, "tokenizer_training_raw_mb": 5.836331435531031, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1150528577863680.0, "train_compute_hours": 0.10877724736165703, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 99.99900%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "aze_arab_5mb": {"language_name": "Azerbaijani", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "aze", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.198662364, "dataset_raw_mb": 5.993832002307443, "dataset_scaled_mb": 5.000433969, "dataset_tokens": 1126400, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000433969, "tokenizer_training_raw_mb": 5.993832002307443, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 852272087040000.0, "train_compute_hours": 0.0805784518656, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.47341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 22.81190%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.00426%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.71043%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fra_latn_5mb": {"language_name": "French", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fra", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173979035, "dataset_raw_mb": 5.87194100017351, "dataset_scaled_mb": 5.001742642, "dataset_tokens": 1299968, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001742642, "tokenizer_training_raw_mb": 5.87194100017351, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 983173331681280.0, "train_compute_hours": 0.09295456954077558, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nor_latn_5mb": {"language_name": "Norwegian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nor", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["nno", "nob"], "language_byte_premium": 1.125315993, "dataset_raw_mb": 5.62690501477521, "dataset_scaled_mb": 5.000288852, "dataset_tokens": 1321984, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000288852, "tokenizer_training_raw_mb": 5.62690501477521, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 999560199536640.0, "train_compute_hours": 0.09450387341073688, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.92843%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.07157%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "tgl_latn_5mb": {"language_name": "Tagalog", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tgl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.117593432, "dataset_raw_mb": 5.5897839847034785, "dataset_scaled_mb": 5.001625658, "dataset_tokens": 1286656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001625658, "tokenizer_training_raw_mb": 5.5897839847034785, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 972403711672320.0, "train_compute_hours": 0.09193635092174662, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.29659%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.79155%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.61906%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29280%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TICO", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "zho_hant_5mb": {"language_name": "Chinese", "language_script": "Han Traditional", "dataset_category": "5mb", "language_iso6393": "zho", "language_iso15924": "hant", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.989382544, "dataset_raw_mb": 4.947886000343097, "dataset_scaled_mb": 5.000983725, "dataset_tokens": 1239552, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000983725, "tokenizer_training_raw_mb": 4.947886000343097, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 936763242577920.0, "train_compute_hours": 0.08856670657100335, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 36.42318%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.54996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 29.93702%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.08984%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "CCNet", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "deu_latn_5mb": {"language_name": "German", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "deu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.053648018, "dataset_raw_mb": 5.270702004973883, "dataset_scaled_mb": 5.002336563, "dataset_tokens": 1104384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002336563, "tokenizer_training_raw_mb": 5.270702004973883, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 834490592133120.0, "train_compute_hours": 0.07889729234713136, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "sun_latn_5mb": {"language_name": "Sundanese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.096980852, "dataset_raw_mb": 5.48687999820613, "dataset_scaled_mb": 5.001801069, "dataset_tokens": 1292288, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001801069, "tokenizer_training_raw_mb": 5.48687999820613, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 977788521676800.0, "train_compute_hours": 0.0924454602312611, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.54837%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 32.02343%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.03343%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.55888%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81131%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.02459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "MC4", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tam_latn_5mb": {"language_name": "Tamil", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.267529359, "dataset_raw_mb": 6.339114000793982, "dataset_scaled_mb": 5.001157532, "dataset_tokens": 1298944, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001157532, "tokenizer_training_raw_mb": 6.339114000793982, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 981701225349120.0, "train_compute_hours": 0.09281538857846226, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "crh_cyrl_5mb": {"language_name": "Crimean Tatar", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "crh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.89497331, "dataset_raw_mb": 9.47807799727295, "dataset_scaled_mb": 5.001694719, "dataset_tokens": 1090048, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001694719, "tokenizer_training_raw_mb": 9.47807799727295, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 823643492843520.0, "train_compute_hours": 0.07787174841429645, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 89.82288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.17712%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "jpn_jpan_5mb": {"language_name": "Japanese", "language_script": "Japanese", "dataset_category": "5mb", "language_iso6393": "jpn", "language_iso15924": "jpan", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321973899, "dataset_raw_mb": 6.610830000253823, "dataset_scaled_mb": 5.000726569, "dataset_tokens": 1158656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000726569, "tokenizer_training_raw_mb": 6.610830000253823, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 876251924398080.0, "train_compute_hours": 0.08284563648854575, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "meo_latn_5mb": {"language_name": "Kedah Malay", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "meo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.276200961, "dataset_raw_mb": 6.381806999400065, "dataset_scaled_mb": 5.00062858, "dataset_tokens": 1542656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00062858, "tokenizer_training_raw_mb": 6.381806999400065, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "49657", "model_parameters": "39087104", "train_compute_flops": 1166101913272320.0, "train_compute_hours": 0.11024963543665572, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ilo_latn_5mb": {"language_name": "Iloko", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ilo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.07650844, "dataset_raw_mb": 5.383399000602955, "dataset_scaled_mb": 5.000795907, "dataset_tokens": 1353216, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000795907, "tokenizer_training_raw_mb": 5.383399000602955, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1023462557614080.0, "train_compute_hours": 0.09676373271987666, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 41.64934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.63745%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.20790%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.12547%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.86976%: [eBible](https://ebible.org/find/)\n* 0.51008%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kha_latn_5mb": {"language_name": "Khasi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kha", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.303114193, "dataset_raw_mb": 6.516978002549892, "dataset_scaled_mb": 5.00107975, "dataset_tokens": 1558528, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00107975, "tokenizer_training_raw_mb": 6.516978002549892, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "29501", "model_parameters": "28601344", "train_compute_flops": 1179234651340800.0, "train_compute_hours": 0.11149127612676656, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 99.86112%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.13888%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/"]}, "mhr_cyrl_5mb": {"language_name": "Eastern Mari", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mhr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["chm"], "language_code_individuals": [], "language_byte_premium": 1.810192209, "dataset_raw_mb": 9.055052135508298, "dataset_scaled_mb": 5.002260031, "dataset_tokens": 1223680, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002260031, "tokenizer_training_raw_mb": 9.055052135508298, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 925180090122240.0, "train_compute_hours": 0.08747157215701179, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 85.57926%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 7.02298%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.37537%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.95016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07223%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "amh_ethi_5mb": {"language_name": "Amharic", "language_script": "Ge'ez", "dataset_category": "5mb", "language_iso6393": "amh", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.720917143, "dataset_raw_mb": 8.605544000827411, "dataset_scaled_mb": 5.000556846, "dataset_tokens": 1136128, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000556846, "tokenizer_training_raw_mb": 8.605544000827411, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 859206482657280.0, "train_compute_hours": 0.08123406745123375, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 35.99036%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [OSCAR](https://oscar-project.org/), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 33.89884%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.09942%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.48807%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.48971%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03361%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "OSCAR", "Parallel Corpora for Ethiopian Languages", "TICO", "W2C", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://oscar-project.org/", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "gla_latn_5mb": {"language_name": "Scottish Gaelic", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "gla", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9936126744, "dataset_raw_mb": 4.968990000275956, "dataset_scaled_mb": 5.000932585, "dataset_tokens": 1152000, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000932585, "tokenizer_training_raw_mb": 4.968990000275956, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "44264", "model_parameters": "35941376", "train_compute_flops": 870944593674240.0, "train_compute_hours": 0.08234385249283724, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.49478%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.15694%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.70049%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 0.49333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.15447%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bug_latn_5mb": {"language_name": "Buginese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22786025, "dataset_raw_mb": 6.141299149299325, "dataset_scaled_mb": 5.001627139, "dataset_tokens": 1554944, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001627139, "tokenizer_training_raw_mb": 6.141299149299325, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1175399426949120.0, "train_compute_hours": 0.11112867309337135, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 93.76215%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.23785%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/indonlp"]}, "mlt_latn_5mb": {"language_name": "Maltese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mlt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.08846667, "dataset_raw_mb": 5.443807005432113, "dataset_scaled_mb": 5.001353882, "dataset_tokens": 1488384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001353882, "tokenizer_training_raw_mb": 5.443807005432113, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1125735208058880.0, "train_compute_hours": 0.10643314694374867, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 94.92627%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MaCoCu](https://macocu.eu/), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23828%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.42133%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.41412%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MaCoCu", "MC4", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://macocu.eu/", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "bod_tibt_5mb": {"language_name": "Tibetan", "language_script": "Tibetan", "dataset_category": "5mb", "language_iso6393": "bod", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.616914857, "dataset_raw_mb": 13.08758499843851, "dataset_scaled_mb": 5.001150482, "dataset_tokens": 986624, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001150482, "tokenizer_training_raw_mb": 13.08758499843851, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 746086732922880.0, "train_compute_hours": 0.07053910929452684, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 35.45938%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 27.32632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.94564%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.69915%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.63626%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.93326%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "MoT", "OSCAR", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "cym_latn_5mb": {"language_name": "Welsh", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cym", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.026556848, "dataset_raw_mb": 5.1332970020659054, "dataset_scaled_mb": 5.000499497, "dataset_tokens": 1230848, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000499497, "tokenizer_training_raw_mb": 5.1332970020659054, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 931300953292800.0, "train_compute_hours": 0.08805027194768292, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.76968%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.88536%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.87431%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.28932%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.18133%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "urd_arab_5mb": {"language_name": "Urdu", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "urd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.707950617, "dataset_raw_mb": 8.540170994896721, "dataset_scaled_mb": 5.000244685, "dataset_tokens": 1314304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000244685, "tokenizer_training_raw_mb": 8.540170994896721, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "37138", "model_parameters": "32795648", "train_compute_flops": 994020430970880.0, "train_compute_hours": 0.09398011347361047, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 37.89138%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.70994%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 26.24866%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 0.15002%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "Anuvaad", "CCNet", "Earthlings", "OSCAR", "TICO", "W2C", "Workshop on NER for South and South East Asian Languages", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://ebible.org/find/"]}, "pap_latn_5mb": {"language_name": "Papiamento", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.001675271, "dataset_raw_mb": 5.0084499991676, "dataset_scaled_mb": 5.000073521, "dataset_tokens": 1222656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000073521, "tokenizer_training_raw_mb": 5.0084499991676, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 925102610841600.0, "train_compute_hours": 0.08746424684320582, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 68.24074%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.46139%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.28720%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.01066%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "war_latn_5mb": {"language_name": "Waray", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "war", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.093149717, "dataset_raw_mb": 5.466451998420997, "dataset_scaled_mb": 5.000643474, "dataset_tokens": 1493504, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000643474, "tokenizer_training_raw_mb": 5.466451998420997, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1128911858565120.0, "train_compute_hours": 0.10673348480979317, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.84724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 25.81890%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.17011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.16375%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bel_cyrl_5mb": {"language_name": "Belarusian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bel", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.007901403, "dataset_raw_mb": 10.040037000575321, "dataset_scaled_mb": 5.00026395, "dataset_tokens": 1347584, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00026395, "tokenizer_training_raw_mb": 10.040037000575321, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1019627333222400.0, "train_compute_hours": 0.09640112968648146, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.85869%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.04732%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 14.98527%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.10871%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "sme_latn_5mb": {"language_name": "Northern Sami", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sme", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.267947826, "dataset_raw_mb": 6.340776001942398, "dataset_scaled_mb": 5.000817756, "dataset_tokens": 1250304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000817756, "tokenizer_training_raw_mb": 6.340776001942398, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 945324703088640.0, "train_compute_hours": 0.08937615374656234, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06677%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.99484%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97487%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.96230%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00123%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nhe_latn_5mb": {"language_name": "Eastern Huasteca Nahuatl", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nhe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.492247034, "dataset_raw_mb": 7.463786000665203, "dataset_scaled_mb": 5.001709389, "dataset_tokens": 1266176, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001709389, "tokenizer_training_raw_mb": 7.463786000665203, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41483", "model_parameters": "34892800", "train_compute_flops": 956907855544320.0, "train_compute_hours": 0.0904712881605539, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 56.31133%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.68867%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "asm_beng_5mb": {"language_name": "Assamese", "language_script": "Bengali", "dataset_category": "5mb", "language_iso6393": "asm", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.526329303, "dataset_raw_mb": 12.637383000421734, "dataset_scaled_mb": 5.00227068, "dataset_tokens": 1166336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00227068, "tokenizer_training_raw_mb": 12.637383000421734, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 881791692963840.0, "train_compute_hours": 0.08336939642567215, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.77996%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 25.83555%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.99529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.32777%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.47875%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.58268%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CC100", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "bew_latn_5mb": {"language_name": "Betawi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bew", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300925888, "dataset_raw_mb": 6.505484997813022, "dataset_scaled_mb": 5.000657653, "dataset_tokens": 1378816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000657653, "tokenizer_training_raw_mb": 6.505484997813022, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1042135064248320.0, "train_compute_hours": 0.0985291333471139, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 51.56687%: [IndoNLP](https://huggingface.co/indonlp)\n* 46.73817%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.69496%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["IndoNLP", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/indonlp", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "gom_deva_5mb": {"language_name": "Goan Konkani", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "gom", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.736906273, "dataset_raw_mb": 8.685578997387623, "dataset_scaled_mb": 5.00060316, "dataset_tokens": 1005568, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00060316, "tokenizer_training_raw_mb": 8.685578997387623, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 760846535884800.0, "train_compute_hours": 0.07193458157456291, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.12288%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.53101%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.87562%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.47037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_latn_5mb": {"language_name": "Azerbaijani", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "aze", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.29878361, "dataset_raw_mb": 6.496436004382275, "dataset_scaled_mb": 5.001938702, "dataset_tokens": 1229824, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001938702, "tokenizer_training_raw_mb": 6.496436004382275, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 929828846960640.0, "train_compute_hours": 0.0879110909853696, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.37652%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 18.62348%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "sna_latn_5mb": {"language_name": "Shona", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sna", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119177764, "dataset_raw_mb": 5.5977870014466315, "dataset_scaled_mb": 5.00169605, "dataset_tokens": 1326080, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00169605, "tokenizer_training_raw_mb": 5.5977870014466315, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1002659370762240.0, "train_compute_hours": 0.09479688596297543, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.75185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 27.93918%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.33727%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.07291%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.53601%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.36278%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kur_arab_5mb": {"language_name": "Kurdish", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "kur", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.569891339, "dataset_raw_mb": 7.849966997599091, "dataset_scaled_mb": 5.000325056, "dataset_tokens": 1137664, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000325056, "tokenizer_training_raw_mb": 7.849966997599091, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 860097494384640.0, "train_compute_hours": 0.08131830856000233, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 43.19057%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.23880%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.93282%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05661%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.32587%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.25533%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TICO", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "mal_mlym_5mb": {"language_name": "Malayalam", "language_script": "Malayalam", "dataset_category": "5mb", "language_iso6393": "mal", "language_iso15924": "mlym", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.884868527, "dataset_raw_mb": 14.425401035565185, "dataset_scaled_mb": 5.00036688, "dataset_tokens": 1291264, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00036688, "tokenizer_training_raw_mb": 14.425401035565185, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 976316415344640.0, "train_compute_hours": 0.09230627926894779, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 79.43378%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 20.26204%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30417%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "scn_latn_5mb": {"language_name": "Sicilian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "scn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.036272745, "dataset_raw_mb": 5.182945209851781, "dataset_scaled_mb": 5.001526128, "dataset_tokens": 1465856, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001526128, "tokenizer_training_raw_mb": 5.182945209851781, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1108689766318080.0, "train_compute_hours": 0.10482157790643666, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.07639%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.21878%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.70271%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00212%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dyu_latn_5mb": {"language_name": "Dyula", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "dyu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.15445855, "dataset_raw_mb": 5.772568095290926, "dataset_scaled_mb": 5.000238506, "dataset_tokens": 1646592, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000238506, "tokenizer_training_raw_mb": 5.772568095290926, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "39052", "model_parameters": "33844224", "train_compute_flops": 1245866832691200.0, "train_compute_hours": 0.11779104599989529, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.41765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 31.58235%: [eBible](https://ebible.org/find/)\n* 0.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "eBible", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "pam_latn_5mb": {"language_name": "Pampanga", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.18679287, "dataset_raw_mb": 5.934984085158448, "dataset_scaled_mb": 5.000859236, "dataset_tokens": 1563648, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000859236, "tokenizer_training_raw_mb": 5.934984085158448, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1182411301847040.0, "train_compute_hours": 0.11179161399281107, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 91.78710%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.56188%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.58011%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.06645%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00446%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fuv_latn_5mb": {"language_name": "Nigerian Fulfulde", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fuv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["ful"], "language_code_individuals": [], "language_byte_premium": 1.110831498, "dataset_raw_mb": 5.554614255025325, "dataset_scaled_mb": 5.000411192, "dataset_tokens": 1498624, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000411192, "tokenizer_training_raw_mb": 5.554614255025325, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1133483136122880.0, "train_compute_hours": 0.10716567832434502, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "dzo_tibt_5mb": {"language_name": "Dzongkha", "language_script": "Tibetan", "dataset_category": "5mb", "language_iso6393": "dzo", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.261842241, "dataset_raw_mb": 16.314850998733377, "dataset_scaled_mb": 5.001729021, "dataset_tokens": 1396736, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001729021, "tokenizer_training_raw_mb": 16.314850998733377, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1056817387929600.0, "train_compute_hours": 0.09991728031334401, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 74.77296%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.41761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 9.58626%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.22317%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "epo_latn_5mb": {"language_name": "Esperanto", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "epo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9954937917, "dataset_raw_mb": 4.977636998847532, "dataset_scaled_mb": 5.000168801, "dataset_tokens": 1205760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000168801, "tokenizer_training_raw_mb": 4.977636998847532, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 911892393492480.0, "train_compute_hours": 0.08621528083928903, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.12373%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.22117%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.49198%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 14.01395%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14917%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "xmf_geor_5mb": {"language_name": "Mingrelian", "language_script": "Georgian", "dataset_category": "5mb", "language_iso6393": "xmf", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.509423461, "dataset_raw_mb": 12.548335331468923, "dataset_scaled_mb": 5.000485381, "dataset_tokens": 1267200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000485381, "tokenizer_training_raw_mb": 12.548335331468923, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 958379961876480.0, "train_compute_hours": 0.0906104691228672, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.93889%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.36833%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 13.69277%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ceb_latn_5mb": {"language_name": "Cebuano", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ceb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113380333, "dataset_raw_mb": 5.566994999673316, "dataset_scaled_mb": 5.00008383, "dataset_tokens": 1385984, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00008383, "tokenizer_training_raw_mb": 5.566994999673316, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1048255927418880.0, "train_compute_hours": 0.09910783313778503, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 36.46583%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 33.35484%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.15251%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.56324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.46359%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "mon_latn_5mb": {"language_name": "Mongolian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.181843271, "dataset_raw_mb": 5.9108565596378515, "dataset_scaled_mb": 5.001387836, "dataset_tokens": 1357312, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001387836, "tokenizer_training_raw_mb": 5.9108565596378515, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1026561728839680.0, "train_compute_hours": 0.0970567452721152, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "kmb_latn_5mb": {"language_name": "Kimbundu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kmb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.132798568, "dataset_raw_mb": 5.6644030003071055, "dataset_scaled_mb": 5.000362077, "dataset_tokens": 1547264, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000362077, "tokenizer_training_raw_mb": 5.6644030003071055, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1170014616944640.0, "train_compute_hours": 0.11061956378385689, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 68.73144%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.26856%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mal_latn_5mb": {"language_name": "Malayalam", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.265435969, "dataset_raw_mb": 6.329776002045077, "dataset_scaled_mb": 5.002051591, "dataset_tokens": 1235456, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002051591, "tokenizer_training_raw_mb": 6.329776002045077, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 933664071352320.0, "train_compute_hours": 0.08827369401876481, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "aka_latn_5mb": {"language_name": "Akan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "aka", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["fat", "twi"], "language_byte_premium": 1.574942782, "dataset_raw_mb": 7.875641001643567, "dataset_scaled_mb": 5.000588651, "dataset_tokens": 2353664, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000588651, "tokenizer_training_raw_mb": 7.875641001643567, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1779737815941120.0, "train_compute_hours": 0.16826612077988773, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43728%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.29722%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.29716%: [eBible](https://ebible.org/find/)\n* 5.04766%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.84113%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Akuapem](https://zenodo.org/record/4432117#.Y00gXOxBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.07955%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Akuapem", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/4432117#.Y00gXOxBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "mkd_cyrl_5mb": {"language_name": "Macedonian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mkd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.834986754, "dataset_raw_mb": 9.178339019143936, "dataset_scaled_mb": 5.001855735, "dataset_tokens": 1167872, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001855735, "tokenizer_training_raw_mb": 9.178339019143936, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 882527746129920.0, "train_compute_hours": 0.08343898690682881, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.44064%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.55936%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tso_latn_5mb": {"language_name": "Tsonga", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.207440889, "dataset_raw_mb": 6.038793999877377, "dataset_scaled_mb": 5.001316466, "dataset_tokens": 1570816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001316466, "tokenizer_training_raw_mb": 6.038793999877377, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1188532165017600.0, "train_compute_hours": 0.1123703137834822, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 57.07934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 29.98123%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.39404%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.54534%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nep_deva_5mb": {"language_name": "Nepali", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "nep", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dty", "npi"], "language_byte_premium": 2.629966681, "dataset_raw_mb": 13.153205998002681, "dataset_scaled_mb": 5.001282371, "dataset_tokens": 1160192, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001282371, "tokenizer_training_raw_mb": 13.153205998002681, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 877142936125440.0, "train_compute_hours": 0.08292987759731434, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 53.66687%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.99086%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 16.12038%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.97982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.24207%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Tatoeba", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "swa_latn_5mb": {"language_name": "Swahili", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "swa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["swc", "swh"], "language_byte_premium": 1.258733422, "dataset_raw_mb": 6.295051998720486, "dataset_scaled_mb": 5.001100224, "dataset_tokens": 1378304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001100224, "tokenizer_training_raw_mb": 6.295051998720486, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1042871117414400.0, "train_compute_hours": 0.09859872382827055, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 57.90197%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.61041%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 5.07304%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.23078%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18380%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ang_latn_5mb": {"language_name": "Old English", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ang", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.289057339, "dataset_raw_mb": 6.446659999970447, "dataset_scaled_mb": 5.001065356, "dataset_tokens": 1548800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001065356, "tokenizer_training_raw_mb": 6.446659999970447, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1170750670110720.0, "train_compute_hours": 0.11068915426501354, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 92.13059%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 7.77618%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09323%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "est_latn_5mb": {"language_name": "Estonian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "est", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ekk", "vro"], "language_byte_premium": 0.9677856419, "dataset_raw_mb": 4.840993994406347, "dataset_scaled_mb": 5.002134548, "dataset_tokens": 995840, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002134548, "tokenizer_training_raw_mb": 4.840993994406347, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 752362554654720.0, "train_compute_hours": 0.0711324597128099, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cor_latn_5mb": {"language_name": "Cornish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.215014217, "dataset_raw_mb": 6.077171997672984, "dataset_scaled_mb": 5.001729126, "dataset_tokens": 1550848, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001729126, "tokenizer_training_raw_mb": 6.077171997672984, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1172300255723520.0, "train_compute_hours": 0.11083566054113281, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 71.08004%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.11609%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.80387%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "spa_latn_5mb": {"language_name": "Spanish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "spa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.083831966, "dataset_raw_mb": 5.419347999486776, "dataset_scaled_mb": 5.000173615, "dataset_tokens": 1154560, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000173615, "tokenizer_training_raw_mb": 5.419347999486776, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 873152753172480.0, "train_compute_hours": 0.0825526239363072, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "knc_arab_5mb": {"language_name": "Central Kanuri", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "knc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 2.50197962, "dataset_raw_mb": 12.510744819943001, "dataset_scaled_mb": 5.00033842, "dataset_tokens": 5341184, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00033842, "tokenizer_training_raw_mb": 12.510744819943001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "16465", "model_parameters": "22309888", "train_compute_flops": 4041319278182400.0, "train_compute_hours": 0.3820883681190633, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "srp_cyrl_5mb": {"language_name": "Serbian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "srp", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.424878645, "dataset_raw_mb": 7.127638994537168, "dataset_scaled_mb": 5.002277927, "dataset_tokens": 983040, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002277927, "tokenizer_training_raw_mb": 7.127638994537168, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 743801094144000.0, "train_compute_hours": 0.07032301253725091, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaa_cyrl_5mb": {"language_name": "Kara-Kalpak", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kaa", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.920886208, "dataset_raw_mb": 9.607376002509412, "dataset_scaled_mb": 5.001533127, "dataset_tokens": 1123840, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001533127, "tokenizer_training_raw_mb": 9.607376002509412, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 849908968980480.0, "train_compute_hours": 0.08035502979451811, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 99.51383%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.48617%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "mos_latn_5mb": {"language_name": "Mossi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141292925, "dataset_raw_mb": 5.707972170237561, "dataset_scaled_mb": 5.00132091, "dataset_tokens": 1732608, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00132091, "tokenizer_training_raw_mb": 5.707972170237561, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1310949428428800.0, "train_compute_hours": 0.12394430959690474, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.01233%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lit_latn_5mb": {"language_name": "Lithuanian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lit", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.030036841, "dataset_raw_mb": 5.151631994912641, "dataset_scaled_mb": 5.001405571, "dataset_tokens": 1074176, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001405571, "tokenizer_training_raw_mb": 5.151631994912641, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 812060340387840.0, "train_compute_hours": 0.07677661400030487, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "otq_latn_5mb": {"language_name": "Quer\u00e9taro Otomi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "otq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.252835364, "dataset_raw_mb": 6.264226000052214, "dataset_scaled_mb": 5.000039255, "dataset_tokens": 1678336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000039255, "tokenizer_training_raw_mb": 6.264226000052214, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "33973", "model_parameters": "30698496", "train_compute_flops": 1269188096163840.0, "train_compute_hours": 0.11999596545549034, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 95.35934%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.64066%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "nya_latn_5mb": {"language_name": "Nyanja", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nya", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.21429492, "dataset_raw_mb": 6.072450999973633, "dataset_scaled_mb": 5.000804088, "dataset_tokens": 1328640, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000804088, "tokenizer_training_raw_mb": 6.072450999973633, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1004867530260480.0, "train_compute_hours": 0.09500565740644538, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.17651%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.50780%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.70728%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.45060%: [eBible](https://ebible.org/find/)\n* 0.15782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "ady_cyrl_5mb": {"language_name": "Adyghe", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "ady", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.806684783, "dataset_raw_mb": 9.034889002664338, "dataset_scaled_mb": 5.000810926, "dataset_tokens": 1395200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000810926, "tokenizer_training_raw_mb": 9.034889002664338, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1054531749150720.0, "train_compute_hours": 0.09970118355606808, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 75.48292%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.94295%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.57302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00111%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "bgp_latn_5mb": {"language_name": "Eastern Balochi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bgp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["bal"], "language_code_individuals": [], "language_byte_premium": 1.288416545, "dataset_raw_mb": 6.443282001157918, "dataset_scaled_mb": 5.000930814, "dataset_tokens": 1567232, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000930814, "tokenizer_training_raw_mb": 6.443282001157918, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "44178", "model_parameters": "35941376", "train_compute_flops": 1184696940625920.0, "train_compute_hours": 0.11200771075008699, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "quz_latn_5mb": {"language_name": "Cusco Quechua", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "quz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.297913163, "dataset_raw_mb": 6.4920318694783985, "dataset_scaled_mb": 5.001900015, "dataset_tokens": 1143808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001900015, "tokenizer_training_raw_mb": 6.4920318694783985, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48160", "model_parameters": "38038528", "train_compute_flops": 864746251223040.0, "train_compute_hours": 0.08175782738836015, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 99.99007%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.00993%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm)\n", "dataset_names": ["AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM"], "dataset_links": ["https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm"]}, "aym_latn_5mb": {"language_name": "Aymara", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "aym", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ayc", "ayr"], "language_byte_premium": 1.213786361, "dataset_raw_mb": 6.070938002076488, "dataset_scaled_mb": 5.001652842, "dataset_tokens": 1561088, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001652842, "tokenizer_training_raw_mb": 6.070938002076488, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1180048183787520.0, "train_compute_hours": 0.11156819192172918, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.19759%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 24.44245%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 7.18196%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.90026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.73814%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.53960%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/"]}, "tcy_knda_5mb": {"language_name": "Tulu", "language_script": "Kannada", "dataset_category": "5mb", "language_iso6393": "tcy", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.669467794, "dataset_raw_mb": 13.352463000083244, "dataset_scaled_mb": 5.001919495, "dataset_tokens": 1153024, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001919495, "tokenizer_training_raw_mb": 13.352463000083244, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 872416700006400.0, "train_compute_hours": 0.08248303345515055, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 55.60953%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 44.39047%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "sot_latn_5mb": {"language_name": "Southern Sotho", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sot", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.166011747, "dataset_raw_mb": 5.831627999083571, "dataset_scaled_mb": 5.001345839, "dataset_tokens": 1468928, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001345839, "tokenizer_training_raw_mb": 5.831627999083571, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1110316831211520.0, "train_compute_hours": 0.1049754094963619, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 34.03982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.19565%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.65729%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.10724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "iba_latn_5mb": {"language_name": "Iban", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "iba", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.298224214, "dataset_raw_mb": 6.493377997743985, "dataset_scaled_mb": 5.001738473, "dataset_tokens": 1380352, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001738473, "tokenizer_training_raw_mb": 6.493377997743985, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "36221", "model_parameters": "31747072", "train_compute_flops": 1044420703027200.0, "train_compute_hours": 0.09874523010438983, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98818%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.00622%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00559%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "dan_latn_5mb": {"language_name": "Danish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "dan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.021065783, "dataset_raw_mb": 5.1060420048794, "dataset_scaled_mb": 5.000698378, "dataset_tokens": 1085440, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000698378, "tokenizer_training_raw_mb": 5.1060420048794, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 821280374784000.0, "train_compute_hours": 0.07764832634321456, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "rus_cyrl_5mb": {"language_name": "Russian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "rus", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.822744411, "dataset_raw_mb": 9.115085004842348, "dataset_scaled_mb": 5.000747746, "dataset_tokens": 1175552, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000747746, "tokenizer_training_raw_mb": 9.115085004842348, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 889462141747200.0, "train_compute_hours": 0.08409460249246255, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lao_laoo_5mb": {"language_name": "Lao", "language_script": "Lao", "dataset_category": "5mb", "language_iso6393": "lao", "language_iso15924": "laoo", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.706914388, "dataset_raw_mb": 13.53849399908443, "dataset_scaled_mb": 5.001448904, "dataset_tokens": 1210368, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001448904, "tokenizer_training_raw_mb": 13.53849399908443, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 915805097164800.0, "train_compute_hours": 0.0865852091864902, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.18671%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.72107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.03480%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.64542%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 0.41139%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00061%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://dumps.wikimedia.org/"]}, "ctd_latn_5mb": {"language_name": "Tedim Chin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ctd", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.295581309, "dataset_raw_mb": 6.479488000211574, "dataset_scaled_mb": 5.001220653, "dataset_tokens": 1590272, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001220653, "tokenizer_training_raw_mb": 6.479488000211574, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "46306", "model_parameters": "36989952", "train_compute_flops": 1202555914813440.0, "train_compute_hours": 0.1136961955823616, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "san_deva_5mb": {"language_name": "Sanskrit", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "san", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 2.542780452, "dataset_raw_mb": 12.715943001507899, "dataset_scaled_mb": 5.000802563, "dataset_tokens": 1414656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000802563, "tokenizer_training_raw_mb": 12.715943001507899, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1069950125998080.0, "train_compute_hours": 0.10115892100345485, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 39.60135%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 37.07458%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.73656%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.30509%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 5.49767%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 0.78475%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "CCNet", "Hindialect", "Wortschatz Leipzig Data", "OSCAR", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://ebible.org/find/"]}, "abk_cyrl_5mb": {"language_name": "Abkhazian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "abk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.014757281, "dataset_raw_mb": 10.074583080324054, "dataset_scaled_mb": 5.00039542, "dataset_tokens": 1320448, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00039542, "tokenizer_training_raw_mb": 10.074583080324054, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 998669187809280.0, "train_compute_hours": 0.0944196323019683, "dataset_hugging_face": ["cis-lmu/Glot500", "Nart/abkhaz_text", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.32220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Nart/abkhaz](https://huggingface.co/datasets/Nart/abkhaz_text), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.67618%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Nart/abkhaz", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/Nart/abkhaz_text", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pck_latn_5mb": {"language_name": "Paite Chin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pck", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321489762, "dataset_raw_mb": 6.608029998554348, "dataset_scaled_mb": 5.000439798, "dataset_tokens": 1586688, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000439798, "tokenizer_training_raw_mb": 6.608029998554348, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48104", "model_parameters": "38038528", "train_compute_flops": 1200115317473280.0, "train_compute_hours": 0.11346544819747376, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "pon_latn_5mb": {"language_name": "Pohnpeian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pon", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.897776051, "dataset_raw_mb": 4.490514746078451, "dataset_scaled_mb": 5.0018206, "dataset_tokens": 1015808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.0018206, "tokenizer_training_raw_mb": 4.490514746078451, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "18219", "model_parameters": "22309888", "train_compute_flops": 768594463948800.0, "train_compute_hours": 0.07266711295515928, "dataset_hugging_face": [], "dataset_readme_str": "* 80.41176%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["eBible"], "dataset_links": ["https://ebible.org/find/"]}, "gsw_latn_5mb": {"language_name": "Swiss German", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "gsw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.142566025, "dataset_raw_mb": 5.71482200148376, "dataset_scaled_mb": 5.001743336, "dataset_tokens": 1583104, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001743336, "tokenizer_training_raw_mb": 5.71482200148376, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1197829678694400.0, "train_compute_hours": 0.11324935144019783, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.51719%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.37695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 11.72500%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 2.37834%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00253%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kpv_cyrl_5mb": {"language_name": "Komi-Zyrian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kpv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["kom"], "language_code_individuals": [], "language_byte_premium": 1.671354249, "dataset_raw_mb": 8.361366882539409, "dataset_scaled_mb": 5.002749649, "dataset_tokens": 1319936, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002749649, "tokenizer_training_raw_mb": 8.361366882539409, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 998010613923840.0, "train_compute_hours": 0.0943573671346176, "dataset_hugging_face": [], "dataset_readme_str": "* 99.99709%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00291%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Tatoeba"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "kik_latn_5mb": {"language_name": "Kikuyu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kik", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.292947561, "dataset_raw_mb": 6.464967740208354, "dataset_scaled_mb": 5.000177838, "dataset_tokens": 1467904, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000177838, "tokenizer_training_raw_mb": 6.464967740208354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1110239351930880.0, "train_compute_hours": 0.10496808418255593, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 56.84771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.17631%: [eBible](https://ebible.org/find/)\n* 3.33430%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.64167%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "ewe_latn_5mb": {"language_name": "Ewe", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ewe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078257744, "dataset_raw_mb": 5.392967000143835, "dataset_scaled_mb": 5.001556474, "dataset_tokens": 1519104, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001556474, "tokenizer_training_raw_mb": 5.392967000143835, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1148978992250880.0, "train_compute_hours": 0.10863074108553776, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 56.57066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.38723%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.00459%: [eBible](https://ebible.org/find/)\n* 3.36620%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.60581%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.06551%: [Ewe Language Corpus](https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Ewe Language Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus"]}, "nde_latn_5mb": {"language_name": "North Ndebele", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nde", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.969100135, "dataset_raw_mb": 4.846334320132731, "dataset_scaled_mb": 5.000860226, "dataset_tokens": 903168, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000860226, "tokenizer_training_raw_mb": 4.846334320132731, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 683367255244800.0, "train_compute_hours": 0.06460926776859928, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 65.86973%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MoT](https://github.com/bltlab/mot)\n* 34.13027%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CORP.NCHLT", "Mburisano_Covid", "MoT", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://github.com/bltlab/mot", "https://ebible.org/find/"]}, "tuk_latn_5mb": {"language_name": "Turkmen", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tuk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.78535505, "dataset_raw_mb": 8.929144998177096, "dataset_scaled_mb": 5.001327326, "dataset_tokens": 1615872, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001327326, "tokenizer_training_raw_mb": 8.929144998177096, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1222623048499200.0, "train_compute_hours": 0.1155934518581062, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 83.09655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.30174%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23979%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.35026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01166%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "ayr_latn_5mb": {"language_name": "Central Aymara", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ayr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aym"], "language_code_individuals": [], "language_byte_premium": 1.097680523, "dataset_raw_mb": 5.488695179789795, "dataset_scaled_mb": 5.00026653, "dataset_tokens": 1406464, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00026653, "tokenizer_training_raw_mb": 5.488695179789795, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1063751783546880.0, "train_compute_hours": 0.10057289589897775, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain"]}, "tum_latn_5mb": {"language_name": "Tumbuka", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tum", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.20719527, "dataset_raw_mb": 6.037582024735088, "dataset_scaled_mb": 5.001330087, "dataset_tokens": 1408512, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001330087, "tokenizer_training_raw_mb": 6.037582024735088, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1065301369159680.0, "train_compute_hours": 0.10071940217509703, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 82.53031%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05112%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.41857%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "vec_latn_5mb": {"language_name": "Venetian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "vec", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9963413972, "dataset_raw_mb": 4.98293000004677, "dataset_scaled_mb": 5.001227505, "dataset_tokens": 1401344, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001227505, "tokenizer_training_raw_mb": 4.98293000004677, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1059180505989120.0, "train_compute_hours": 0.1001407023844259, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.87927%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.55145%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.45405%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.10163%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01360%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cjk_latn_5mb": {"language_name": "Chokwe", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cjk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.170350075, "dataset_raw_mb": 5.853225699578943, "dataset_scaled_mb": 5.001260584, "dataset_tokens": 1538560, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001260584, "tokenizer_training_raw_mb": 5.853225699578943, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1163002742046720.0, "train_compute_hours": 0.10995662288441717, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "ltg_latn_5mb": {"language_name": "Latgalian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ltg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.002770005, "dataset_raw_mb": 5.0141089993787205, "dataset_scaled_mb": 5.000258259, "dataset_tokens": 1123840, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000258259, "tokenizer_training_raw_mb": 5.0141089993787205, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 849908968980480.0, "train_compute_hours": 0.08035502979451811, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 65.03249%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.87600%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.09151%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "ell_latn_5mb": {"language_name": "Modern Greek", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ell", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238882746, "dataset_raw_mb": 6.196606998458209, "dataset_scaled_mb": 5.00177036, "dataset_tokens": 1305600, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00177036, "tokenizer_training_raw_mb": 6.196606998458209, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 987163514634240.0, "train_compute_hours": 0.0933318232017827, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mri_latn_5mb": {"language_name": "Maori", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mri", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182543083, "dataset_raw_mb": 5.9144839980539015, "dataset_scaled_mb": 5.001495576, "dataset_tokens": 1563648, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001495576, "tokenizer_training_raw_mb": 5.9144839980539015, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1182411301847040.0, "train_compute_hours": 0.11179161399281107, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 42.28220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 39.00578%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 17.44951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.04465%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.21786%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "MC4", "NLLB_seed", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "dik_latn_5mb": {"language_name": "Southwestern Dinka", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "dik", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["din"], "language_code_individuals": [], "language_byte_premium": 1.123839426, "dataset_raw_mb": 5.6206672153624355, "dataset_scaled_mb": 5.001308092, "dataset_tokens": 1650688, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001308092, "tokenizer_training_raw_mb": 5.6206672153624355, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1248966003916800.0, "train_compute_hours": 0.11808405855213383, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 92.46146%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.53854%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "dar_cyrl_5mb": {"language_name": "Dargwa", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "dar", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.015493724, "dataset_raw_mb": 10.077549054323537, "dataset_scaled_mb": 5.000039908, "dataset_tokens": 1366528, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000039908, "tokenizer_training_raw_mb": 10.077549054323537, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1032837550571520.0, "train_compute_hours": 0.09765009569039826, "dataset_hugging_face": [], "dataset_readme_str": "* 100.00000%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["Languages of Russia"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download"]}, "lus_latn_5mb": {"language_name": "Lushai", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.168842547, "dataset_raw_mb": 5.845650998091768, "dataset_scaled_mb": 5.001230502, "dataset_tokens": 1528320, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001230502, "tokenizer_training_raw_mb": 5.845650998091768, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "47956", "model_parameters": "38038528", "train_compute_flops": 1155254813982720.0, "train_compute_hours": 0.10922409150382081, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 54.32199%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 39.88323%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.79478%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download"]}, "ina_latn_5mb": {"language_name": "Interlingua", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ina", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238452493, "dataset_raw_mb": 6.1923696047636225, "dataset_scaled_mb": 5.000086511, "dataset_tokens": 1511936, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000086511, "tokenizer_training_raw_mb": 6.1923696047636225, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1142858129080320.0, "train_compute_hours": 0.10805204129486663, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06085%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.53752%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.40163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lua_latn_5mb": {"language_name": "Luba-Lulua", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.185565668, "dataset_raw_mb": 5.929421250619171, "dataset_scaled_mb": 5.001343587, "dataset_tokens": 1572352, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001343587, "tokenizer_training_raw_mb": 5.929421250619171, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1189268218183680.0, "train_compute_hours": 0.11243990426463885, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "kum_cyrl_5mb": {"language_name": "Kumyk", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kum", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.964075989, "dataset_raw_mb": 9.824064997400614, "dataset_scaled_mb": 5.001876227, "dataset_tokens": 1034240, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001876227, "tokenizer_training_raw_mb": 9.824064997400614, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 782540734464000.0, "train_compute_hours": 0.07398566944023273, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 84.31257%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.68506%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00238%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "tat_cyrl_5mb": {"language_name": "Tatar", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "tat", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.854541191, "dataset_raw_mb": 9.273460999918717, "dataset_scaled_mb": 5.000407133, "dataset_tokens": 1224192, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000407133, "tokenizer_training_raw_mb": 9.273460999918717, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 925838664007680.0, "train_compute_hours": 0.08753383732436248, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 42.95952%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.38275%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.13842%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.01257%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.69512%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.81162%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "tgk_cyrl_5mb": {"language_name": "Tajik", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "tgk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.746052186, "dataset_raw_mb": 8.730359002259185, "dataset_scaled_mb": 5.000056168, "dataset_tokens": 1173504, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000056168, "tokenizer_training_raw_mb": 8.730359002259185, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 887912556134400.0, "train_compute_hours": 0.08394809621634328, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.68005%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.31925%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.47599%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.49619%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.02853%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "tpi_latn_5mb": {"language_name": "Tok Pisin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tpi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176877977, "dataset_raw_mb": 5.88620361519484, "dataset_scaled_mb": 5.001541137, "dataset_tokens": 1567744, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001541137, "tokenizer_training_raw_mb": 5.88620361519484, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "40601", "model_parameters": "33844224", "train_compute_flops": 1185510473072640.0, "train_compute_hours": 0.11208462654504961, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.07802%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.84121%: [eBible](https://ebible.org/find/)\n* 19.34190%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.73886%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Glot500", "BLOOM", "Earthlings", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lbe_cyrl_5mb": {"language_name": "Lak", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "lbe", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.012712204, "dataset_raw_mb": 10.06735807805591, "dataset_scaled_mb": 5.001886538, "dataset_tokens": 1228800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001886538, "tokenizer_training_raw_mb": 10.06735807805591, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 929751367680000.0, "train_compute_hours": 0.08790376567156365, "dataset_hugging_face": [], "dataset_readme_str": "* 99.11772%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.88228%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/"]}, "ckb_arab_5mb": {"language_name": "Central Kurdish", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "ckb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.651286029, "dataset_raw_mb": 8.258902997018295, "dataset_scaled_mb": 5.001497531, "dataset_tokens": 1190912, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001497531, "tokenizer_training_raw_mb": 8.258902997018295, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 900386720317440.0, "train_compute_hours": 0.08512747173910343, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.06661%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.31684%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.60071%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 15.36201%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.39332%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.26051%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "TICO", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kat_geor_5mb": {"language_name": "Georgian", "language_script": "Georgian", "dataset_category": "5mb", "language_iso6393": "kat", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 4.338956454, "dataset_raw_mb": 21.69571300219312, "dataset_scaled_mb": 5.000214506, "dataset_tokens": 1849344, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000214506, "tokenizer_training_raw_mb": 21.69571300219312, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1399275808358400.0, "train_compute_hours": 0.1322951673357033, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "allenai/nllb"], "dataset_readme_str": "* 60.04062%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.58420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.78020%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 7.59499%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb"]}, "ydd_hebr_5mb": {"language_name": "Eastern Yiddish", "language_script": "Hebrew", "dataset_category": "5mb", "language_iso6393": "ydd", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": ["yid"], "language_code_individuals": [], "language_byte_premium": 1.806848485, "dataset_raw_mb": 9.034698374180946, "dataset_scaled_mb": 5.000252345, "dataset_tokens": 1219584, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000252345, "tokenizer_training_raw_mb": 9.034698374180946, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 922080918896640.0, "train_compute_hours": 0.08717855960477325, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "fao_latn_5mb": {"language_name": "Faroese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fao", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155644359, "dataset_raw_mb": 5.779182001407584, "dataset_scaled_mb": 5.000830884, "dataset_tokens": 1259520, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000830884, "tokenizer_training_raw_mb": 5.779182001407584, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 952995151872000.0, "train_compute_hours": 0.09010135981335274, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 47.79193%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 42.53753%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.12327%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.54727%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "Ndc without informant codes", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "kac_latn_5mb": {"language_name": "Kachin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kac", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345469973, "dataset_raw_mb": 6.727627002214098, "dataset_scaled_mb": 5.000205978, "dataset_tokens": 1790976, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000205978, "tokenizer_training_raw_mb": 6.727627002214098, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "26984", "model_parameters": "27552768", "train_compute_flops": 1354415304867840.0, "train_compute_hours": 0.12805381064205035, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 57.50265%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 42.49735%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb"]}, "haw_latn_5mb": {"language_name": "Hawaiian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "haw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.114669379, "dataset_raw_mb": 5.57429299961683, "dataset_scaled_mb": 5.000848776, "dataset_tokens": 1701376, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000848776, "tokenizer_training_raw_mb": 5.57429299961683, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1286892111790080.0, "train_compute_hours": 0.12166979966015302, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.98608%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.94220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.78601%: [Ulukau](https://ulukau.org/index.php?l=en)\n* 1.02324%: [eBible](https://ebible.org/find/)\n* 0.26199%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00047%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "Ulukau", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ulukau.org/index.php?l=en", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "grc_grek_5mb": {"language_name": "Ancient Greek", "language_script": "Greek", "dataset_category": "5mb", "language_iso6393": "grc", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.76699504, "dataset_raw_mb": 8.836404999376517, "dataset_scaled_mb": 5.00080917, "dataset_tokens": 1236992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00080917, "tokenizer_training_raw_mb": 8.836404999376517, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 935949710131200.0, "train_compute_hours": 0.08848979077604073, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 97.28398%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.44105%: [eBible](https://ebible.org/find/)\n* 0.26854%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00642%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "bjn_latn_5mb": {"language_name": "Banjar", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bjn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.166394033, "dataset_raw_mb": 5.833438764728586, "dataset_scaled_mb": 5.001259094, "dataset_tokens": 1393152, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001259094, "tokenizer_training_raw_mb": 5.833438764728586, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1052982163537920.0, "train_compute_hours": 0.09955467727994881, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 90.26394%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.99946%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.53524%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12237%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.07900%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://ebible.org/find/"]}, "kmr_latn_5mb": {"language_name": "Northern Kurdish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kmr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.034619587, "dataset_raw_mb": 5.1747187596987985, "dataset_scaled_mb": 5.00156659, "dataset_tokens": 1204736, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00156659, "tokenizer_training_raw_mb": 5.1747187596987985, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 910420287160320.0, "train_compute_hours": 0.08607609987697572, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 98.85869%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.14131%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://tico-19.github.io/"]}, "nds_latn_5mb": {"language_name": "Low German", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nds", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.135906023, "dataset_raw_mb": 5.679654904499781, "dataset_scaled_mb": 5.000109859, "dataset_tokens": 1411072, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000109859, "tokenizer_training_raw_mb": 5.679654904499781, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1067664487219200.0, "train_compute_hours": 0.10094282424617891, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.35251%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.73725%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63050%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.27974%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ace_latn_5mb": {"language_name": "Achinese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ace", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241956888, "dataset_raw_mb": 6.2103571050791, "dataset_scaled_mb": 5.000461099, "dataset_tokens": 1594880, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000461099, "tokenizer_training_raw_mb": 6.2103571050791, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1206313659924480.0, "train_compute_hours": 0.11405147330195084, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 94.09064%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.78765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97639%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14531%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp"]}, "mrj_cyrl_5mb": {"language_name": "Western Mari", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mrj", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["chm"], "language_code_individuals": [], "language_byte_premium": 1.51492647, "dataset_raw_mb": 7.576543001017714, "dataset_scaled_mb": 5.001261217, "dataset_tokens": 1060352, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001261217, "tokenizer_training_raw_mb": 7.576543001017714, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 801871814983680.0, "train_compute_hours": 0.07581333523482066, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.44714%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90362%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.64924%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "msa_latn_5mb": {"language_name": "Malay", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "msa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bjn", "btj", "bve", "bvu", "coa", "dup", "hji", "ind", "jak", "jax", "kvb", "kvr", "kxd", "lce", "lcf", "liw", "max", "meo", "mfa", "mfb", "min", "mqg", "msi", "mui", "orn", "ors", "pel", "pse", "tmw", "urk", "vkk", "vkt", "xmm", "zlm", "zmi", "zsm"], "language_byte_premium": 1.285705434, "dataset_raw_mb": 6.430382000658593, "dataset_scaled_mb": 5.001442656, "dataset_tokens": 1240064, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001442656, "tokenizer_training_raw_mb": 6.430382000658593, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 937576775024640.0, "train_compute_hours": 0.08864362236596597, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 82.84912%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.03127%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.11781%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.00181%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "TICO", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "min_latn_5mb": {"language_name": "Minangkabau", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "min", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 0.949751483, "dataset_raw_mb": 4.749623999594888, "dataset_scaled_mb": 5.000912433, "dataset_tokens": 1248768, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000912433, "tokenizer_training_raw_mb": 4.749623999594888, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 944433691361280.0, "train_compute_hours": 0.08929191263779375, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.69385%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Minangkabau corpora](https://github.com/fajri91/minangNLP), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.18903%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 19.75253%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.74636%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.61822%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Minangkabau corpora", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/fajri91/minangNLP", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/indonlp"]}, "bak_latn_5mb": {"language_name": "Bashkir", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bak", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.19129188, "dataset_raw_mb": 5.95656481027071, "dataset_scaled_mb": 5.000088484, "dataset_tokens": 1210880, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000088484, "tokenizer_training_raw_mb": 5.95656481027071, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 915069043998720.0, "train_compute_hours": 0.08651561870533353, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "TIL", "Wikipedia Hugging Face", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "grn_latn_5mb": {"language_name": "Guarani", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "grn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gnw", "gug", "gui", "gun", "nhd"], "language_byte_premium": 0.992314188, "dataset_raw_mb": 4.962861000044739, "dataset_scaled_mb": 5.001300052, "dataset_tokens": 1328128, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001300052, "tokenizer_training_raw_mb": 4.962861000044739, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1004208956375040.0, "train_compute_hours": 0.0949433922390947, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.98971%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.31205%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.49453%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [GiossaMedia](https://github.com/sgongora27/giossa-gongora-guarani-2021), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.12356%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.05310%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.02706%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "GiossaMedia", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/sgongora27/giossa-gongora-guarani-2021", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "als_latn_5mb": {"language_name": "Tosk Albanian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "als", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["alb", "sqi"], "language_code_individuals": [], "language_byte_premium": 1.167247178, "dataset_raw_mb": 5.83723400189466, "dataset_scaled_mb": 5.000855099, "dataset_tokens": 1376256, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000855099, "tokenizer_training_raw_mb": 5.83723400189466, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1041321531801600.0, "train_compute_hours": 0.09845221755215128, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 85.38346%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.76858%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.84796%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "bxr_cyrl_5mb": {"language_name": "Russia Buriat", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bxr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["bua"], "language_code_individuals": [], "language_byte_premium": 1.588863716, "dataset_raw_mb": 7.9463235242239, "dataset_scaled_mb": 5.001261873, "dataset_tokens": 1266688, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001261873, "tokenizer_training_raw_mb": 7.9463235242239, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 957721387991040.0, "train_compute_hours": 0.09054820395551652, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.66194%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.33383%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00423%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kjh_cyrl_5mb": {"language_name": "Khakas", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kjh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.926386897, "dataset_raw_mb": 9.632874999095561, "dataset_scaled_mb": 5.000488227, "dataset_tokens": 1045504, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000488227, "tokenizer_training_raw_mb": 9.632874999095561, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 790366141808640.0, "train_compute_hours": 0.07472552613463507, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.38390%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 32.53557%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.07691%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00363%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Tatoeba", "MADLAD-400 (CommonCrawl)", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "snd_arab_5mb": {"language_name": "Sindhi", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "snd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.587969761, "dataset_raw_mb": 7.9403789995197425, "dataset_scaled_mb": 5.000333882, "dataset_tokens": 1313280, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000333882, "tokenizer_training_raw_mb": 7.9403789995197425, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "47015", "model_parameters": "36989952", "train_compute_flops": 992548324638720.0, "train_compute_hours": 0.09384093251129717, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.13852%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.07857%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.09349%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 4.06666%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.62277%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "quy_latn_5mb": {"language_name": "Ayacucho Quechua", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "quy", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.163741957, "dataset_raw_mb": 5.819685835162949, "dataset_scaled_mb": 5.000838717, "dataset_tokens": 1487360, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000838717, "tokenizer_training_raw_mb": 5.819685835162949, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1124263101726720.0, "train_compute_hours": 0.10629396598143535, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 86.82052%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.17948%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/"]}, "hne_deva_5mb": {"language_name": "Chhattisgarhi", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "hne", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.165410702, "dataset_raw_mb": 10.828288001979567, "dataset_scaled_mb": 5.000570096, "dataset_tokens": 1249280, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000570096, "tokenizer_training_raw_mb": 10.828288001979567, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 945247223808000.0, "train_compute_hours": 0.08936882843275637, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/MADLAD-400"], "dataset_readme_str": "* 77.35712%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.41578%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md)\n* 7.22709%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Hindialect", "NLLB_seed", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "srn_latn_5mb": {"language_name": "Sranan Tongo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "srn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.060725469, "dataset_raw_mb": 5.304270999578943, "dataset_scaled_mb": 5.000606806, "dataset_tokens": 1371648, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000606806, "tokenizer_training_raw_mb": 5.304270999578943, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "15860", "model_parameters": "21261312", "train_compute_flops": 1037408828129280.0, "train_compute_hours": 0.09808228920495012, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.32395%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.83517%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.18414%: [eBible](https://ebible.org/find/)\n* 1.65674%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "arg_latn_5mb": {"language_name": "Aragonese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "arg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.192925647, "dataset_raw_mb": 5.965869749701528, "dataset_scaled_mb": 5.001040731, "dataset_tokens": 1515520, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001040731, "tokenizer_training_raw_mb": 5.965869749701528, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1146693353472000.0, "train_compute_hours": 0.10841464432826182, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.30879%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 38.24094%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.44878%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00149%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mon_cyrl_5mb": {"language_name": "Mongolian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mon", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.783772557, "dataset_raw_mb": 8.923201999041021, "dataset_scaled_mb": 5.002432605, "dataset_tokens": 1102336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002432605, "tokenizer_training_raw_mb": 8.923201999041021, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 832941006520320.0, "train_compute_hours": 0.07875078607101207, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 63.59654%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.77330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 11.63016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mag_deva_5mb": {"language_name": "Magahi", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "mag", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.555403846, "dataset_raw_mb": 12.781099997843542, "dataset_scaled_mb": 5.001596917, "dataset_tokens": 1400832, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001596917, "tokenizer_training_raw_mb": 12.781099997843542, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1059916559155200.0, "train_compute_hours": 0.10021029286558256, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 77.72701%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.27299%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "fil_latn_5mb": {"language_name": "Filipino", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.328161865, "dataset_raw_mb": 6.640921002162257, "dataset_scaled_mb": 5.000084084, "dataset_tokens": 1441280, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000084084, "tokenizer_training_raw_mb": 6.640921002162257, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1090094738964480.0, "train_compute_hours": 0.1030635025930054, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "roh_latn_5mb": {"language_name": "Romansh", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "roh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.273368995, "dataset_raw_mb": 6.367954001719137, "dataset_scaled_mb": 5.000870939, "dataset_tokens": 1427968, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000870939, "tokenizer_training_raw_mb": 6.367954001719137, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1079325118955520.0, "train_compute_hours": 0.10204528397397644, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.92821%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71848%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.35087%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00179%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00065%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "azj_latn_5mb": {"language_name": "North Azerbaijani", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "azj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.076115956, "dataset_raw_mb": 5.383168040107863, "dataset_scaled_mb": 5.002405187, "dataset_tokens": 959488, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002405187, "tokenizer_training_raw_mb": 5.383168040107863, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 725283546071040.0, "train_compute_hours": 0.0685722625376256, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "zul_latn_5mb": {"language_name": "Zulu", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "zul", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.163840155, "dataset_raw_mb": 5.821410998233877, "dataset_scaled_mb": 5.001899078, "dataset_tokens": 1380864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001899078, "tokenizer_training_raw_mb": 5.821410998233877, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1043684649861120.0, "train_compute_hours": 0.09867563962323317, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 46.89015%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [isiZulu](https://zenodo.org/record/5035171#.YaippvHMJDZ), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 34.71066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.29050%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.78489%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.32380%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "isiZulu", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "TeDDi", "TICO", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://zenodo.org/record/5035171#.YaippvHMJDZ", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "kaz_cyrl_5mb": {"language_name": "Kazakh", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kaz", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.764638392, "dataset_raw_mb": 8.824331990516043, "dataset_scaled_mb": 5.000646042, "dataset_tokens": 1064448, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000646042, "tokenizer_training_raw_mb": 8.824331990516043, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 804970986209280.0, "train_compute_hours": 0.0761063477870592, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 68.35707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.84524%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.79769%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "W2C", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb"]}, "heb_hebr_5mb": {"language_name": "Hebrew", "language_script": "Hebrew", "dataset_category": "5mb", "language_iso6393": "heb", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.355477183, "dataset_raw_mb": 6.777524000171587, "dataset_scaled_mb": 5.000101872, "dataset_tokens": 1029120, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000101872, "tokenizer_training_raw_mb": 6.777524000171587, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 777969456906240.0, "train_compute_hours": 0.07355347592568087, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ind_latn_5mb": {"language_name": "Indonesian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ind", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.178746238, "dataset_raw_mb": 5.894402000339091, "dataset_scaled_mb": 5.000569088, "dataset_tokens": 1107456, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000569088, "tokenizer_training_raw_mb": 5.894402000339091, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48504", "model_parameters": "38038528", "train_compute_flops": 837512284078080.0, "train_compute_hours": 0.07918297958556393, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "vep_latn_5mb": {"language_name": "Veps", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "vep", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173646093, "dataset_raw_mb": 5.869815961111724, "dataset_scaled_mb": 5.001350915, "dataset_tokens": 1290752, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001350915, "tokenizer_training_raw_mb": 5.869815961111724, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 975502882897920.0, "train_compute_hours": 0.09222936347398517, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 54.55117%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 45.44728%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00154%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "guj_gujr_5mb": {"language_name": "Gujarati", "language_script": "Gujarati", "dataset_category": "5mb", "language_iso6393": "guj", "language_iso15924": "gujr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.162862919, "dataset_raw_mb": 10.818152009113915, "dataset_scaled_mb": 5.001774229, "dataset_tokens": 1037312, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001774229, "tokenizer_training_raw_mb": 10.818152009113915, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 784167799357440.0, "train_compute_hours": 0.07413950103015797, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 86.12642%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 13.58996%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.28351%: [eBible](https://ebible.org/find/)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "XLSum", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "arb_arab_5mb": {"language_name": "Standard Arabic", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "arb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.465017978, "dataset_raw_mb": 7.325713000236512, "dataset_scaled_mb": 5.000425326, "dataset_tokens": 1050624, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000425326, "tokenizer_training_raw_mb": 7.325713000236512, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 794937419366400.0, "train_compute_hours": 0.07515771964918691, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "guj_latn_5mb": {"language_name": "Gujarati", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "guj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.190454087, "dataset_raw_mb": 5.95282799964755, "dataset_scaled_mb": 5.000468363, "dataset_tokens": 1281024, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000468363, "tokenizer_training_raw_mb": 5.95282799964755, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 968568487280640.0, "train_compute_hours": 0.09157374788835143, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400"], "dataset_readme_str": "* 99.66021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.33979%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "W2C", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "bos_latn_5mb": {"language_name": "Bosnian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.9699882939, "dataset_raw_mb": 4.850695009756057, "dataset_scaled_mb": 5.000776855, "dataset_tokens": 1200128, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000776855, "tokenizer_training_raw_mb": 4.850695009756057, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 908057169100800.0, "train_compute_hours": 0.08585267780589383, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 70.89860%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.71791%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.38348%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tel_telu_5mb": {"language_name": "Telugu", "language_script": "Telugu", "dataset_category": "5mb", "language_iso6393": "tel", "language_iso15924": "telu", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.619819498, "dataset_raw_mb": 13.099738984241919, "dataset_scaled_mb": 5.000244862, "dataset_tokens": 1104384, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000244862, "tokenizer_training_raw_mb": 13.099738984241919, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 834490592133120.0, "train_compute_hours": 0.07889729234713136, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.10762%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.59426%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29812%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "bcl_latn_5mb": {"language_name": "Central Bikol", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bcl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["bik"], "language_code_individuals": [], "language_byte_premium": 1.218848876, "dataset_raw_mb": 6.094662285495265, "dataset_scaled_mb": 5.000342869, "dataset_tokens": 1420800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000342869, "tokenizer_training_raw_mb": 6.094662285495265, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1074598882836480.0, "train_compute_hours": 0.10159843983181266, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 54.73555%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 45.26445%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "por_latn_5mb": {"language_name": "Portuguese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "por", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.097878434, "dataset_raw_mb": 5.489954999574947, "dataset_scaled_mb": 5.000512652, "dataset_tokens": 1167360, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000512652, "tokenizer_training_raw_mb": 5.489954999574947, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 883263799296000.0, "train_compute_hours": 0.08350857738798546, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mwl_latn_5mb": {"language_name": "Mirandese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "mwl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241381732, "dataset_raw_mb": 6.20723748960699, "dataset_scaled_mb": 5.00026489, "dataset_tokens": 1428992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00026489, "tokenizer_training_raw_mb": 6.20723748960699, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1080797225287680.0, "train_compute_hours": 0.10218446493628976, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 63.81709%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 36.18263%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "che_cyrl_5mb": {"language_name": "Chechen", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "che", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.831603287, "dataset_raw_mb": 9.159089998129419, "dataset_scaled_mb": 5.000586133, "dataset_tokens": 1424896, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000586133, "tokenizer_training_raw_mb": 9.159089998129419, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1077698054062080.0, "train_compute_hours": 0.10189145238405121, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.55150%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.11809%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.84904%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.36799%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.11311%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00027%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tyv_cyrl_5mb": {"language_name": "Tuvinian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "tyv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.860256075, "dataset_raw_mb": 9.302678999228707, "dataset_scaled_mb": 5.000751845, "dataset_tokens": 1196032, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000751845, "tokenizer_training_raw_mb": 9.302678999228707, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 904957997875200.0, "train_compute_hours": 0.08555966525365528, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.97941%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 35.15487%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.54558%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.31541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00397%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00076%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_cyrl_5mb": {"language_name": "Azerbaijani", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "aze", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.818803281, "dataset_raw_mb": 9.095260997534762, "dataset_scaled_mb": 5.000684292, "dataset_tokens": 1100288, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000684292, "tokenizer_training_raw_mb": 9.095260997534762, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 831391420907520.0, "train_compute_hours": 0.0786042797948928, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ngu_latn_5mb": {"language_name": "Guerrero Nahuatl", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ngu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.435018628, "dataset_raw_mb": 7.176507999486313, "dataset_scaled_mb": 5.000985952, "dataset_tokens": 1273856, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000985952, "tokenizer_training_raw_mb": 7.176507999486313, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 963842251161600.0, "train_compute_hours": 0.09112690374618765, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 80.79780%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.69637%: [eBible](https://ebible.org/find/)\n* 9.49744%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00839%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "sco_latn_5mb": {"language_name": "Scots", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sco", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.193629502, "dataset_raw_mb": 5.969906494953846, "dataset_scaled_mb": 5.001473644, "dataset_tokens": 1531904, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001473644, "tokenizer_training_raw_mb": 5.969906494953846, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1159090038374400.0, "train_compute_hours": 0.109586694537216, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 95.84087%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.15532%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00381%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "ita_latn_5mb": {"language_name": "Italian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ita", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066886932, "dataset_raw_mb": 5.3364759998470985, "dataset_scaled_mb": 5.001913361, "dataset_tokens": 1134080, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001913361, "tokenizer_training_raw_mb": 5.3364759998470985, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 857656897044480.0, "train_compute_hours": 0.08108756117511448, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lin_latn_5mb": {"language_name": "Lingala", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.139208008, "dataset_raw_mb": 5.696413001892115, "dataset_scaled_mb": 5.000327387, "dataset_tokens": 1453056, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000327387, "tokenizer_training_raw_mb": 5.696413001892115, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1098733678755840.0, "train_compute_hours": 0.10388027508237034, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 49.58207%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.44683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.44468%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Lingala_Song_Lyrics](https://github.com/espoirMur/songs_lyrics_webscrap), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.78539%: [eBible](https://ebible.org/find/)\n* 1.74103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "Lingala_Song_Lyrics", "MoT", "TICO", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/espoirMur/songs_lyrics_webscrap", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "kon_latn_5mb": {"language_name": "Kongo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["kng", "kwy", "ldi"], "language_byte_premium": 1.230994587, "dataset_raw_mb": 6.1559580002574155, "dataset_scaled_mb": 5.000800219, "dataset_tokens": 1603072, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000800219, "tokenizer_training_raw_mb": 6.1559580002574155, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1212512002375680.0, "train_compute_hours": 0.11463749840642794, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.64648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.58706%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.42232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.34414%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "zsm_latn_5mb": {"language_name": "Standard Malay", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "zsm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.143822408, "dataset_raw_mb": 5.7197640153410925, "dataset_scaled_mb": 5.000569997, "dataset_tokens": 1136640, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000569997, "tokenizer_training_raw_mb": 5.7197640153410925, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "47973", "model_parameters": "38038528", "train_compute_flops": 860020015104000.0, "train_compute_hours": 0.08131098324619637, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 91.23035%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.76965%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "pag_latn_5mb": {"language_name": "Pangasinan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.043908443, "dataset_raw_mb": 5.220733380299696, "dataset_scaled_mb": 5.001141063, "dataset_tokens": 1487360, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001141063, "tokenizer_training_raw_mb": 5.220733380299696, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1124263101726720.0, "train_compute_hours": 0.10629396598143535, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.89102%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.15082%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.95815%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "ava_cyrl_5mb": {"language_name": "Avaric", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "ava", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.938794023, "dataset_raw_mb": 9.69476999937248, "dataset_scaled_mb": 5.000412568, "dataset_tokens": 1340416, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000412568, "tokenizer_training_raw_mb": 9.69476999937248, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1013506470051840.0, "train_compute_hours": 0.09582242989581033, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.59506%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.75473%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 6.23003%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.42018%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "knc_latn_5mb": {"language_name": "Central Kanuri", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "knc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 1.176891381, "dataset_raw_mb": 5.884728579783281, "dataset_scaled_mb": 5.000230841, "dataset_tokens": 1741824, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000230841, "tokenizer_training_raw_mb": 5.884728579783281, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1317225250160640.0, "train_compute_hours": 0.12453766001518779, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "glv_latn_5mb": {"language_name": "Manx", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "glv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.224310206, "dataset_raw_mb": 6.121787002324794, "dataset_scaled_mb": 5.000192739, "dataset_tokens": 1482240, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000192739, "tokenizer_training_raw_mb": 6.121787002324794, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1121086451220480.0, "train_compute_hours": 0.10599362811539084, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 42.26375%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 41.05658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 16.67753%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00213%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tir_ethi_5mb": {"language_name": "Tigrinya", "language_script": "Ge'ez", "dataset_category": "5mb", "language_iso6393": "tir", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.762985532, "dataset_raw_mb": 8.817299998562325, "dataset_scaled_mb": 5.001345637, "dataset_tokens": 1187328, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001345637, "tokenizer_training_raw_mb": 8.817299998562325, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 897946122977280.0, "train_compute_hours": 0.08489672435421557, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 50.52196%: [Tigrinya Language Modeling Dataset](https://zenodo.org/record/5139094)\n* 20.99445%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.46805%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.08565%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 4.86709%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.06280%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Tigrinya Language Modeling Dataset", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://zenodo.org/record/5139094", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "kal_latn_5mb": {"language_name": "Kalaallisut", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.341175059, "dataset_raw_mb": 6.708482997996566, "dataset_scaled_mb": 5.001944342, "dataset_tokens": 1158144, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001944342, "tokenizer_training_raw_mb": 6.708482997996566, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 875593350512640.0, "train_compute_hours": 0.08278337132119507, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.53226%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.83885%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.54939%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.07923%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "cfm_latn_5mb": {"language_name": "Falam Chin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cfm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323887964, "dataset_raw_mb": 6.6200999992393434, "dataset_scaled_mb": 5.000498667, "dataset_tokens": 1579520, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000498667, "tokenizer_training_raw_mb": 6.6200999992393434, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "36222", "model_parameters": "31747072", "train_compute_flops": 1193994454302720.0, "train_compute_hours": 0.11288674840680263, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ssw_latn_5mb": {"language_name": "Swati", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ssw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141426619, "dataset_raw_mb": 5.70812199734561, "dataset_scaled_mb": 5.000866374, "dataset_tokens": 1381888, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000866374, "tokenizer_training_raw_mb": 5.70812199734561, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1045156756193280.0, "train_compute_hours": 0.09881482058554648, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.30477%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.07695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 14.60799%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.01028%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "bam_latn_5mb": {"language_name": "Bambara", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.256878532, "dataset_raw_mb": 6.2848049978556, "dataset_scaled_mb": 5.000328065, "dataset_tokens": 1792512, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000328065, "tokenizer_training_raw_mb": 6.2848049978556, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1355151358033920.0, "train_compute_hours": 0.128123401123207, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 67.90603%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.26277%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.19633%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.75328%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.88159%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "sag_latn_5mb": {"language_name": "Sango", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.162464677, "dataset_raw_mb": 5.813640001792887, "dataset_scaled_mb": 5.001132608, "dataset_tokens": 1600000, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001132608, "tokenizer_training_raw_mb": 5.813640001792887, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "35609", "model_parameters": "31747072", "train_compute_flops": 1209490310430720.0, "train_compute_hours": 0.11435181116799535, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 60.53835%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.27653%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93802%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.24653%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00058%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "hrv_latn_5mb": {"language_name": "Croatian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hrv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.989672823, "dataset_raw_mb": 4.949449000286609, "dataset_scaled_mb": 5.001096206, "dataset_tokens": 1167872, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001096206, "tokenizer_training_raw_mb": 4.949449000286609, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 882527746129920.0, "train_compute_hours": 0.08343898690682881, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.01986%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.11618%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.57132%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.27830%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.96930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "gom_latn_5mb": {"language_name": "Goan Konkani", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "gom", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.2133745, "dataset_raw_mb": 6.067272999734589, "dataset_scaled_mb": 5.000330071, "dataset_tokens": 1382400, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000330071, "tokenizer_training_raw_mb": 6.067272999734589, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1045970288640000.0, "train_compute_hours": 0.0988917363805091, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43524%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 44.56476%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "xho_latn_5mb": {"language_name": "Xhosa", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "xho", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.198781752, "dataset_raw_mb": 5.9960710014473, "dataset_scaled_mb": 5.001803699, "dataset_tokens": 1416704, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001803699, "tokenizer_training_raw_mb": 5.9960710014473, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1071499711610880.0, "train_compute_hours": 0.10130542727957412, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 41.02036%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.08328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.57971%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536)\n* 0.21721%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09945%: [Lacuna Project: IsiXhosa](https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "CORP.NCHLT", "Mburisano_Covid", "Wikipedia 2023/08", "Lacuna Project: IsiXhosa"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://dumps.wikimedia.org/", "https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho"]}, "rue_cyrl_5mb": {"language_name": "Rusyn", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "rue", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.560953869, "dataset_raw_mb": 7.804960799113941, "dataset_scaled_mb": 5.000122652, "dataset_tokens": 1158144, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000122652, "tokenizer_training_raw_mb": 7.804960799113941, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 875593350512640.0, "train_compute_hours": 0.08278337132119507, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 51.53918%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 42.05551%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.38884%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.01646%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "BLOOM", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "szl_latn_5mb": {"language_name": "Silesian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "szl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066563024, "dataset_raw_mb": 5.332926115080781, "dataset_scaled_mb": 5.000104068, "dataset_tokens": 1477120, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000104068, "tokenizer_training_raw_mb": 5.332926115080781, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1116515173662720.0, "train_compute_hours": 0.10556143460083899, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 39.46050%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.03961%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 26.49989%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "khk_cyrl_5mb": {"language_name": "Halh Mongolian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "khk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["mon"], "language_code_individuals": [], "language_byte_premium": 1.801810967, "dataset_raw_mb": 9.00997558562949, "dataset_scaled_mb": 5.000511014, "dataset_tokens": 1152512, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000511014, "tokenizer_training_raw_mb": 9.00997558562949, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 871603167559680.0, "train_compute_hours": 0.08240611766018793, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "tzo_latn_5mb": {"language_name": "Tzotzil", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tzo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.487549999, "dataset_raw_mb": 7.437858000055227, "dataset_scaled_mb": 5.000072606, "dataset_tokens": 1944576, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000072606, "tokenizer_training_raw_mb": 7.437858000055227, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "32229", "model_parameters": "29649920", "train_compute_flops": 1470634225827840.0, "train_compute_hours": 0.1390417813509958, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 50.76388%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.89747%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "bua_cyrl_5mb": {"language_name": "Buriat", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bua", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bxm", "bxr", "bxu"], "language_byte_premium": 1.701900789, "dataset_raw_mb": 8.513030002621177, "dataset_scaled_mb": 5.002071835, "dataset_tokens": 1199104, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002071835, "tokenizer_training_raw_mb": 8.513030002621177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 906585062768640.0, "train_compute_hours": 0.08571349684358051, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 89.97116%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 8.41374%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.21634%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.38769%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00722%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00386%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Wortschatz Leipzig Data", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pms_latn_5mb": {"language_name": "Piemontese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pms", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22629916, "dataset_raw_mb": 6.132252305178103, "dataset_scaled_mb": 5.000616901, "dataset_tokens": 1628672, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000616901, "tokenizer_training_raw_mb": 6.132252305178103, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1231184509009920.0, "train_compute_hours": 0.11640289903366517, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.69647%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.63180%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04052%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lug_latn_5mb": {"language_name": "Ganda", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.217417428, "dataset_raw_mb": 6.089293000551307, "dataset_scaled_mb": 5.001811918, "dataset_tokens": 1493504, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001811918, "tokenizer_training_raw_mb": 6.089293000551307, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1128911858565120.0, "train_compute_hours": 0.10673348480979317, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 40.49929%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.20499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Makerere MT Corpus](https://zenodo.org/record/5089560#.Y00i3uxBw-S), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.27645%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.29489%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.85655%: [eBible](https://ebible.org/find/)\n* 0.49599%: [Makerere Radio Corpus](https://zenodo.org/record/5855017)\n* 0.37185%: [Makerere Parallel Corpus](https://zenodo.org/record/4764039)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CC100", "Wortschatz Leipzig Data", "Makerere MT Corpus", "TICO", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible", "Makerere Radio Corpus", "Makerere Parallel Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://zenodo.org/record/5089560#.Y00i3uxBw-S", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://zenodo.org/record/5855017", "https://zenodo.org/record/4764039"]}, "wuu_hani_5mb": {"language_name": "Wu Chinese", "language_script": "Han", "dataset_category": "5mb", "language_iso6393": "wuu", "language_iso15924": "hani", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 3.5000859998999996, "dataset_scaled_mb": 5.000122857, "dataset_tokens": 867840, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000122857, "tokenizer_training_raw_mb": 3.5000859998999996, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 656210767380480.0, "train_compute_hours": 0.062041745279609026, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 62.28265%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 37.41809%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29927%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "lij_latn_5mb": {"language_name": "Ligurian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.14375901, "dataset_raw_mb": 5.720212165125871, "dataset_scaled_mb": 5.001238998, "dataset_tokens": 1587200, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001238998, "tokenizer_training_raw_mb": 5.720212165125871, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1200928849920000.0, "train_compute_hours": 0.11354236399243638, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 58.41627%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.25415%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 20.32958%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "apc_arab_5mb": {"language_name": "Levantine Arabic", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "apc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.473448613, "dataset_raw_mb": 7.369080999122948, "dataset_scaled_mb": 5.001247369, "dataset_tokens": 1219072, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001247369, "tokenizer_training_raw_mb": 7.369080999122948, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 921267386449920.0, "train_compute_hours": 0.08710164380981063, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.99983%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [QADI](https://alt.qcri.org/resources/qadi), [Shami](https://github.com/GU-CLASP/shami-corpus/tree/master/Data), [Tatoeba](https://tatoeba.org/en/)\n* 0.00017%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "ADD", "AraBench", "DART", "Habibi", "QADI", "Shami", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://alt.qcri.org/resources/qadi", "https://github.com/GU-CLASP/shami-corpus/tree/master/Data", "https://tatoeba.org/en/"]}, "cak_latn_5mb": {"language_name": "Kaqchikel", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cak", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.82231949, "dataset_raw_mb": 9.112865800765915, "dataset_scaled_mb": 5.000696009, "dataset_tokens": 2454016, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000696009, "tokenizer_training_raw_mb": 9.112865800765915, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "25504", "model_parameters": "26504192", "train_compute_flops": 1855667510968320.0, "train_compute_hours": 0.1754449283097321, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.21517%: [eBible](https://ebible.org/find/)\n* 39.78483%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/)\n", "dataset_names": ["eBible", "Glot500", "BLOOM", "Earthlings"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/"]}, "vie_latn_5mb": {"language_name": "Vietnamese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "vie", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.349331266, "dataset_raw_mb": 6.7483499944131085, "dataset_scaled_mb": 5.001255188, "dataset_tokens": 1358336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001255188, "tokenizer_training_raw_mb": 6.7483499944131085, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "25682", "model_parameters": "26504192", "train_compute_flops": 1026639208120320.0, "train_compute_hours": 0.09706407058592117, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "diq_latn_5mb": {"language_name": "Dimli", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "diq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["zza"], "language_code_individuals": [], "language_byte_premium": 0.9584231474, "dataset_raw_mb": 4.793311206944447, "dataset_scaled_mb": 5.00124733, "dataset_tokens": 1369088, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00124733, "tokenizer_training_raw_mb": 4.793311206944447, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1035200668631040.0, "train_compute_hours": 0.09787351776148015, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.10048%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 30.14203%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 17.75749%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kan_knda_5mb": {"language_name": "Kannada", "language_script": "Kannada", "dataset_category": "5mb", "language_iso6393": "kan", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.641985282, "dataset_raw_mb": 13.214736001538675, "dataset_scaled_mb": 5.001820446, "dataset_tokens": 1132544, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001820446, "tokenizer_training_raw_mb": 13.214736001538675, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 856920843878400.0, "train_compute_hours": 0.08101797069395783, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 49.18479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 40.78353%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.59325%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.16622%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.00209%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.27013%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "TeDDi", "W2C", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "som_latn_5mb": {"language_name": "Somali", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "som", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.42230286, "dataset_raw_mb": 7.113472998782198, "dataset_scaled_mb": 5.001377132, "dataset_tokens": 1582592, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001377132, "tokenizer_training_raw_mb": 7.113472998782198, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1197016146247680.0, "train_compute_hours": 0.11317243564523521, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 38.68291%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 30.77395%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.30160%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TICO](https://tico-19.github.io/)\n* 7.64665%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.41754%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.17735%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "Wortschatz Leipzig Data", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nap_latn_5mb": {"language_name": "Neapolitan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23163545, "dataset_raw_mb": 6.1603133689652285, "dataset_scaled_mb": 5.001734376, "dataset_tokens": 1590784, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001734376, "tokenizer_training_raw_mb": 6.1603133689652285, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1203214488698880.0, "train_compute_hours": 0.1137584607497123, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 62.19815%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.80185%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "sin_sinh_5mb": {"language_name": "Sinhala", "language_script": "Sinhala", "dataset_category": "5mb", "language_iso6393": "sin", "language_iso15924": "sinh", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.446318518, "dataset_raw_mb": 12.235098998630908, "dataset_scaled_mb": 5.001433341, "dataset_tokens": 1219584, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001433341, "tokenizer_training_raw_mb": 12.235098998630908, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 922080918896640.0, "train_compute_hours": 0.08717855960477325, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.49015%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.08102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.71980%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.70904%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "smo_latn_5mb": {"language_name": "Samoan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "smo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.178001518, "dataset_raw_mb": 5.891045000570832, "dataset_scaled_mb": 5.000880653, "dataset_tokens": 1656832, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000880653, "tokenizer_training_raw_mb": 5.891045000570832, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1253614760755200.0, "train_compute_hours": 0.11852357738049164, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 43.50048%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.24768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.13951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.11208%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00025%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "pol_latn_5mb": {"language_name": "Polish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "pol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.077373477, "dataset_raw_mb": 5.387587004762855, "dataset_scaled_mb": 5.000667939, "dataset_tokens": 1149440, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000667939, "tokenizer_training_raw_mb": 5.387587004762855, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 868581475614720.0, "train_compute_hours": 0.08212043042175535, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "krc_cyrl_5mb": {"language_name": "Karachay-Balkar", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "krc", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.865302377, "dataset_raw_mb": 9.327455000534835, "dataset_scaled_mb": 5.00050561, "dataset_tokens": 1133056, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00050561, "tokenizer_training_raw_mb": 9.327455000534835, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 856184790712320.0, "train_compute_hours": 0.08094838021280117, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.46882%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.41364%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 11.40588%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.24320%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.46452%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00394%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nzi_latn_5mb": {"language_name": "Nzima", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nzi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.418370521, "dataset_raw_mb": 7.093335002184688, "dataset_scaled_mb": 5.001045141, "dataset_tokens": 1508864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001045141, "tokenizer_training_raw_mb": 7.093335002184688, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "20430", "model_parameters": "23358464", "train_compute_flops": 1141231064186880.0, "train_compute_hours": 0.10789820970494139, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ban_latn_5mb": {"language_name": "Balinese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ban", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.269543636, "dataset_raw_mb": 6.348595514633725, "dataset_scaled_mb": 5.000691063, "dataset_tokens": 1504768, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000691063, "tokenizer_training_raw_mb": 6.348595514633725, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1138131892961280.0, "train_compute_hours": 0.10760519715270285, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 70.01948%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.06831%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.61178%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md)\n* 0.30043%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/indonlp"]}, "syr_syrc_5mb": {"language_name": "Syriac", "language_script": "Syriac", "dataset_category": "5mb", "language_iso6393": "syr", "language_iso15924": "syrc", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aii", "cld"], "language_byte_premium": 1.410191627, "dataset_raw_mb": 7.05150400184632, "dataset_scaled_mb": 5.000387087, "dataset_tokens": 1134592, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000387087, "tokenizer_training_raw_mb": 7.05150400184632, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 858470429491200.0, "train_compute_hours": 0.0811644769700771, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "yua_latn_5mb": {"language_name": "Yucateco", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "yua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241346656, "dataset_raw_mb": 6.208244999405756, "dataset_scaled_mb": 5.001217806, "dataset_tokens": 1629696, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001217806, "tokenizer_training_raw_mb": 6.208244999405756, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "41277", "model_parameters": "34892800", "train_compute_flops": 1232656615342080.0, "train_compute_hours": 0.11654207999597849, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "chm_cyrl_5mb": {"language_name": "Mari (Russia)", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "chm", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["mhr", "mrj"], "language_byte_premium": 1.756255296, "dataset_raw_mb": 8.78144299934589, "dataset_scaled_mb": 5.000094815, "dataset_tokens": 1194496, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000094815, "tokenizer_training_raw_mb": 8.78144299934589, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 902672359096320.0, "train_compute_hours": 0.08534356849637935, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 73.38471%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 12.77683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.02224%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.09681%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.65747%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.06194%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "khm_khmr_5mb": {"language_name": "Central Khmer", "language_script": "Khmer", "dataset_category": "5mb", "language_iso6393": "khm", "language_iso15924": "khmr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.903484241, "dataset_raw_mb": 19.52155399942274, "dataset_scaled_mb": 5.001058745, "dataset_tokens": 1639424, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001058745, "tokenizer_training_raw_mb": 19.52155399942274, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1239745969520640.0, "train_compute_hours": 0.11721234620922416, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 55.59600%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.62172%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.83408%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.36741%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/)\n* 1.58079%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "ukr_cyrl_5mb": {"language_name": "Ukrainian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "ukr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.751370357, "dataset_raw_mb": 8.759057000217203, "dataset_scaled_mb": 5.001259137, "dataset_tokens": 1143296, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001259137, "tokenizer_training_raw_mb": 8.759057000217203, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 863932718776320.0, "train_compute_hours": 0.08168091159339753, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cnh_latn_5mb": {"language_name": "Hakha Chin", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cnh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323849924, "dataset_raw_mb": 6.620796999335967, "dataset_scaled_mb": 5.001168848, "dataset_tokens": 1628160, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001168848, "tokenizer_training_raw_mb": 6.620796999335967, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "34232", "model_parameters": "30698496", "train_compute_flops": 1231920562176000.0, "train_compute_hours": 0.11647248951482182, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "sat_olck_5mb": {"language_name": "Santali", "language_script": "Ol Chiki", "dataset_category": "5mb", "language_iso6393": "sat", "language_iso15924": "olck", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.803663545, "dataset_raw_mb": 14.018949906270782, "dataset_scaled_mb": 5.000225484, "dataset_tokens": 1335296, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000225484, "tokenizer_training_raw_mb": 14.018949906270782, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1010329819545600.0, "train_compute_hours": 0.09552209202976583, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 52.13518%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 41.86472%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.00009%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "BLOOM", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb"]}, "tiv_latn_5mb": {"language_name": "Tiv", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tiv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.309214276, "dataset_raw_mb": 6.547727999520994, "dataset_scaled_mb": 5.001265354, "dataset_tokens": 1693696, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001265354, "tokenizer_training_raw_mb": 6.547727999520994, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "15532", "model_parameters": "21261312", "train_compute_flops": 1281507301785600.0, "train_compute_hours": 0.12116069035063856, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "tha_thai_5mb": {"language_name": "Thai", "language_script": "Thai", "dataset_category": "5mb", "language_iso6393": "tha", "language_iso15924": "thai", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.741647235, "dataset_raw_mb": 13.713392994117255, "dataset_scaled_mb": 5.00188092, "dataset_tokens": 1081344, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00188092, "tokenizer_training_raw_mb": 13.713392994117255, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 818181203558400.0, "train_compute_hours": 0.077355313790976, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 99.89966%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09995%: [eBible](https://ebible.org/find/)\n* 0.00039%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["OSCAR 2021/09", "eBible", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/", "https://tatoeba.org/en/"]}, "gle_latn_5mb": {"language_name": "Irish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "gle", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.975875656, "dataset_raw_mb": 9.88070699710239, "dataset_scaled_mb": 5.00067247, "dataset_tokens": 2154496, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00067247, "tokenizer_training_raw_mb": 9.88070699710239, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1630164064665600.0, "train_compute_hours": 0.15412460247747492, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.77543%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.32257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.85422%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.21381%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.83397%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "nnb_latn_5mb": {"language_name": "Nande", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nnb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.307887294, "dataset_raw_mb": 6.540180002618751, "dataset_scaled_mb": 5.000568499, "dataset_tokens": 1378816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000568499, "tokenizer_training_raw_mb": 6.540180002618751, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1042135064248320.0, "train_compute_hours": 0.0985291333471139, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 94.01204%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "que_latn_5mb": {"language_name": "Quechua", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "que", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["qub", "qud", "quf", "qug", "quh", "quk", "qul", "qup", "qur", "qus", "quw", "qux", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvj", "qvl", "qvm", "qvn", "qvo", "qvp", "qvs", "qvw", "qvz", "qwa", "qwc", "qwh", "qws", "qxa", "qxc", "qxh", "qxl", "qxn", "qxo", "qxp", "qxr", "qxt", "qxu", "qxw"], "language_byte_premium": 1.21478134, "dataset_raw_mb": 6.075578002241479, "dataset_scaled_mb": 5.001375805, "dataset_tokens": 1530368, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001375805, "tokenizer_training_raw_mb": 6.075578002241479, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1156804399595520.0, "train_compute_hours": 0.10937059777994008, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "Llamacha/monolingual-quechua-iic", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.54127%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.31458%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 7.98999%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Quechua-IIC](https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.28328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.76909%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09735%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00445%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM", "CC100", "Earthlings", "OSCAR", "Quechua-IIC", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "new_deva_5mb": {"language_name": "Newari", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "new", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.561603163, "dataset_raw_mb": 12.811520001032477, "dataset_scaled_mb": 5.001367966, "dataset_tokens": 1222144, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001367966, "tokenizer_training_raw_mb": 12.811520001032477, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 924289078394880.0, "train_compute_hours": 0.08738733104824321, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.70102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.62528%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 20.39344%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.65085%: [eBible](https://ebible.org/find/)\n* 3.62941%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "eus_latn_5mb": {"language_name": "Basque", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "eus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059583723, "dataset_raw_mb": 5.300264000392109, "dataset_scaled_mb": 5.002213497, "dataset_tokens": 1097216, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002213497, "tokenizer_training_raw_mb": 5.300264000392109, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 829764356014080.0, "train_compute_hours": 0.07845044820496758, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.69288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.18968%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.11744%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ron_latn_5mb": {"language_name": "Romanian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ron", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115120928, "dataset_raw_mb": 5.576800999717847, "dataset_scaled_mb": 5.001072852, "dataset_tokens": 1217024, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001072852, "tokenizer_training_raw_mb": 5.576800999717847, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 919717800837120.0, "train_compute_hours": 0.08695513753369136, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ori_orya_5mb": {"language_name": "Odia", "language_script": "Oriya", "dataset_category": "5mb", "language_iso6393": "ori", "language_iso15924": "orya", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ory", "spv"], "language_byte_premium": 2.595114712, "dataset_raw_mb": 12.978880003439903, "dataset_scaled_mb": 5.001274103, "dataset_tokens": 1127936, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001274103, "tokenizer_training_raw_mb": 12.978880003439903, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 853008140206080.0, "train_compute_hours": 0.08064804234675667, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.28209%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 26.83885%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.83892%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.35162%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.43853%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.24999%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Workshop on NER for South and South East Asian Languages", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "fin_latn_5mb": {"language_name": "Finnish", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "fin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.05886439, "dataset_raw_mb": 5.296519999997169, "dataset_scaled_mb": 5.002075856, "dataset_tokens": 1005568, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002075856, "tokenizer_training_raw_mb": 5.296519999997169, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 760846535884800.0, "train_compute_hours": 0.07193458157456291, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kab_latn_5mb": {"language_name": "Kabyle", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kab", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.028599141, "dataset_raw_mb": 5.144795530290736, "dataset_scaled_mb": 5.001749783, "dataset_tokens": 1610240, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001749783, "tokenizer_training_raw_mb": 5.144795530290736, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1217238238494720.0, "train_compute_hours": 0.11508434254859172, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 73.42841%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.37499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.19660%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "inh_cyrl_5mb": {"language_name": "Ingush", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "inh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.700418497, "dataset_raw_mb": 8.50263904031498, "dataset_scaled_mb": 5.000321424, "dataset_tokens": 1463296, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000321424, "tokenizer_training_raw_mb": 8.50263904031498, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1106481606819840.0, "train_compute_hours": 0.1046128064629667, "dataset_hugging_face": [], "dataset_readme_str": "* 89.48455%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 10.51545%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/"]}, "ces_latn_5mb": {"language_name": "Czech", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ces", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035849214, "dataset_raw_mb": 5.179785004736607, "dataset_scaled_mb": 5.000520283, "dataset_tokens": 1102336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000520283, "tokenizer_training_raw_mb": 5.179785004736607, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 832941006520320.0, "train_compute_hours": 0.07875078607101207, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "wol_latn_5mb": {"language_name": "Wolof", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "wol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078647211, "dataset_raw_mb": 5.394594000647933, "dataset_scaled_mb": 5.001258934, "dataset_tokens": 1657856, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001258934, "tokenizer_training_raw_mb": 5.394594000647933, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1253692240035840.0, "train_compute_hours": 0.11853090269429761, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.24103%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 17.08456%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.36785%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.80354%: [eBible](https://ebible.org/find/)\n* 4.50302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "iso_latn_5mb": {"language_name": "Isoko", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "iso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.483217547, "dataset_raw_mb": 7.4165910025794375, "dataset_scaled_mb": 5.000339308, "dataset_tokens": 1655808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000339308, "tokenizer_training_raw_mb": 7.4165910025794375, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "22668", "model_parameters": "25455616", "train_compute_flops": 1252142654423040.0, "train_compute_hours": 0.11838439641817834, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "nno_latn_5mb": {"language_name": "Norwegian Nynorsk", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nno", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 1.033289113, "dataset_raw_mb": 5.167684000233627, "dataset_scaled_mb": 5.001198537, "dataset_tokens": 1218560, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001198537, "tokenizer_training_raw_mb": 5.167684000233627, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 922003439616000.0, "train_compute_hours": 0.08717123429096728, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.90768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.53541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.55180%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00511%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mkw_cyrl_5mb": {"language_name": "Kituba", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "mkw", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.808563089, "dataset_raw_mb": 9.043654469971123, "dataset_scaled_mb": 5.000463918, "dataset_tokens": 1236480, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000463918, "tokenizer_training_raw_mb": 9.043654469971123, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 935136177684480.0, "train_compute_hours": 0.08841287498107811, "dataset_hugging_face": [], "dataset_readme_str": "* 100.00000%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Wortschatz Leipzig Data"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download"]}, "luo_latn_5mb": {"language_name": "Luo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "luo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035745154, "dataset_raw_mb": 5.18013583034542, "dataset_scaled_mb": 5.001361397, "dataset_tokens": 1478656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001361397, "tokenizer_training_raw_mb": 5.18013583034542, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1118800812441600.0, "train_compute_hours": 0.10577753135811492, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 88.95975%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.45137%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "mar_deva_5mb": {"language_name": "Marathi", "language_script": "Devanagari", "dataset_category": "5mb", "language_iso6393": "mar", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.479356527, "dataset_raw_mb": 12.399239000453925, "dataset_scaled_mb": 5.000990727, "dataset_tokens": 1110016, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000990727, "tokenizer_training_raw_mb": 12.399239000453925, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 839875402137600.0, "train_compute_hours": 0.07940640165664582, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.53905%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 40.24253%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13450%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.08391%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Hindialect", "Indiccorp", "OSCAR", "TICO", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "yue_hant_5mb": {"language_name": "Yue Chinese", "language_script": "Han Traditional", "dataset_category": "5mb", "language_iso6393": "yue", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.862461402, "dataset_raw_mb": 4.313005999605403, "dataset_scaled_mb": 5.000810459, "dataset_tokens": 1060864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000810459, "tokenizer_training_raw_mb": 4.313005999605403, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 802685347430400.0, "train_compute_hours": 0.07589025102978328, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.60663%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 47.24904%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.14433%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "lzh_hant_5mb": {"language_name": "Literary Chinese", "language_script": "Han Traditional", "dataset_category": "5mb", "language_iso6393": "lzh", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 3.5018939998, "dataset_scaled_mb": 5.002705714, "dataset_tokens": 934400, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.002705714, "tokenizer_training_raw_mb": 3.5018939998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 705874986270720.0, "train_compute_hours": 0.06673727142923172, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.82278%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 46.05529%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.12193%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "cat_latn_5mb": {"language_name": "Catalan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "cat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.092622079, "dataset_raw_mb": 5.463217005414115, "dataset_scaled_mb": 5.000097573, "dataset_tokens": 1235456, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000097573, "tokenizer_training_raw_mb": 5.463217005414115, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 933664071352320.0, "train_compute_hours": 0.08827369401876481, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tsn_latn_5mb": {"language_name": "Tswana", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "tsn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173940296, "dataset_raw_mb": 5.8710039984548, "dataset_scaled_mb": 5.001109527, "dataset_tokens": 1606656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001109527, "tokenizer_training_raw_mb": 5.8710039984548, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1214952599715840.0, "train_compute_hours": 0.1148682457913158, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 54.15070%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.72794%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.23011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.14046%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.74997%: [eBible](https://ebible.org/find/)\n* 0.00083%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "sqi_latn_5mb": {"language_name": "Albanian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "sqi", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aae", "aat", "aln", "als"], "language_byte_premium": 1.335619892, "dataset_raw_mb": 6.678972979450525, "dataset_scaled_mb": 5.000654018, "dataset_tokens": 1440768, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000654018, "tokenizer_training_raw_mb": 6.678972979450525, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1089436165079040.0, "train_compute_hours": 0.1030012374256547, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 62.65815%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 37.34185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "zap_latn_5mb": {"language_name": "Zapotec", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "zap", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["zaa", "zab", "zac", "zad", "zae", "zaf", "zai", "zam", "zao", "zaq", "zar", "zas", "zat", "zav", "zaw", "zax", "zca", "zcd", "zoo", "zpa", "zpb", "zpc", "zpd", "zpe", "zpf", "zpg", "zph", "zpi", "zpj", "zpk", "zpl", "zpm", "zpn", "zpo", "zpp", "zpq", "zpr", "zps", "zpt", "zpu", "zpv", "zpw", "zpx", "zpy", "zpz", "zsr", "zte", "ztg", "ztl", "ztm", "ztn", "ztp", "ztq", "zts", "ztt", "ztu", "ztx", "zty"], "language_byte_premium": 1.075680556, "dataset_raw_mb": 5.379914000386083, "dataset_scaled_mb": 5.001404897, "dataset_tokens": 1284608, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001404897, "tokenizer_training_raw_mb": 5.379914000386083, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "33468", "model_parameters": "30698496", "train_compute_flops": 970854126059520.0, "train_compute_hours": 0.09178984464562735, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 91.84155%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.15845%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "ibo_latn_5mb": {"language_name": "Igbo", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ibo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345020968, "dataset_raw_mb": 6.726917998205955, "dataset_scaled_mb": 5.001348052, "dataset_tokens": 1595904, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001348052, "tokenizer_training_raw_mb": 6.726917998205955, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1206391139205120.0, "train_compute_hours": 0.11405879861575681, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 31.78330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 31.30256%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.52382%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93791%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 4.92177%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.53064%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "oci_latn_5mb": {"language_name": "Occitan", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "oci", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.014733462, "dataset_raw_mb": 5.074469998669379, "dataset_scaled_mb": 5.000791034, "dataset_tokens": 1392640, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000791034, "tokenizer_training_raw_mb": 5.074469998669379, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1053718216704000.0, "train_compute_hours": 0.09962426776110546, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.78812%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.07771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.38373%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.02047%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.72997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "crh_latn_5mb": {"language_name": "Crimean Tatar", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "crh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.312089034, "dataset_raw_mb": 6.560646485132576, "dataset_scaled_mb": 5.000153431, "dataset_tokens": 1438720, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000153431, "tokenizer_training_raw_mb": 6.560646485132576, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1087886579466240.0, "train_compute_hours": 0.10285473114953543, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.04959%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.27361%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.67679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "san_latn_5mb": {"language_name": "Sanskrit", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "san", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 0.966275057, "dataset_raw_mb": 4.832327619606753, "dataset_scaled_mb": 5.000985573, "dataset_tokens": 1022464, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000985573, "tokenizer_training_raw_mb": 4.832327619606753, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 772507167621120.0, "train_compute_hours": 0.07303704130236044, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 98.91275%: [eBible](https://ebible.org/find/)\n* 1.08725%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["eBible", "Glot500", "CCNet", "Wortschatz Leipzig Data"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download"]}, "bre_latn_5mb": {"language_name": "Breton", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "bre", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01282485, "dataset_raw_mb": 5.065288999590326, "dataset_scaled_mb": 5.001150001, "dataset_tokens": 1383424, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001150001, "tokenizer_training_raw_mb": 5.065288999590326, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1046047767920640.0, "train_compute_hours": 0.09889906169431506, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.39389%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.93949%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.16716%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.31819%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.13530%: [eBible](https://ebible.org/find/)\n* 0.04596%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "kir_cyrl_5mb": {"language_name": "Kirghiz", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kir", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.963770708, "dataset_raw_mb": 9.818885999166032, "dataset_scaled_mb": 5.000016529, "dataset_tokens": 1176064, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000016529, "tokenizer_training_raw_mb": 9.818885999166032, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 888726088581120.0, "train_compute_hours": 0.0840250120113059, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.41839%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.62164%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.53594%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt)\n* 8.32078%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.10325%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "kin_latn_5mb": {"language_name": "Kinyarwanda", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "kin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.133982717, "dataset_raw_mb": 5.6717250010542575, "dataset_scaled_mb": 5.001597393, "dataset_tokens": 1273856, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001597393, "tokenizer_training_raw_mb": 5.6717250010542575, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 963842251161600.0, "train_compute_hours": 0.09112690374618765, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.08100%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.87658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [KinyaSMT](https://github.com/pniyongabo/kinyarwandaSMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.34254%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.79880%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "BLOOM", "KinyaSMT", "Wortschatz Leipzig Data", "MoT", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/pniyongabo/kinyarwandaSMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "alt_cyrl_5mb": {"language_name": "Southern Altai", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "alt", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.856580455, "dataset_raw_mb": 9.285483001665408, "dataset_scaled_mb": 5.001390043, "dataset_tokens": 1192960, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001390043, "tokenizer_training_raw_mb": 9.285483001665408, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 901936305930240.0, "train_compute_hours": 0.0852739780152227, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 58.22529%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.73797%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "lav_latn_5mb": {"language_name": "Latvian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lav", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ltg", "lvs"], "language_byte_premium": 1.288843709, "dataset_raw_mb": 6.4463199943567755, "dataset_scaled_mb": 5.001630492, "dataset_tokens": 1297408, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001630492, "tokenizer_training_raw_mb": 6.4463199943567755, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 980965172183040.0, "train_compute_hours": 0.09274579809730561, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ton_latn_5mb": {"language_name": "Tonga (Tonga Islands)", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ton", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.270386963, "dataset_raw_mb": 6.352314000100716, "dataset_scaled_mb": 5.00029848, "dataset_tokens": 1825280, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00029848, "tokenizer_training_raw_mb": 6.352314000100716, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "32963", "model_parameters": "30698496", "train_compute_flops": 1379944727838720.0, "train_compute_hours": 0.13046750154111536, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.09550%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.64313%: [eBible](https://ebible.org/find/)\n* 1.76795%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.49248%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pbt_arab_5mb": {"language_name": "Southern Pashto", "language_script": "Arabic", "dataset_category": "5mb", "language_iso6393": "pbt", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["pus"], "language_code_individuals": [], "language_byte_premium": 1.736020956, "dataset_raw_mb": 8.682375984968358, "dataset_scaled_mb": 5.001308282, "dataset_tokens": 1431552, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001308282, "tokenizer_training_raw_mb": 8.682375984968358, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "42290", "model_parameters": "34892800", "train_compute_flops": 1083160343347200.0, "train_compute_hours": 0.10240788700737165, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "uzb_cyrl_5mb": {"language_name": "Uzbek", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "uzb", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.980686782, "dataset_raw_mb": 9.90548800103414, "dataset_scaled_mb": 5.00103706, "dataset_tokens": 1116160, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.00103706, "tokenizer_training_raw_mb": 9.90548800103414, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 844524158976000.0, "train_compute_hours": 0.07984592048500365, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "kbd_cyrl_5mb": {"language_name": "Kabardian", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "kbd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.783468787, "dataset_raw_mb": 8.919148001068022, "dataset_scaled_mb": 5.001011549, "dataset_tokens": 1452544, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001011549, "tokenizer_training_raw_mb": 8.919148001068022, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1097920146309120.0, "train_compute_hours": 0.10380335928740772, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 59.32058%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.38457%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.04549%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.24936%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nan_latn_5mb": {"language_name": "Min Nan Chinese", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "nan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 1.14871192, "dataset_raw_mb": 5.744273001794356, "dataset_scaled_mb": 5.000621045, "dataset_tokens": 1916416, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000621045, "tokenizer_training_raw_mb": 5.744273001794356, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1449598601134080.0, "train_compute_hours": 0.13705295865267667, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 49.98250%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 42.06803%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.94947%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "hin_latn_5mb": {"language_name": "Hindi", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "hin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.255306654, "dataset_raw_mb": 6.27656099846868, "dataset_scaled_mb": 5.000022089, "dataset_tokens": 1517568, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000022089, "tokenizer_training_raw_mb": 6.27656099846868, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1148242939084800.0, "train_compute_hours": 0.1085611506043811, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 74.45561%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.54439%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [IITB](https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "IITB", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "myv_cyrl_5mb": {"language_name": "Erzya", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "myv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.773695539, "dataset_raw_mb": 8.871776000692373, "dataset_scaled_mb": 5.001859567, "dataset_tokens": 1186304, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001859567, "tokenizer_training_raw_mb": 8.871776000692373, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 896474016645120.0, "train_compute_hours": 0.08475754339190227, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.71457%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.46832%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 13.56506%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.24591%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00155%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ben_beng_5mb": {"language_name": "Bengali", "language_script": "Bengali", "dataset_category": "5mb", "language_iso6393": "ben", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.430759774, "dataset_raw_mb": 12.157986019002381, "dataset_scaled_mb": 5.001722568, "dataset_tokens": 1033728, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001722568, "tokenizer_training_raw_mb": 12.157986019002381, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 781727202017280.0, "train_compute_hours": 0.07390875364527011, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ido_latn_5mb": {"language_name": "Ido", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "ido", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.17576321, "dataset_raw_mb": 5.880174075319761, "dataset_scaled_mb": 5.001155016, "dataset_tokens": 1463808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001155016, "tokenizer_training_raw_mb": 5.880174075319761, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1107140180705280.0, "train_compute_hours": 0.10467507163031739, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.56311%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 39.86649%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29534%: [Tatoeba](https://tatoeba.org/en/)\n* 0.27506%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bew_cyrl_5mb": {"language_name": "Betawi", "language_script": "Cyrillic", "dataset_category": "5mb", "language_iso6393": "bew", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740538562, "dataset_raw_mb": 8.70347715411381, "dataset_scaled_mb": 5.000450633, "dataset_tokens": 1186816, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000450633, "tokenizer_training_raw_mb": 8.70347715411381, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 897287549091840.0, "train_compute_hours": 0.08483445918686489, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 52.29436%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 47.70564%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lmo_latn_5mb": {"language_name": "Lombard", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "lmo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9431924587, "dataset_raw_mb": 4.717291903420297, "dataset_scaled_mb": 5.001409691, "dataset_tokens": 1500672, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001409691, "tokenizer_training_raw_mb": 4.717291903420297, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1135032721735680.0, "train_compute_hours": 0.1073121846004643, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.48817%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.05952%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.79259%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.65971%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "vol_latn_5mb": {"language_name": "Volap\u00fck", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "vol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129514541, "dataset_raw_mb": 5.64835062990225, "dataset_scaled_mb": 5.000688725, "dataset_tokens": 1458176, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.000688725, "tokenizer_training_raw_mb": 5.64835062990225, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1103304956313600.0, "train_compute_hours": 0.1043124685969222, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.27628%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.97146%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.60410%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14817%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "rus_latn_5mb": {"language_name": "Russian", "language_script": "Latin", "dataset_category": "5mb", "language_iso6393": "rus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182252535, "dataset_raw_mb": 5.913292002018307, "dataset_scaled_mb": 5.001716492, "dataset_tokens": 1441792, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.001716492, "tokenizer_training_raw_mb": 5.913292002018307, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1090908271411200.0, "train_compute_hours": 0.103140418387968, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kaa_latn_10mb": {"language_name": "Kara-Kalpak", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kaa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22736123, "dataset_raw_mb": 12.275266009700465, "dataset_scaled_mb": 10.00134737, "dataset_tokens": 2451968, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00134737, "tokenizer_training_raw_mb": 12.275266009700465, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1854117925355520.0, "train_compute_hours": 0.17529842203361282, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 97.52670%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.65677%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81649%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "hsb_latn_10mb": {"language_name": "Upper Sorbian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hsb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.124658236, "dataset_raw_mb": 11.248710224629095, "dataset_scaled_mb": 10.00189201, "dataset_tokens": 2439680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00189201, "tokenizer_training_raw_mb": 11.248710224629095, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1844820411678720.0, "train_compute_hours": 0.1744193843768972, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 52.92169%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.06296%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.86926%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14609%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hmn_latn_10mb": {"language_name": "Hmong", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hmn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cqd", "hea", "hma", "hmc", "hmd", "hme", "hmg", "hmh", "hmi", "hmj", "hml", "hmm", "hmp", "hmq", "hms", "hmw", "hmy", "hmz", "hnj", "hrm", "huj", "mmr", "muq", "mww", "sfm"], "language_byte_premium": 1.18983824, "dataset_raw_mb": 11.900016000309991, "dataset_scaled_mb": 10.00137296, "dataset_tokens": 2942464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00137296, "tokenizer_training_raw_mb": 11.900016000309991, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2224391407534080.0, "train_compute_hours": 0.21030609671231304, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4"], "dataset_readme_str": "* 54.54885%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 45.45115%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4"]}, "hil_latn_10mb": {"language_name": "Hiligaynon", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.351057958, "dataset_raw_mb": 13.51118400279867, "dataset_scaled_mb": 10.00044737, "dataset_tokens": 2877952, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00044737, "tokenizer_training_raw_mb": 13.51118400279867, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2176431732817920.0, "train_compute_hours": 0.20577172746642156, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 99.96778%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.01707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/)\n* 0.01288%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00227%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/"]}, "jav_latn_10mb": {"language_name": "Javanese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "jav", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.146845795, "dataset_raw_mb": 11.47040400969262, "dataset_scaled_mb": 10.00169688, "dataset_tokens": 2544128, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00169688, "tokenizer_training_raw_mb": 11.47040400969262, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1922454650880000.0, "train_compute_hours": 0.18175934881047273, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.56951%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 38.81475%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.78232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 6.69194%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.05217%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.08930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kon_latn_10mb": {"language_name": "Kongo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["kng", "kwy", "ldi"], "language_byte_premium": 1.230994587, "dataset_raw_mb": 12.31183600063861, "dataset_scaled_mb": 10.00153545, "dataset_tokens": 3147264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00153545, "tokenizer_training_raw_mb": 12.31183600063861, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2379349968814080.0, "train_compute_hours": 0.2249567243242403, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.64648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.58706%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.42232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.34414%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "knc_arab_10mb": {"language_name": "Central Kanuri", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "knc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 2.50197962, "dataset_raw_mb": 25.020760638084123, "dataset_scaled_mb": 10.00038547, "dataset_tokens": 10667008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00038547, "tokenizer_training_raw_mb": 25.020760638084123, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "26468", "model_parameters": "26504192", "train_compute_flops": 8068769765130240.0, "train_compute_hours": 0.7628655050668591, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "tet_latn_10mb": {"language_name": "Tetum", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tet", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.400532559, "dataset_raw_mb": 14.006570005194646, "dataset_scaled_mb": 10.00088853, "dataset_tokens": 2990592, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00088853, "tokenizer_training_raw_mb": 14.006570005194646, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2260264314470400.0, "train_compute_hours": 0.2136977170044742, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 98.26624%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.01934%: [eBible](https://ebible.org/find/)\n* 0.71376%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00066%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "zsm_latn_10mb": {"language_name": "Standard Malay", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "zsm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.143822408, "dataset_raw_mb": 11.44021102534316, "dataset_scaled_mb": 10.00173711, "dataset_tokens": 2225664, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00173711, "tokenizer_training_raw_mb": 11.44021102534316, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1682036443054080.0, "train_compute_hours": 0.15902890007056758, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 91.23035%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.76965%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "isl_latn_10mb": {"language_name": "Icelandic", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "isl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.154319145, "dataset_raw_mb": 11.545148009407585, "dataset_scaled_mb": 10.00169499, "dataset_tokens": 2450944, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00169499, "tokenizer_training_raw_mb": 11.545148009407585, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1852490860462080.0, "train_compute_hours": 0.17514459044368758, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 80.12926%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.11564%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 8.86357%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.85697%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03456%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Ndc without informant codes", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "srp_cyrl_10mb": {"language_name": "Serbian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "srp", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.424878645, "dataset_raw_mb": 14.251556984037336, "dataset_scaled_mb": 10.0019444, "dataset_tokens": 1928704, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0019444, "tokenizer_training_raw_mb": 14.251556984037336, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1458741156249600.0, "train_compute_hours": 0.13791734568178038, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaa_cyrl_10mb": {"language_name": "Kara-Kalpak", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kaa", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.920886208, "dataset_raw_mb": 19.211463997204184, "dataset_scaled_mb": 10.00135454, "dataset_tokens": 2196480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00135454, "tokenizer_training_raw_mb": 19.211463997204184, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1659683670589440.0, "train_compute_hours": 0.15691554703754707, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 99.51383%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.48617%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "uzb_latn_10mb": {"language_name": "Uzbek", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "uzb", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.229496706, "dataset_raw_mb": 12.295593992665356, "dataset_scaled_mb": 10.00050991, "dataset_tokens": 2688512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00050991, "tokenizer_training_raw_mb": 12.295593992665356, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2032242791546880.0, "train_compute_hours": 0.19213931847352322, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 76.42069%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.51914%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.42511%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.63507%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mos_latn_10mb": {"language_name": "Mossi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "mos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141292925, "dataset_raw_mb": 11.413205340171485, "dataset_scaled_mb": 10.00024191, "dataset_tokens": 3417088, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00024191, "tokenizer_training_raw_mb": 11.413205340171485, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2583236695818240.0, "train_compute_hours": 0.24423328760463361, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.01233%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "pag_latn_10mb": {"language_name": "Pangasinan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.043908443, "dataset_raw_mb": 10.439798755669376, "dataset_scaled_mb": 10.00068428, "dataset_tokens": 2927104, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00068428, "tokenizer_training_raw_mb": 10.439798755669376, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2213621787525120.0, "train_compute_hours": 0.20928787809328409, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.89102%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.15082%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.95815%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "uig_arab_10mb": {"language_name": "Uighur", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "uig", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.308625158, "dataset_raw_mb": 23.090687002998553, "dataset_scaled_mb": 10.00192124, "dataset_tokens": 2714624, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00192124, "tokenizer_training_raw_mb": 23.090687002998553, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2053123457679360.0, "train_compute_hours": 0.1941134905442304, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 65.05923%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.33662%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.64713%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.70495%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 2.67723%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.57484%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "WikiMatrix", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "ava_cyrl_10mb": {"language_name": "Avaric", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "ava", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.938794023, "dataset_raw_mb": 19.390958990457634, "dataset_scaled_mb": 10.00155703, "dataset_tokens": 2655744, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00155703, "tokenizer_training_raw_mb": 19.390958990457634, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2007449421742080.0, "train_compute_hours": 0.18979521805561486, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.59506%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.75473%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 6.23003%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.42018%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "knc_latn_10mb": {"language_name": "Central Kanuri", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "knc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 1.176891381, "dataset_raw_mb": 11.77061716020246, "dataset_scaled_mb": 10.00144733, "dataset_tokens": 3413504, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00144733, "tokenizer_training_raw_mb": 11.77061716020246, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2580796098478080.0, "train_compute_hours": 0.24400254021974577, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "bul_cyrl_10mb": {"language_name": "Bulgarian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bul", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.812315686, "dataset_raw_mb": 18.125565010714087, "dataset_scaled_mb": 10.00132877, "dataset_tokens": 2323968, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00132877, "tokenizer_training_raw_mb": 18.125565010714087, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1756416552468480.0, "train_compute_hours": 0.16606120132429267, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bem_latn_10mb": {"language_name": "Bemba", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bem", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155237492, "dataset_raw_mb": 11.553645484850826, "dataset_scaled_mb": 10.00109983, "dataset_tokens": 2942976, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00109983, "tokenizer_training_raw_mb": 11.553645484850826, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2225359898542080.0, "train_compute_hours": 0.21039766313488759, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.99219%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.00781%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "uzn_latn_10mb": {"language_name": "Northern Uzbek", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "uzn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 1.645764497, "dataset_raw_mb": 16.458303456832894, "dataset_scaled_mb": 10.00040011, "dataset_tokens": 3656192, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00040011, "tokenizer_training_raw_mb": 16.458303456832894, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2765274265681920.0, "train_compute_hours": 0.2614441123917452, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "kur_latn_10mb": {"language_name": "Kurdish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kur", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.288322619, "dataset_raw_mb": 12.884030992256863, "dataset_scaled_mb": 10.00062469, "dataset_tokens": 2879488, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00062469, "tokenizer_training_raw_mb": 12.884030992256863, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2178717371596800.0, "train_compute_hours": 0.20598782422369746, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 71.61672%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.40823%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.33884%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.36208%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.27236%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00178%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "BLOOM", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "TICO", "W2C", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "lit_latn_10mb": {"language_name": "Lithuanian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lit", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.030036841, "dataset_raw_mb": 10.301281990875756, "dataset_scaled_mb": 10.00088694, "dataset_tokens": 2102272, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00088694, "tokenizer_training_raw_mb": 10.301281990875756, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1588402732400640.0, "train_compute_hours": 0.15017625833606052, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kor_hang_10mb": {"language_name": "Korean", "language_script": "Hangul", "dataset_category": "10mb", "language_iso6393": "kor", "language_iso15924": "hang", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.293311204, "dataset_raw_mb": 12.934932996309009, "dataset_scaled_mb": 10.00140798, "dataset_tokens": 2388480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00140798, "tokenizer_training_raw_mb": 12.934932996309009, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1804686144307200.0, "train_compute_hours": 0.170624871825408, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ekk_latn_10mb": {"language_name": "Standard Estonian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ekk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["est"], "language_code_individuals": [], "language_byte_premium": 0.993591004, "dataset_raw_mb": 9.936580664248151, "dataset_scaled_mb": 10.00067495, "dataset_tokens": 2086912, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00067495, "tokenizer_training_raw_mb": 9.936580664248151, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1577633112391680.0, "train_compute_hours": 0.14915803971703157, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.24897%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.75103%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/"]}, "bik_latn_10mb": {"language_name": "Bikol", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bik", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bcl", "bln", "bto", "cts", "fbl", "lbl", "rbl", "ubl"], "language_byte_premium": 1.272069912, "dataset_raw_mb": 12.72190399917925, "dataset_scaled_mb": 10.00094718, "dataset_tokens": 2872832, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00094718, "tokenizer_training_raw_mb": 12.72190399917925, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2171705496698880.0, "train_compute_hours": 0.20532488332425777, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 43.21685%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.79360%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.98955%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "kal_latn_10mb": {"language_name": "Kalaallisut", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.341175059, "dataset_raw_mb": 13.41365099482335, "dataset_scaled_mb": 10.00141697, "dataset_tokens": 2224640, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00141697, "tokenizer_training_raw_mb": 13.41365099482335, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1680719295283200.0, "train_compute_hours": 0.1589043697358662, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.53226%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.83885%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.54939%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.07923%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "otq_latn_10mb": {"language_name": "Quer\u00e9taro Otomi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "otq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.252835364, "dataset_raw_mb": 12.529460996121532, "dataset_scaled_mb": 10.00088388, "dataset_tokens": 3298304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00088388, "tokenizer_training_raw_mb": 12.529460996121532, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48198", "model_parameters": "38038528", "train_compute_flops": 2493360730275840.0, "train_compute_hours": 0.2357359235897158, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 95.35934%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.64066%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "nya_latn_10mb": {"language_name": "Nyanja", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nya", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.21429492, "dataset_raw_mb": 12.143154998703041, "dataset_scaled_mb": 10.00016948, "dataset_tokens": 2604544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00016948, "tokenizer_training_raw_mb": 12.143154998703041, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1970104408473600.0, "train_compute_hours": 0.1862644168011404, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.17651%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.50780%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.70728%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.45060%: [eBible](https://ebible.org/find/)\n* 0.15782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "pnb_arab_10mb": {"language_name": "Western Panjabi", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "pnb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["lah"], "language_code_individuals": [], "language_byte_premium": 1.414617177, "dataset_raw_mb": 14.148428522928642, "dataset_scaled_mb": 10.00159531, "dataset_tokens": 2564608, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00159531, "tokenizer_training_raw_mb": 14.148428522928642, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1937950507008000.0, "train_compute_hours": 0.18322441157166547, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.15181%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.73099%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.11696%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00023%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ady_cyrl_10mb": {"language_name": "Adyghe", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "ady", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.806684783, "dataset_raw_mb": 18.06814300618724, "dataset_scaled_mb": 10.00071688, "dataset_tokens": 2749440, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00071688, "tokenizer_training_raw_mb": 18.06814300618724, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2078071786045440.0, "train_compute_hours": 0.1964722415897507, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 75.48292%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.94295%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.57302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00111%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "tir_ethi_10mb": {"language_name": "Tigrinya", "language_script": "Ge'ez", "dataset_category": "10mb", "language_iso6393": "tir", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.762985532, "dataset_raw_mb": 17.63144999330326, "dataset_scaled_mb": 10.00090453, "dataset_tokens": 2320896, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00090453, "tokenizer_training_raw_mb": 17.63144999330326, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1754944446136320.0, "train_compute_hours": 0.16592202036197937, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 50.52196%: [Tigrinya Language Modeling Dataset](https://zenodo.org/record/5139094)\n* 20.99445%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.46805%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.08565%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 4.86709%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.06280%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Tigrinya Language Modeling Dataset", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://zenodo.org/record/5139094", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "hye_armn_10mb": {"language_name": "Armenian", "language_script": "Armenian", "dataset_category": "10mb", "language_iso6393": "hye", "language_iso15924": "armn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.724226492, "dataset_raw_mb": 17.246251986855572, "dataset_scaled_mb": 10.00231238, "dataset_tokens": 2113024, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00231238, "tokenizer_training_raw_mb": 17.246251986855572, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1598203861401600.0, "train_compute_hours": 0.15110291053251493, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 52.58958%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 24.27630%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.13412%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "aym_latn_10mb": {"language_name": "Aymara", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "aym", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ayc", "ayr"], "language_byte_premium": 1.213786361, "dataset_raw_mb": 12.138595001247547, "dataset_scaled_mb": 10.00060257, "dataset_tokens": 3058688, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00060257, "tokenizer_training_raw_mb": 12.138595001247547, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2313453840629760.0, "train_compute_hours": 0.21872654493226823, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.19759%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 24.44245%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 7.18196%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.90026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.73814%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.53960%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/"]}, "lvs_latn_10mb": {"language_name": "Standard Latvian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lvs", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.206980036, "dataset_raw_mb": 12.071710695582178, "dataset_scaled_mb": 10.00158274, "dataset_tokens": 2438656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00158274, "tokenizer_training_raw_mb": 12.071710695582178, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1843193346785280.0, "train_compute_hours": 0.17426555278697195, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "afb_arab_10mb": {"language_name": "Gulf Arabic", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "afb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.374141124, "dataset_raw_mb": 13.743946255545557, "dataset_scaled_mb": 10.0018448, "dataset_tokens": 2306048, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0018448, "tokenizer_training_raw_mb": 13.743946255545557, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1744833400012800.0, "train_compute_hours": 0.1649660669103011, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.98461%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/)\n* 0.01539%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "ADD", "AraBench", "DART", "Habibi", "QADI", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/"]}, "cfm_latn_10mb": {"language_name": "Falam Chin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cfm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323887964, "dataset_raw_mb": 13.240860997204681, "dataset_scaled_mb": 10.00149662, "dataset_tokens": 3089920, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00149662, "tokenizer_training_raw_mb": 13.240860997204681, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2335961571655680.0, "train_compute_hours": 0.22085454859290068, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mai_deva_10mb": {"language_name": "Maithili", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "mai", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.38980971, "dataset_raw_mb": 23.901054991376167, "dataset_scaled_mb": 10.00123771, "dataset_tokens": 2728448, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00123771, "tokenizer_training_raw_mb": 23.901054991376167, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2061917356032000.0, "train_compute_hours": 0.19494491366120728, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.00767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.01673%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.36867%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.75652%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.83462%: [eBible](https://ebible.org/find/)\n* 0.01578%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fur_latn_10mb": {"language_name": "Friulian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.067269167, "dataset_raw_mb": 10.674131053234767, "dataset_scaled_mb": 10.00134866, "dataset_tokens": 2675200, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00134866, "tokenizer_training_raw_mb": 10.674131053234767, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2021628130099200.0, "train_compute_hours": 0.1911357504821062, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 72.56517%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 15.82188%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.61295%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "ssw_latn_10mb": {"language_name": "Swati", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ssw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141426619, "dataset_raw_mb": 11.415927993014602, "dataset_scaled_mb": 10.0014559, "dataset_tokens": 2698752, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0014559, "tokenizer_training_raw_mb": 11.415927993014602, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2041385346662400.0, "train_compute_hours": 0.19300370550262694, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.30477%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.07695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 14.60799%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.01028%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "bam_latn_10mb": {"language_name": "Bambara", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.256878532, "dataset_raw_mb": 12.570440000587377, "dataset_scaled_mb": 10.0013165, "dataset_tokens": 3532288, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0013165, "tokenizer_training_raw_mb": 12.570440000587377, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2670672064020480.0, "train_compute_hours": 0.25249990423466356, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 67.90603%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.26277%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.19633%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.75328%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.88159%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pcm_latn_10mb": {"language_name": "Nigerian Pidgin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pcm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.954503897, "dataset_raw_mb": 9.54697115314862, "dataset_scaled_mb": 10.00202428, "dataset_tokens": 2189824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00202428, "tokenizer_training_raw_mb": 9.54697115314862, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48383", "model_parameters": "38038528", "train_compute_flops": 1655770966917120.0, "train_compute_hours": 0.15654561869034592, "dataset_hugging_face": ["castorini/afriberta-corpus", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 48.40160%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 48.06214%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 3.19131%: [eBible](https://ebible.org/find/)\n* 0.34495%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["AfriBERTa", "Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/castorini/afriberta-corpus", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "sag_latn_10mb": {"language_name": "Sango", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.162464677, "dataset_raw_mb": 11.62648299920379, "dataset_scaled_mb": 10.0015796, "dataset_tokens": 3138048, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0015796, "tokenizer_training_raw_mb": 11.62648299920379, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2371834478592000.0, "train_compute_hours": 0.22424616888506183, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 60.53835%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.27653%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93802%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.24653%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00058%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "iba_latn_10mb": {"language_name": "Iban", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "iba", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.298224214, "dataset_raw_mb": 12.98446999660916, "dataset_scaled_mb": 10.00171608, "dataset_tokens": 2683392, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00171608, "tokenizer_training_raw_mb": 12.98446999660916, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2027826472550400.0, "train_compute_hours": 0.1917217755865833, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98818%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.00622%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00559%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "sot_latn_10mb": {"language_name": "Southern Sotho", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sot", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.166011747, "dataset_raw_mb": 11.66061300167224, "dataset_scaled_mb": 10.00042498, "dataset_tokens": 2891776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00042498, "tokenizer_training_raw_mb": 11.66061300167224, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2188014885273600.0, "train_compute_hours": 0.20686686188041312, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 34.03982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.19565%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.65729%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.10724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "dan_latn_10mb": {"language_name": "Danish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "dan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.021065783, "dataset_raw_mb": 10.212325005789166, "dataset_scaled_mb": 10.00163278, "dataset_tokens": 2143232, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00163278, "tokenizer_training_raw_mb": 10.212325005789166, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1619394444656640.0, "train_compute_hours": 0.15310638385844597, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hrv_latn_10mb": {"language_name": "Croatian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hrv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.989672823, "dataset_raw_mb": 9.897404997966552, "dataset_scaled_mb": 10.00068383, "dataset_tokens": 2284544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00068383, "tokenizer_training_raw_mb": 9.897404997966552, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1727710478991360.0, "train_compute_hours": 0.16334717255918313, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.01986%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.11618%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.57132%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.27830%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.96930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ful_latn_10mb": {"language_name": "Fulah", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ful", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ffm", "fub", "fuc", "fue", "fuf", "fuh", "fui", "fuq", "fuv"], "language_byte_premium": 1.258383084, "dataset_raw_mb": 12.585701992726754, "dataset_scaled_mb": 10.00148695, "dataset_tokens": 3298304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00148695, "tokenizer_training_raw_mb": 12.585701992726754, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2493360730275840.0, "train_compute_hours": 0.2357359235897158, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "cawoylel/FulaSpeechCorpora", "allenai/MADLAD-400"], "dataset_readme_str": "* 66.44435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.91335%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.61833%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.98971%: [Fula Speech Corpora](https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora)\n* 0.75836%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.27589%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Fula Speech Corpora", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "pes_arab_10mb": {"language_name": "Iranian Persian", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "pes", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.59732627, "dataset_raw_mb": 15.974252004025322, "dataset_scaled_mb": 10.00061935, "dataset_tokens": 2219520, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00061935, "tokenizer_training_raw_mb": 15.974252004025322, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1678782313267200.0, "train_compute_hours": 0.1587212368907171, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pus_arab_10mb": {"language_name": "Pushto", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "pus", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pbt", "pbu", "pst"], "language_byte_premium": 1.585665259, "dataset_raw_mb": 15.858709990673551, "dataset_scaled_mb": 10.0012975, "dataset_tokens": 2475008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0012975, "tokenizer_training_raw_mb": 15.858709990673551, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1870427313930240.0, "train_compute_hours": 0.17684040058976816, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.56138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95008%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.68420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.73176%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/)\n* 4.07251%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00007%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Tatoeba", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "rus_cyrl_10mb": {"language_name": "Russian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "rus", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.822744411, "dataset_raw_mb": 18.22989200106164, "dataset_scaled_mb": 10.00134297, "dataset_tokens": 2305024, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00134297, "tokenizer_training_raw_mb": 18.22989200106164, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1743206335119360.0, "train_compute_hours": 0.16481223532037587, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lao_laoo_10mb": {"language_name": "Lao", "language_script": "Lao", "dataset_category": "10mb", "language_iso6393": "lao", "language_iso15924": "laoo", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.706914388, "dataset_raw_mb": 27.073706005232957, "dataset_scaled_mb": 10.00168536, "dataset_tokens": 2371072, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00168536, "tokenizer_training_raw_mb": 27.073706005232957, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1793451648614400.0, "train_compute_hours": 0.16956270132354329, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.18671%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.72107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.03480%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.64542%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 0.41139%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00061%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://dumps.wikimedia.org/"]}, "srp_latn_10mb": {"language_name": "Serbian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "srp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.826257686, "dataset_raw_mb": 8.262652991383188, "dataset_scaled_mb": 10.00009214, "dataset_tokens": 2127872, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00009214, "tokenizer_training_raw_mb": 8.262652991383188, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1608624824647680.0, "train_compute_hours": 0.15208816523941704, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.93765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.75254%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30981%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "hun_latn_10mb": {"language_name": "Hungarian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01995535, "dataset_raw_mb": 10.201373008548419, "dataset_scaled_mb": 10.00178391, "dataset_tokens": 1982464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00178391, "tokenizer_training_raw_mb": 10.201373008548419, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1499998873190400.0, "train_compute_hours": 0.14181807528345602, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ctd_latn_10mb": {"language_name": "Tedim Chin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ctd", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.295581309, "dataset_raw_mb": 12.957453995551099, "dataset_scaled_mb": 10.00126654, "dataset_tokens": 3126272, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00126654, "tokenizer_training_raw_mb": 12.957453995551099, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2363195538800640.0, "train_compute_hours": 0.2234293963956969, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "plt_latn_10mb": {"language_name": "Plateau Malagasy", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "plt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.151182211, "dataset_raw_mb": 11.511905110237414, "dataset_scaled_mb": 10.0000721, "dataset_tokens": 2701312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0000721, "tokenizer_training_raw_mb": 11.511905110237414, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2042508796231680.0, "train_compute_hours": 0.1931099225528134, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 95.80021%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.19979%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TeDDi](https://github.com/MorphDiv/TeDDi_sample)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "TeDDi"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/MorphDiv/TeDDi_sample"]}, "san_deva_10mb": {"language_name": "Sanskrit", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "san", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 2.542780452, "dataset_raw_mb": 25.428826997447555, "dataset_scaled_mb": 10.00040211, "dataset_tokens": 2752000, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00040211, "tokenizer_training_raw_mb": 25.428826997447555, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2081674572595200.0, "train_compute_hours": 0.196812868681728, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 39.60135%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 37.07458%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.73656%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.30509%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 5.49767%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 0.78475%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "CCNet", "Hindialect", "Wortschatz Leipzig Data", "OSCAR", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://ebible.org/find/"]}, "gom_latn_10mb": {"language_name": "Goan Konkani", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "gom", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.2133745, "dataset_raw_mb": 12.134941993944249, "dataset_scaled_mb": 10.0009865, "dataset_tokens": 2713600, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0009865, "tokenizer_training_raw_mb": 12.134941993944249, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2051806309908480.0, "train_compute_hours": 0.19398896020952905, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43524%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 44.56476%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "yid_hebr_10mb": {"language_name": "Yiddish", "language_script": "Hebrew", "dataset_category": "10mb", "language_iso6393": "yid", "language_iso15924": "hebr", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ydd", "yih"], "language_byte_premium": 1.546105069, "dataset_raw_mb": 15.461908994768004, "dataset_scaled_mb": 10.00055514, "dataset_tokens": 1975296, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00055514, "tokenizer_training_raw_mb": 15.461908994768004, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1492328424407040.0, "train_compute_hours": 0.1410928692166656, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.13900%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.11646%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.12670%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.66901%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.94883%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "hif_latn_10mb": {"language_name": "Fiji Hindi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hif", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.281123121, "dataset_raw_mb": 12.813024000884296, "dataset_scaled_mb": 10.00139939, "dataset_tokens": 3238912, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00139939, "tokenizer_training_raw_mb": 12.813024000884296, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2449817374556160.0, "train_compute_hours": 0.23161909723076424, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 74.06143%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.82293%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.11449%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00116%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "bos_cyrl_10mb": {"language_name": "Bosnian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bos", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.148940187, "dataset_raw_mb": 11.491063007722365, "dataset_scaled_mb": 10.0014458, "dataset_tokens": 2430464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0014458, "tokenizer_training_raw_mb": 11.491063007722365, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1836995004334080.0, "train_compute_hours": 0.17367952768249484, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "lat_latn_10mb": {"language_name": "Latin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.87538342, "dataset_raw_mb": 8.75484799904637, "dataset_scaled_mb": 10.00115812, "dataset_tokens": 1958400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00115812, "tokenizer_training_raw_mb": 8.75484799904637, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1479273165619200.0, "train_compute_hours": 0.13985855384036075, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.83255%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.81393%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.54910%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.52148%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.21200%: [eBible](https://ebible.org/find/)\n* 0.07094%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "abk_cyrl_10mb": {"language_name": "Abkhazian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "abk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.014757281, "dataset_raw_mb": 20.151425166954283, "dataset_scaled_mb": 10.00191207, "dataset_tokens": 2589696, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00191207, "tokenizer_training_raw_mb": 20.151425166954283, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1957204108247040.0, "train_compute_hours": 0.18504475205244744, "dataset_hugging_face": ["cis-lmu/Glot500", "Nart/abkhaz_text", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.32220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Nart/abkhaz](https://huggingface.co/datasets/Nart/abkhaz_text), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.67618%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Nart/abkhaz", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/Nart/abkhaz_text", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "zho_hans_10mb": {"language_name": "Chinese", "language_script": "Han Simplified", "dataset_category": "10mb", "language_iso6393": "zho", "language_iso15924": "hans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.9359663918, "dataset_raw_mb": 9.36042000101096, "dataset_scaled_mb": 10.00080781, "dataset_tokens": 2137088, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00080781, "tokenizer_training_raw_mb": 9.36042000101096, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1616140314869760.0, "train_compute_hours": 0.1527987206785955, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "shn_mymr_10mb": {"language_name": "Shan", "language_script": "Burmese", "dataset_category": "10mb", "language_iso6393": "shn", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.819448606, "dataset_raw_mb": 28.196112994623604, "dataset_scaled_mb": 10.00057704, "dataset_tokens": 1934336, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00057704, "tokenizer_training_raw_mb": 28.196112994623604, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1461336712151040.0, "train_compute_hours": 0.13816274369428017, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 67.30922%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 32.69078%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "xho_latn_10mb": {"language_name": "Xhosa", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "xho", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.198781752, "dataset_raw_mb": 11.989059002358005, "dataset_scaled_mb": 10.00103562, "dataset_tokens": 2765824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103562, "tokenizer_training_raw_mb": 11.989059002358005, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2090468470947840.0, "train_compute_hours": 0.19764429179870488, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 41.02036%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.08328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.57971%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536)\n* 0.21721%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09945%: [Lacuna Project: IsiXhosa](https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "CORP.NCHLT", "Mburisano_Covid", "Wikipedia 2023/08", "Lacuna Project: IsiXhosa"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://dumps.wikimedia.org/", "https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho"]}, "gsw_latn_10mb": {"language_name": "Swiss German", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "gsw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.142566025, "dataset_raw_mb": 11.426170005832054, "dataset_scaled_mb": 10.00044615, "dataset_tokens": 3087360, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00044615, "tokenizer_training_raw_mb": 11.426170005832054, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2335148039208960.0, "train_compute_hours": 0.22077763279793805, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.51719%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.37695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 11.72500%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 2.37834%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00253%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nde_latn_10mb": {"language_name": "North Ndebele", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nde", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.969100135, "dataset_raw_mb": 9.692559643635077, "dataset_scaled_mb": 10.00160798, "dataset_tokens": 1741824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00160798, "tokenizer_training_raw_mb": 9.692559643635077, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1315675664547840.0, "train_compute_hours": 0.12439115373906852, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 65.86973%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MoT](https://github.com/bltlab/mot)\n* 34.13027%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CORP.NCHLT", "Mburisano_Covid", "MoT", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://github.com/bltlab/mot", "https://ebible.org/find/"]}, "ewe_latn_10mb": {"language_name": "Ewe", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ewe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078257744, "dataset_raw_mb": 10.784441003641534, "dataset_scaled_mb": 10.00172831, "dataset_tokens": 2987520, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00172831, "tokenizer_training_raw_mb": 10.784441003641534, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2258482291015680.0, "train_compute_hours": 0.21352923478693703, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 56.57066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.38723%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.00459%: [eBible](https://ebible.org/find/)\n* 3.36620%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.60581%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.06551%: [Ewe Language Corpus](https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Ewe Language Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus"]}, "szl_latn_10mb": {"language_name": "Silesian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "szl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066563024, "dataset_raw_mb": 10.666472227513665, "dataset_scaled_mb": 10.00078944, "dataset_tokens": 2886656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00078944, "tokenizer_training_raw_mb": 10.666472227513665, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2183288649154560.0, "train_compute_hours": 0.20642001773824933, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 39.46050%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.03961%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 26.49989%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tuk_latn_10mb": {"language_name": "Turkmen", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tuk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.78535505, "dataset_raw_mb": 17.85559000034137, "dataset_scaled_mb": 10.00114235, "dataset_tokens": 3155456, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00114235, "tokenizer_training_raw_mb": 17.85559000034137, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2385548311265280.0, "train_compute_hours": 0.2255427494287174, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 83.09655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.30174%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23979%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.35026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01166%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "ell_grek_10mb": {"language_name": "Modern Greek", "language_script": "Greek", "dataset_category": "10mb", "language_iso6393": "ell", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.967262375, "dataset_raw_mb": 19.6755049828018, "dataset_scaled_mb": 10.00146459, "dataset_tokens": 2489344, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00146459, "tokenizer_training_raw_mb": 19.6755049828018, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1882669040271360.0, "train_compute_hours": 0.17799780017111042, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ayr_latn_10mb": {"language_name": "Central Aymara", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ayr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aym"], "language_code_individuals": [], "language_byte_premium": 1.097680523, "dataset_raw_mb": 10.977520357883929, "dataset_scaled_mb": 10.00065149, "dataset_tokens": 2749440, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00065149, "tokenizer_training_raw_mb": 10.977520357883929, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2078071786045440.0, "train_compute_hours": 0.1964722415897507, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain"]}, "tum_latn_10mb": {"language_name": "Tumbuka", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tum", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.20719527, "dataset_raw_mb": 12.074148044958257, "dataset_scaled_mb": 10.00181855, "dataset_tokens": 2748928, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00181855, "tokenizer_training_raw_mb": 12.074148044958257, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2077413212160000.0, "train_compute_hours": 0.19640997642240002, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 82.53031%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05112%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.41857%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "bar_latn_10mb": {"language_name": "Bavarian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bar", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129561954, "dataset_raw_mb": 11.296809364084245, "dataset_scaled_mb": 10.00105335, "dataset_tokens": 2929152, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00105335, "tokenizer_training_raw_mb": 11.296809364084245, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2213776746086400.0, "train_compute_hours": 0.20930252872089603, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 64.07962%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 35.91237%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00802%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "khk_cyrl_10mb": {"language_name": "Halh Mongolian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "khk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["mon"], "language_code_individuals": [], "language_byte_premium": 1.801810967, "dataset_raw_mb": 18.018122174568113, "dataset_scaled_mb": 10.00000694, "dataset_tokens": 2262016, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00000694, "tokenizer_training_raw_mb": 18.018122174568113, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1709270410199040.0, "train_compute_hours": 0.1616037478733638, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "vec_latn_10mb": {"language_name": "Venetian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "vec", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9963413972, "dataset_raw_mb": 9.964654994917526, "dataset_scaled_mb": 10.00124558, "dataset_tokens": 2761216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00124558, "tokenizer_training_raw_mb": 9.964654994917526, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2086710725836800.0, "train_compute_hours": 0.19728901407911564, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.87927%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.55145%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.45405%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.10163%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01360%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cjk_latn_10mb": {"language_name": "Chokwe", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cjk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.170350075, "dataset_raw_mb": 11.703671398744435, "dataset_scaled_mb": 10.00014581, "dataset_tokens": 3011072, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00014581, "tokenizer_training_raw_mb": 11.703671398744435, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2275760170598400.0, "train_compute_hours": 0.21516277976566692, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "oss_cyrl_10mb": {"language_name": "Ossetian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "oss", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.846676453, "dataset_raw_mb": 18.46764799848188, "dataset_scaled_mb": 10.00047841, "dataset_tokens": 3687936, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00047841, "tokenizer_training_raw_mb": 18.46764799848188, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2788440570593280.0, "train_compute_hours": 0.2636343812197283, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 71.94447%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.35828%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.22885%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.45171%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.01669%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hau_latn_10mb": {"language_name": "Hausa", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hau", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176529442, "dataset_raw_mb": 11.76723499942693, "dataset_scaled_mb": 10.00164941, "dataset_tokens": 2834944, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00164941, "tokenizer_training_raw_mb": 11.76723499942693, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2142495807897600.0, "train_compute_hours": 0.20256324001940948, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 31.25887%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.98045%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.12632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/)\n* 8.31691%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 3.07782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23963%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "TeDDi", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "ltg_latn_10mb": {"language_name": "Latgalian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ltg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.002770005, "dataset_raw_mb": 10.028230996662247, "dataset_scaled_mb": 10.00052948, "dataset_tokens": 2201088, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00052948, "tokenizer_training_raw_mb": 10.028230996662247, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1663441415700480.0, "train_compute_hours": 0.1572708247571363, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 65.03249%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.87600%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.09151%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tur_latn_10mb": {"language_name": "Turkish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.044489573, "dataset_raw_mb": 10.446234003155196, "dataset_scaled_mb": 10.00128127, "dataset_tokens": 1941504, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00128127, "tokenizer_training_raw_mb": 10.446234003155196, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1469007160934400.0, "train_compute_hours": 0.13888794976107055, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "gaz_latn_10mb": {"language_name": "West Central Oromo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "gaz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["orm"], "language_code_individuals": [], "language_byte_premium": 1.332217666, "dataset_raw_mb": 13.322587515928195, "dataset_scaled_mb": 10.0003084, "dataset_tokens": 3301376, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0003084, "tokenizer_training_raw_mb": 13.322587515928195, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2497932007833600.0, "train_compute_hours": 0.23616811710426766, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 96.08435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.91565%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "umb_latn_10mb": {"language_name": "Umbundu", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "umb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.167261104, "dataset_raw_mb": 11.67275986579076, "dataset_scaled_mb": 10.0001275, "dataset_tokens": 2974720, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0001275, "tokenizer_training_raw_mb": 11.67275986579076, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2248526203453440.0, "train_compute_hours": 0.2125879319628707, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 99.99900%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "aze_arab_10mb": {"language_name": "Azerbaijani", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "aze", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.198662364, "dataset_raw_mb": 11.987498999151184, "dataset_scaled_mb": 10.00073028, "dataset_tokens": 2203136, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00073028, "tokenizer_training_raw_mb": 11.987498999151184, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1666385628364800.0, "train_compute_hours": 0.15754918668176293, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.47341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 22.81190%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.00426%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.71043%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ell_latn_10mb": {"language_name": "Modern Greek", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ell", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238882746, "dataset_raw_mb": 12.38964200062784, "dataset_scaled_mb": 10.00065748, "dataset_tokens": 2552832, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00065748, "tokenizer_training_raw_mb": 12.38964200062784, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1929311567216640.0, "train_compute_hours": 0.18240763908230054, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mri_latn_10mb": {"language_name": "Maori", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "mri", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182543083, "dataset_raw_mb": 11.826532995629648, "dataset_scaled_mb": 10.00093203, "dataset_tokens": 3083264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00093203, "tokenizer_training_raw_mb": 11.826532995629648, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2332048867983360.0, "train_compute_hours": 0.2204846202456995, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 42.28220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 39.00578%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 17.44951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.04465%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.21786%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "MC4", "NLLB_seed", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "fra_latn_10mb": {"language_name": "French", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fra", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173979035, "dataset_raw_mb": 11.741447996657628, "dataset_scaled_mb": 10.00141199, "dataset_tokens": 2567680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00141199, "tokenizer_training_raw_mb": 11.741447996657628, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1942211867443200.0, "train_compute_hours": 0.18362730383099346, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "udm_cyrl_10mb": {"language_name": "Udmurt", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "udm", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740439775, "dataset_raw_mb": 17.407915004741298, "dataset_scaled_mb": 10.0020209, "dataset_tokens": 2169344, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0020209, "tokenizer_training_raw_mb": 17.407915004741298, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1640275110789120.0, "train_compute_hours": 0.15508055592915318, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.17663%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.77909%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 4.96507%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.07691%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00230%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nor_latn_10mb": {"language_name": "Norwegian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nor", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["nno", "nob"], "language_byte_premium": 1.125315993, "dataset_raw_mb": 11.253953030205547, "dataset_scaled_mb": 10.00070478, "dataset_tokens": 2615808, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00070478, "tokenizer_training_raw_mb": 11.253953030205547, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1978084774379520.0, "train_compute_hours": 0.18701892412315463, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.92843%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.07157%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "bua_cyrl_10mb": {"language_name": "Buriat", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bua", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bxm", "bxr", "bxu"], "language_byte_premium": 1.701900789, "dataset_raw_mb": 17.022054003089174, "dataset_scaled_mb": 10.00178983, "dataset_tokens": 2348544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00178983, "tokenizer_training_raw_mb": 17.022054003089174, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1775011579822080.0, "train_compute_hours": 0.16781927663772395, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 89.97116%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 8.41374%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.21634%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.38769%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00722%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00386%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Wortschatz Leipzig Data", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dik_latn_10mb": {"language_name": "Southwestern Dinka", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "dik", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["din"], "language_code_individuals": [], "language_byte_premium": 1.123839426, "dataset_raw_mb": 11.238807428326576, "dataset_scaled_mb": 10.00036764, "dataset_tokens": 3244544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00036764, "tokenizer_training_raw_mb": 11.238807428326576, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2452412930457600.0, "train_compute_hours": 0.23186449524326402, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 92.46146%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.53854%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "tgl_latn_10mb": {"language_name": "Tagalog", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tgl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.117593432, "dataset_raw_mb": 11.177954973628793, "dataset_scaled_mb": 10.00180804, "dataset_tokens": 2529280, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00180804, "tokenizer_training_raw_mb": 11.177954973628793, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1912343604756480.0, "train_compute_hours": 0.1808033953587945, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.29659%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.79155%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.61906%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29280%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TICO", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "iku_cans_10mb": {"language_name": "Inuktitut", "language_script": "Unified Canadian Aboriginal Syllabics", "dataset_category": "10mb", "language_iso6393": "iku", "language_iso15924": "cans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ike", "ikt"], "language_byte_premium": 2.158751414, "dataset_raw_mb": 21.590703997439384, "dataset_scaled_mb": 10.00147764, "dataset_tokens": 1916416, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00147764, "tokenizer_training_raw_mb": 21.590703997439384, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1449443642572800.0, "train_compute_hours": 0.13703830802506473, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 50.41059%: [Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0](https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60)\n* 42.46450%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.06461%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.06031%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0", "Glot500", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "zho_hant_10mb": {"language_name": "Chinese", "language_script": "Han Traditional", "dataset_category": "10mb", "language_iso6393": "zho", "language_iso15924": "hant", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.989382544, "dataset_raw_mb": 9.894847996540701, "dataset_scaled_mb": 10.00103353, "dataset_tokens": 2445824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103353, "tokenizer_training_raw_mb": 9.894847996540701, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1848074541465600.0, "train_compute_hours": 0.17472704755674764, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 36.42318%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.54996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 29.93702%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.08984%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "CCNet", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "pms_latn_10mb": {"language_name": "Piemontese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pms", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22629916, "dataset_raw_mb": 12.264589614961379, "dataset_scaled_mb": 10.00130312, "dataset_tokens": 3209216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00130312, "tokenizer_training_raw_mb": 12.264589614961379, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2426806028206080.0, "train_compute_hours": 0.22944347903039303, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.69647%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.63180%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04052%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dar_cyrl_10mb": {"language_name": "Dargwa", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "dar", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.015493724, "dataset_raw_mb": 20.15587811278024, "dataset_scaled_mb": 10.00046682, "dataset_tokens": 2684416, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00046682, "tokenizer_training_raw_mb": 20.15587811278024, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2029143620321280.0, "train_compute_hours": 0.19184630592128468, "dataset_hugging_face": [], "dataset_readme_str": "* 100.00000%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["Languages of Russia"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download"]}, "srd_latn_10mb": {"language_name": "Sardinian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "srd", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["sdc", "sdn", "src", "sro"], "language_byte_premium": 1.109372772, "dataset_raw_mb": 11.0945949277896, "dataset_scaled_mb": 10.00078171, "dataset_tokens": 2811904, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00078171, "tokenizer_training_raw_mb": 11.0945949277896, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2126186419322880.0, "train_compute_hours": 0.20102126146325414, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 44.15271%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.42496%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.42165%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00068%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "lus_latn_10mb": {"language_name": "Lushai", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.168842547, "dataset_raw_mb": 11.688939000973024, "dataset_scaled_mb": 10.00043935, "dataset_tokens": 3007488, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00043935, "tokenizer_training_raw_mb": 11.688939000973024, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2273319573258240.0, "train_compute_hours": 0.21493203238077907, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 54.32199%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 39.88323%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.79478%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download"]}, "lug_latn_10mb": {"language_name": "Ganda", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.217417428, "dataset_raw_mb": 12.175045999575147, "dataset_scaled_mb": 10.00071604, "dataset_tokens": 2915328, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00071604, "tokenizer_training_raw_mb": 12.175045999575147, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2204982847733760.0, "train_compute_hours": 0.20847110560391915, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 40.49929%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.20499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Makerere MT Corpus](https://zenodo.org/record/5089560#.Y00i3uxBw-S), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.27645%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.29489%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.85655%: [eBible](https://ebible.org/find/)\n* 0.49599%: [Makerere Radio Corpus](https://zenodo.org/record/5855017)\n* 0.37185%: [Makerere Parallel Corpus](https://zenodo.org/record/4764039)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CC100", "Wortschatz Leipzig Data", "Makerere MT Corpus", "TICO", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible", "Makerere Radio Corpus", "Makerere Parallel Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://zenodo.org/record/5089560#.Y00i3uxBw-S", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://zenodo.org/record/5855017", "https://zenodo.org/record/4764039"]}, "ina_latn_10mb": {"language_name": "Interlingua", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ina", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238452493, "dataset_raw_mb": 12.385430214003334, "dataset_scaled_mb": 10.00073098, "dataset_tokens": 2965504, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00073098, "tokenizer_training_raw_mb": 12.385430214003334, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2243799967334400.0, "train_compute_hours": 0.21214108782070693, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06085%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.53752%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.40163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "fry_latn_10mb": {"language_name": "Western Frisian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fry", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23134294, "dataset_raw_mb": 12.314741001872829, "dataset_scaled_mb": 10.00106518, "dataset_tokens": 2880512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00106518, "tokenizer_training_raw_mb": 12.314741001872829, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2177245265264640.0, "train_compute_hours": 0.20584864326138416, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.10455%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.78619%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.66588%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.44227%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00110%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lua_latn_10mb": {"language_name": "Luba-Lulua", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.185565668, "dataset_raw_mb": 11.855983504887698, "dataset_scaled_mb": 10.00027567, "dataset_tokens": 3083776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00027567, "tokenizer_training_raw_mb": 11.855983504887698, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2332707441868800.0, "train_compute_hours": 0.2205468854130502, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "deu_latn_10mb": {"language_name": "German", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "deu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.053648018, "dataset_raw_mb": 10.536995013494556, "dataset_scaled_mb": 10.00048862, "dataset_tokens": 2168832, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00048862, "tokenizer_training_raw_mb": 10.536995013494556, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1639616536903680.0, "train_compute_hours": 0.15501829076180249, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "wuu_hani_10mb": {"language_name": "Wu Chinese", "language_script": "Han", "dataset_category": "10mb", "language_iso6393": "wuu", "language_iso15924": "hani", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 7.000356999999999, "dataset_scaled_mb": 10.00051, "dataset_tokens": 1715200, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00051, "tokenizer_training_raw_mb": 7.000356999999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1296925678632960.0, "train_compute_hours": 0.12261842779802531, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 62.28265%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 37.41809%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29927%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "sun_latn_10mb": {"language_name": "Sundanese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.096980852, "dataset_raw_mb": 10.971365991474054, "dataset_scaled_mb": 10.00141978, "dataset_tokens": 2532864, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00141978, "tokenizer_training_raw_mb": 10.971365991474054, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1914474284974080.0, "train_compute_hours": 0.1810048414884585, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.54837%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 32.02343%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.03343%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.55888%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81131%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.02459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "MC4", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zza_latn_10mb": {"language_name": "Zaza", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "zza", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["diq", "kiu"], "language_byte_premium": 1.199780688, "dataset_raw_mb": 11.999181996637743, "dataset_scaled_mb": 10.00114614, "dataset_tokens": 3260416, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00114614, "tokenizer_training_raw_mb": 11.999181996637743, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2466940295577600.0, "train_compute_hours": 0.23323799158188221, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 35.78443%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.44703%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 19.35033%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.39981%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01839%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kum_cyrl_10mb": {"language_name": "Kumyk", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kum", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.964075989, "dataset_raw_mb": 19.64410099930565, "dataset_scaled_mb": 10.00170111, "dataset_tokens": 2011136, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00170111, "tokenizer_training_raw_mb": 19.64410099930565, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1521693071769600.0, "train_compute_hours": 0.14386916314912582, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 84.31257%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.68506%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00238%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "slk_latn_10mb": {"language_name": "Slovak", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "slk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.041506091, "dataset_raw_mb": 10.417508990897018, "dataset_scaled_mb": 10.00235052, "dataset_tokens": 2200576, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00235052, "tokenizer_training_raw_mb": 10.417508990897018, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1662782841815040.0, "train_compute_hours": 0.1572085595897856, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "chv_cyrl_10mb": {"language_name": "Chuvash", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "chv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.796969484, "dataset_raw_mb": 17.97134098983521, "dataset_scaled_mb": 10.00091607, "dataset_tokens": 3352064, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00091607, "tokenizer_training_raw_mb": 17.97134098983521, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2534308530094080.0, "train_compute_hours": 0.23960735193616758, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.68880%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.72870%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.16329%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.06333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.35083%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00505%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "TIL", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tat_cyrl_10mb": {"language_name": "Tatar", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "tat", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.854541191, "dataset_raw_mb": 18.546647998794626, "dataset_scaled_mb": 10.00066652, "dataset_tokens": 2400256, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00066652, "tokenizer_training_raw_mb": 18.546647998794626, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1816114338201600.0, "train_compute_hours": 0.17170535561178765, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 42.95952%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.38275%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.13842%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.01257%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.69512%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.81162%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "lij_latn_10mb": {"language_name": "Ligurian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.14375901, "dataset_raw_mb": 11.438480333387842, "dataset_scaled_mb": 10.00077834, "dataset_tokens": 3113472, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00077834, "tokenizer_training_raw_mb": 11.438480333387842, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2353239451238400.0, "train_compute_hours": 0.22248809357163057, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 58.41627%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.25415%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 20.32958%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tgk_cyrl_10mb": {"language_name": "Tajik", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "tgk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.746052186, "dataset_raw_mb": 17.46390600596534, "dataset_scaled_mb": 10.00193817, "dataset_tokens": 2288128, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00193817, "tokenizer_training_raw_mb": 17.46390600596534, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1730151076331520.0, "train_compute_hours": 0.163577919944071, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.68005%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.31925%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.47599%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.49619%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.02853%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kea_latn_10mb": {"language_name": "Kabuverdianu", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kea", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.781652324, "dataset_raw_mb": 7.817330741566354, "dataset_scaled_mb": 10.00103307, "dataset_tokens": 2084864, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103307, "tokenizer_training_raw_mb": 7.817330741566354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1577478153830400.0, "train_compute_hours": 0.14914338908941965, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.94529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.05471%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "tpi_latn_10mb": {"language_name": "Tok Pisin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tpi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176877977, "dataset_raw_mb": 11.770068227778, "dataset_scaled_mb": 10.00109481, "dataset_tokens": 3092480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00109481, "tokenizer_training_raw_mb": 11.770068227778, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2339874275328000.0, "train_compute_hours": 0.22122447694010183, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.07802%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.84121%: [eBible](https://ebible.org/find/)\n* 19.34190%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.73886%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Glot500", "BLOOM", "Earthlings", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nld_latn_10mb": {"language_name": "Dutch", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nld", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.051605721, "dataset_raw_mb": 10.517345006365938, "dataset_scaled_mb": 10.0012246, "dataset_tokens": 2228224, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0012246, "tokenizer_training_raw_mb": 10.517345006365938, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1685949146726400.0, "train_compute_hours": 0.15939882841776873, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "jpn_jpan_10mb": {"language_name": "Japanese", "language_script": "Japanese", "dataset_category": "10mb", "language_iso6393": "jpn", "language_iso15924": "jpan", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321973899, "dataset_raw_mb": 13.221827999594677, "dataset_scaled_mb": 10.00158022, "dataset_tokens": 2249728, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00158022, "tokenizer_training_raw_mb": 13.221827999594677, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1699972896522240.0, "train_compute_hours": 0.16072471021664816, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lbe_cyrl_10mb": {"language_name": "Lak", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "lbe", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.012712204, "dataset_raw_mb": 20.12894016318812, "dataset_scaled_mb": 10.00090332, "dataset_tokens": 2419712, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00090332, "tokenizer_training_raw_mb": 20.12894016318812, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1829983129436160.0, "train_compute_hours": 0.17301658678305515, "dataset_hugging_face": [], "dataset_readme_str": "* 99.11772%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.88228%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/"]}, "fas_arab_10mb": {"language_name": "Persian", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "fas", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pes", "prs"], "language_byte_premium": 1.590775443, "dataset_raw_mb": 15.90908202755371, "dataset_scaled_mb": 10.00083456, "dataset_tokens": 2493440, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00083456, "tokenizer_training_raw_mb": 15.90908202755371, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1885768211496960.0, "train_compute_hours": 0.17829081272334896, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "vie_latn_10mb": {"language_name": "Vietnamese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "vie", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.349331266, "dataset_raw_mb": 13.495151987421936, "dataset_scaled_mb": 10.00136314, "dataset_tokens": 2667520, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00136314, "tokenizer_training_raw_mb": 13.495151987421936, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "38516", "model_parameters": "32795648", "train_compute_flops": 2016088361533440.0, "train_compute_hours": 0.1906119905449798, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "diq_latn_10mb": {"language_name": "Dimli", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "diq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["zza"], "language_code_individuals": [], "language_byte_premium": 0.9584231474, "dataset_raw_mb": 9.58529341643155, "dataset_scaled_mb": 10.00110801, "dataset_tokens": 2688000, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00110801, "tokenizer_training_raw_mb": 9.58529341643155, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2031584217661440.0, "train_compute_hours": 0.19207705330617253, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.10048%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 30.14203%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 17.75749%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "ckb_arab_10mb": {"language_name": "Central Kurdish", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "ckb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.651286029, "dataset_raw_mb": 16.514262000658576, "dataset_scaled_mb": 10.00084886, "dataset_tokens": 2335744, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00084886, "tokenizer_training_raw_mb": 16.514262000658576, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1765055492259840.0, "train_compute_hours": 0.16687797381365763, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.06661%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.31684%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.60071%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 15.36201%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.39332%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.26051%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "TICO", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kan_knda_10mb": {"language_name": "Kannada", "language_script": "Kannada", "dataset_category": "10mb", "language_iso6393": "kan", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.641985282, "dataset_raw_mb": 26.422668991371637, "dataset_scaled_mb": 10.00106593, "dataset_tokens": 2210304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00106593, "tokenizer_training_raw_mb": 26.422668991371637, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1671266823045120.0, "train_compute_hours": 0.15801068145153863, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 49.18479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 40.78353%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.59325%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.16622%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.00209%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.27013%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "TeDDi", "W2C", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "cos_latn_10mb": {"language_name": "Corsican", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176702158, "dataset_raw_mb": 11.768128997934939, "dataset_scaled_mb": 10.00094112, "dataset_tokens": 3149312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00094112, "tokenizer_training_raw_mb": 11.768128997934939, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2382294181478400.0, "train_compute_hours": 0.22523508624886693, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 60.95981%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.95148%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.08866%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00005%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "kat_geor_10mb": {"language_name": "Georgian", "language_script": "Georgian", "dataset_category": "10mb", "language_iso6393": "kat", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 4.338956454, "dataset_raw_mb": 43.39232802136555, "dataset_scaled_mb": 10.0006369, "dataset_tokens": 3629568, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0006369, "tokenizer_training_raw_mb": 43.39232802136555, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2743735025664000.0, "train_compute_hours": 0.2594076751536873, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "allenai/nllb"], "dataset_readme_str": "* 60.04062%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.58420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.78020%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 7.59499%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb"]}, "ydd_hebr_10mb": {"language_name": "Eastern Yiddish", "language_script": "Hebrew", "dataset_category": "10mb", "language_iso6393": "ydd", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": ["yid"], "language_code_individuals": [], "language_byte_premium": 1.806848485, "dataset_raw_mb": 18.0710547486687, "dataset_scaled_mb": 10.00142231, "dataset_tokens": 2403328, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00142231, "tokenizer_training_raw_mb": 18.0710547486687, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1817586444533760.0, "train_compute_hours": 0.17184453657410095, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "afr_latn_10mb": {"language_name": "Afrikaans", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "afr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.03722598, "dataset_raw_mb": 10.372733998973537, "dataset_scaled_mb": 10.00045718, "dataset_tokens": 2460672, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00045718, "tokenizer_training_raw_mb": 10.372733998973537, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1860974841692160.0, "train_compute_hours": 0.1759467123054406, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.06252%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.34080%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 13.07209%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.39061%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.13398%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "Earthlings", "Mburisano_Covid", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://repo.sadilar.org/handle/20.500.12185/536", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bho_deva_10mb": {"language_name": "Bhojpuri", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "bho", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.515275311, "dataset_raw_mb": 25.153140009648336, "dataset_scaled_mb": 10.00015382, "dataset_tokens": 2645504, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00015382, "tokenizer_training_raw_mb": 25.153140009648336, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2001096120729600.0, "train_compute_hours": 0.18919454232352584, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.96562%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 39.06917%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.96521%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "ilo_latn_10mb": {"language_name": "Iloko", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ilo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.07650844, "dataset_raw_mb": 10.765113002829251, "dataset_scaled_mb": 10.00002657, "dataset_tokens": 2679808, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00002657, "tokenizer_training_raw_mb": 10.765113002829251, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2025385875210240.0, "train_compute_hours": 0.19149102820169545, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 41.64934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.63745%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.20790%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.12547%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.86976%: [eBible](https://ebible.org/find/)\n* 0.51008%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kha_latn_10mb": {"language_name": "Khasi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kha", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.303114193, "dataset_raw_mb": 13.032237002012089, "dataset_scaled_mb": 10.00084035, "dataset_tokens": 3059200, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00084035, "tokenizer_training_raw_mb": 13.032237002012089, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "42392", "model_parameters": "34892800", "train_compute_flops": 2314112414515200.0, "train_compute_hours": 0.21878881009961892, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 99.86112%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.13888%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/"]}, "mhr_cyrl_10mb": {"language_name": "Eastern Mari", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "mhr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["chm"], "language_code_individuals": [], "language_byte_premium": 1.810192209, "dataset_raw_mb": 18.106260269934637, "dataset_scaled_mb": 10.00239653, "dataset_tokens": 2406400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00239653, "tokenizer_training_raw_mb": 18.106260269934637, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1819368467988480.0, "train_compute_hours": 0.17201301879163813, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 85.57926%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 7.02298%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.37537%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.95016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07223%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "prs_arab_10mb": {"language_name": "Dari", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "prs", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.66404538, "dataset_raw_mb": 16.6422895083014, "dataset_scaled_mb": 10.00110316, "dataset_tokens": 2368000, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00110316, "tokenizer_training_raw_mb": 16.6422895083014, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "34817", "model_parameters": "31747072", "train_compute_flops": 1789190288179200.0, "train_compute_hours": 0.1691598090642153, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.28229%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.71771%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tico-19.github.io/"]}, "sin_sinh_10mb": {"language_name": "Sinhala", "language_script": "Sinhala", "dataset_category": "10mb", "language_iso6393": "sin", "language_iso15924": "sinh", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.446318518, "dataset_raw_mb": 24.46484199814269, "dataset_scaled_mb": 10.00067727, "dataset_tokens": 2403328, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00067727, "tokenizer_training_raw_mb": 24.46484199814269, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1817586444533760.0, "train_compute_hours": 0.17184453657410095, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.49015%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.08102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.71980%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.70904%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "amh_ethi_10mb": {"language_name": "Amharic", "language_script": "Ge'ez", "dataset_category": "10mb", "language_iso6393": "amh", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.720917143, "dataset_raw_mb": 17.209581008280033, "dataset_scaled_mb": 10.000238, "dataset_tokens": 2212864, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.000238, "tokenizer_training_raw_mb": 17.209581008280033, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1672080355491840.0, "train_compute_hours": 0.15808759724650126, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 35.99036%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [OSCAR](https://oscar-project.org/), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 33.89884%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.09942%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.48807%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.48971%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03361%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "OSCAR", "Parallel Corpora for Ethiopian Languages", "TICO", "W2C", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://oscar-project.org/", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "glk_arab_10mb": {"language_name": "Gilaki", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "glk", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.682310821, "dataset_raw_mb": 16.823153480984192, "dataset_scaled_mb": 10.00002691, "dataset_tokens": 2629632, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00002691, "tokenizer_training_raw_mb": 16.823153480984192, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1989667926835200.0, "train_compute_hours": 0.1881140585371462, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 50.16168%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 48.65632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.13696%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "som_latn_10mb": {"language_name": "Somali", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "som", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.42230286, "dataset_raw_mb": 14.223339003376168, "dataset_scaled_mb": 10.00021824, "dataset_tokens": 3095552, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00021824, "tokenizer_training_raw_mb": 14.223339003376168, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2341346381660160.0, "train_compute_hours": 0.22136365790241513, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 38.68291%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 30.77395%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.30160%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TICO](https://tico-19.github.io/)\n* 7.64665%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.41754%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.17735%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "Wortschatz Leipzig Data", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "smo_latn_10mb": {"language_name": "Samoan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "smo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.178001518, "dataset_raw_mb": 11.78163400146605, "dataset_scaled_mb": 10.00137421, "dataset_tokens": 3292160, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00137421, "tokenizer_training_raw_mb": 11.78163400146605, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2490106600488960.0, "train_compute_hours": 0.23542826040986534, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 43.50048%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.24768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.13951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.11208%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00025%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "fao_latn_10mb": {"language_name": "Faroese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fao", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155644359, "dataset_raw_mb": 11.557189997578591, "dataset_scaled_mb": 10.00064588, "dataset_tokens": 2471424, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00064588, "tokenizer_training_raw_mb": 11.557189997578591, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1867986716590080.0, "train_compute_hours": 0.1766096532048803, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 47.79193%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 42.53753%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.12327%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.54727%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "Ndc without informant codes", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "din_latn_10mb": {"language_name": "Dinka", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "din", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dib", "dik", "dip", "diw", "dks"], "language_byte_premium": 1.241578901, "dataset_raw_mb": 12.416031999406714, "dataset_scaled_mb": 10.00019571, "dataset_tokens": 3547136, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00019571, "tokenizer_training_raw_mb": 12.416031999406714, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2683882281369600.0, "train_compute_hours": 0.2537488702385804, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 73.62398%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.66747%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.00269%: [eBible](https://ebible.org/find/)\n* 2.70586%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "krc_cyrl_10mb": {"language_name": "Karachay-Balkar", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "krc", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.865302377, "dataset_raw_mb": 18.656172997309135, "dataset_scaled_mb": 10.00168832, "dataset_tokens": 2235904, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00168832, "tokenizer_training_raw_mb": 18.656172997309135, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1691178998169600.0, "train_compute_hours": 0.1598932870996713, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.46882%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.41364%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 11.40588%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.24320%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.46452%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00394%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pol_latn_10mb": {"language_name": "Polish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.077373477, "dataset_raw_mb": 10.775981007509667, "dataset_scaled_mb": 10.00208492, "dataset_tokens": 2247680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00208492, "tokenizer_training_raw_mb": 10.775981007509667, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1699817937960960.0, "train_compute_hours": 0.16071005958903622, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ban_latn_10mb": {"language_name": "Balinese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ban", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.269543636, "dataset_raw_mb": 12.696599033452722, "dataset_scaled_mb": 10.00091582, "dataset_tokens": 2938880, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00091582, "tokenizer_training_raw_mb": 12.696599033452722, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2222260727316480.0, "train_compute_hours": 0.21010465058264904, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 70.01948%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.06831%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.61178%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md)\n* 0.30043%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/indonlp"]}, "kac_latn_10mb": {"language_name": "Kachin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kac", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345469973, "dataset_raw_mb": 13.45565900972665, "dataset_scaled_mb": 10.00071297, "dataset_tokens": 3521536, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00071297, "tokenizer_training_raw_mb": 13.45565900972665, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "39602", "model_parameters": "33844224", "train_compute_flops": 2663660189122560.0, "train_compute_hours": 0.2518369633352239, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 57.50265%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 42.49735%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb"]}, "gla_latn_10mb": {"language_name": "Scottish Gaelic", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "gla", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9936126744, "dataset_raw_mb": 9.93764099977966, "dataset_scaled_mb": 10.00152399, "dataset_tokens": 2257408, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00152399, "tokenizer_training_raw_mb": 9.93764099977966, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1705512665088000.0, "train_compute_hours": 0.16124847015377455, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.49478%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.15694%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.70049%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 0.49333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.15447%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "haw_latn_10mb": {"language_name": "Hawaiian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "haw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.114669379, "dataset_raw_mb": 11.147742994846372, "dataset_scaled_mb": 10.00094127, "dataset_tokens": 3372544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00094127, "tokenizer_training_raw_mb": 11.147742994846372, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2549804386222080.0, "train_compute_hours": 0.24107241469736032, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.98608%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.94220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.78601%: [Ulukau](https://ulukau.org/index.php?l=en)\n* 1.02324%: [eBible](https://ebible.org/find/)\n* 0.26199%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00047%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "Ulukau", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ulukau.org/index.php?l=en", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "syr_syrc_10mb": {"language_name": "Syriac", "language_script": "Syriac", "dataset_category": "10mb", "language_iso6393": "syr", "language_iso15924": "syrc", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aii", "cld"], "language_byte_premium": 1.410191627, "dataset_raw_mb": 14.102341005616136, "dataset_scaled_mb": 10.00030119, "dataset_tokens": 2210304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00030119, "tokenizer_training_raw_mb": 14.102341005616136, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1671266823045120.0, "train_compute_hours": 0.15801068145153863, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "yua_latn_10mb": {"language_name": "Yucateco", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "yua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241346656, "dataset_raw_mb": 12.41453599255761, "dataset_scaled_mb": 10.00086151, "dataset_tokens": 3206656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00086151, "tokenizer_training_raw_mb": 12.41453599255761, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2425682578636800.0, "train_compute_hours": 0.22933726198020657, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "grc_grek_10mb": {"language_name": "Ancient Greek", "language_script": "Greek", "dataset_category": "10mb", "language_iso6393": "grc", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.76699504, "dataset_raw_mb": 17.673389007687742, "dataset_scaled_mb": 10.00194602, "dataset_tokens": 2419200, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00194602, "tokenizer_training_raw_mb": 17.673389007687742, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1829324555550720.0, "train_compute_hours": 0.17295432161570445, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 97.28398%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.44105%: [eBible](https://ebible.org/find/)\n* 0.26854%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00642%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "chm_cyrl_10mb": {"language_name": "Mari (Russia)", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "chm", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["mhr", "mrj"], "language_byte_premium": 1.756255296, "dataset_raw_mb": 17.566499002211817, "dataset_scaled_mb": 10.00224685, "dataset_tokens": 2350592, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00224685, "tokenizer_training_raw_mb": 17.566499002211817, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1777955792486400.0, "train_compute_hours": 0.16809763856235055, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 73.38471%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 12.77683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.02224%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.09681%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.65747%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.06194%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bjn_latn_10mb": {"language_name": "Banjar", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bjn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.166394033, "dataset_raw_mb": 11.665242527290262, "dataset_scaled_mb": 10.00111643, "dataset_tokens": 2742784, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00111643, "tokenizer_training_raw_mb": 11.665242527290262, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2074159082373120.0, "train_compute_hours": 0.19610231324254954, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 90.26394%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.99946%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.53524%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12237%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.07900%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://ebible.org/find/"]}, "khm_khmr_10mb": {"language_name": "Central Khmer", "language_script": "Khmer", "dataset_category": "10mb", "language_iso6393": "khm", "language_iso15924": "khmr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.903484241, "dataset_raw_mb": 39.03497598723073, "dataset_scaled_mb": 10.00003422, "dataset_tokens": 3212288, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00003422, "tokenizer_training_raw_mb": 39.03497598723073, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2428278134538240.0, "train_compute_hours": 0.22958265999270636, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 55.59600%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.62172%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.83408%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.36741%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/)\n* 1.58079%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "ukr_cyrl_10mb": {"language_name": "Ukrainian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "ukr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.751370357, "dataset_raw_mb": 17.515538007853035, "dataset_scaled_mb": 10.00104743, "dataset_tokens": 2247168, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00104743, "tokenizer_training_raw_mb": 17.515538007853035, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1699159364075520.0, "train_compute_hours": 0.16064779442168553, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cnh_latn_10mb": {"language_name": "Hakha Chin", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cnh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323849924, "dataset_raw_mb": 13.238538995213217, "dataset_scaled_mb": 10.00003003, "dataset_tokens": 3186688, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00003003, "tokenizer_training_raw_mb": 13.238538995213217, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2411155213516800.0, "train_compute_hours": 0.22796376564158838, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "tha_thai_10mb": {"language_name": "Thai", "language_script": "Thai", "dataset_category": "10mb", "language_iso6393": "tha", "language_iso15924": "thai", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.741647235, "dataset_raw_mb": 27.418336999117468, "dataset_scaled_mb": 10.00068012, "dataset_tokens": 2112512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00068012, "tokenizer_training_raw_mb": 27.418336999117468, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1597545287516160.0, "train_compute_hours": 0.15104064536516423, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 99.89966%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09995%: [eBible](https://ebible.org/find/)\n* 0.00039%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["OSCAR 2021/09", "eBible", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/", "https://tatoeba.org/en/"]}, "bug_latn_10mb": {"language_name": "Buginese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22786025, "dataset_raw_mb": 12.279871297110734, "dataset_scaled_mb": 10.00103334, "dataset_tokens": 3057664, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103334, "tokenizer_training_raw_mb": 12.279871297110734, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2312136692858880.0, "train_compute_hours": 0.21860201459756687, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 93.76215%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.23785%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/indonlp"]}, "gle_latn_10mb": {"language_name": "Irish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "gle", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.975875656, "dataset_raw_mb": 19.761079992183888, "dataset_scaled_mb": 10.0011759, "dataset_tokens": 4237312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0011759, "tokenizer_training_raw_mb": 19.761079992183888, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 3204698005831680.0, "train_compute_hours": 0.30298962964226794, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.77543%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.32257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.85422%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.21381%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.83397%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "mlt_latn_10mb": {"language_name": "Maltese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "mlt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.08846667, "dataset_raw_mb": 10.885712008966536, "dataset_scaled_mb": 10.00096035, "dataset_tokens": 2927616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00096035, "tokenizer_training_raw_mb": 10.885712008966536, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2214280361410560.0, "train_compute_hours": 0.20935014326063478, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 94.92627%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MaCoCu](https://macocu.eu/), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23828%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.42133%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.41412%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MaCoCu", "MC4", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://macocu.eu/", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "run_latn_10mb": {"language_name": "Rundi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "run", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119232514, "dataset_raw_mb": 11.193990580365481, "dataset_scaled_mb": 10.00148802, "dataset_tokens": 2714624, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00148802, "tokenizer_training_raw_mb": 11.193990580365481, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2053123457679360.0, "train_compute_hours": 0.1941134905442304, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 82.69665%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.93706%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.36629%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "bak_cyrl_10mb": {"language_name": "Bashkir", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bak", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.27194034, "dataset_raw_mb": 22.71975200652577, "dataset_scaled_mb": 10.00015344, "dataset_tokens": 3043840, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00015344, "tokenizer_training_raw_mb": 22.71975200652577, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2300553540403200.0, "train_compute_hours": 0.21750688018357528, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 29.93711%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 29.35888%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.91103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.93919%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.89615%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Languages of Russia", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kmr_latn_10mb": {"language_name": "Northern Kurdish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kmr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.034619587, "dataset_raw_mb": 10.347540523692441, "dataset_scaled_mb": 10.00129966, "dataset_tokens": 2366464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00129966, "tokenizer_training_raw_mb": 10.347540523692441, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1789693903503360.0, "train_compute_hours": 0.16920742360395405, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 98.85869%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.14131%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://tico-19.github.io/"]}, "que_latn_10mb": {"language_name": "Quechua", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "que", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["qub", "qud", "quf", "qug", "quh", "quk", "qul", "qup", "qur", "qus", "quw", "qux", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvj", "qvl", "qvm", "qvn", "qvo", "qvp", "qvs", "qvw", "qvz", "qwa", "qwc", "qwh", "qws", "qxa", "qxc", "qxh", "qxl", "qxn", "qxo", "qxp", "qxr", "qxt", "qxu", "qxw"], "language_byte_premium": 1.21478134, "dataset_raw_mb": 12.149456003032322, "dataset_scaled_mb": 10.00135218, "dataset_tokens": 2995712, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00135218, "tokenizer_training_raw_mb": 12.149456003032322, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2264680633466880.0, "train_compute_hours": 0.21411525989141414, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "Llamacha/monolingual-quechua-iic", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.54127%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.31458%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 7.98999%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Quechua-IIC](https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.28328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.76909%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09735%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00445%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM", "CC100", "Earthlings", "OSCAR", "Quechua-IIC", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nds_latn_10mb": {"language_name": "Low German", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nds", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.135906023, "dataset_raw_mb": 11.359636804538214, "dataset_scaled_mb": 10.00050759, "dataset_tokens": 2784768, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00050759, "tokenizer_training_raw_mb": 11.359636804538214, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2106467942400000.0, "train_compute_hours": 0.19915696909963637, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.35251%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.73725%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63050%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.27974%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "new_deva_10mb": {"language_name": "Newari", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "new", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.561603163, "dataset_raw_mb": 25.6179229896954, "dataset_scaled_mb": 10.00073835, "dataset_tokens": 2376704, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00073835, "tokenizer_training_raw_mb": 25.6179229896954, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1796047204515840.0, "train_compute_hours": 0.16980809933604307, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.70102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.62528%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 20.39344%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.65085%: [eBible](https://ebible.org/find/)\n* 3.62941%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bod_tibt_10mb": {"language_name": "Tibetan", "language_script": "Tibetan", "dataset_category": "10mb", "language_iso6393": "bod", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.616914857, "dataset_raw_mb": 26.172954009077603, "dataset_scaled_mb": 10.00145417, "dataset_tokens": 1868800, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00145417, "tokenizer_training_raw_mb": 26.172954009077603, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1411749972541440.0, "train_compute_hours": 0.13347454285846344, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 35.45938%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 27.32632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.94564%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.69915%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.63626%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.93326%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "MoT", "OSCAR", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "cym_latn_10mb": {"language_name": "Welsh", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cym", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.026556848, "dataset_raw_mb": 10.266063003230387, "dataset_scaled_mb": 10.00048173, "dataset_tokens": 2422784, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00048173, "tokenizer_training_raw_mb": 10.266063003230387, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1831765152890880.0, "train_compute_hours": 0.1731850690005923, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.76968%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.88536%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.87431%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.28932%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.18133%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "ace_latn_10mb": {"language_name": "Achinese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ace", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241956888, "dataset_raw_mb": 12.420854206022529, "dataset_scaled_mb": 10.00103492, "dataset_tokens": 3130368, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103492, "tokenizer_training_raw_mb": 12.420854206022529, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2366294710026240.0, "train_compute_hours": 0.22372240894793544, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 94.09064%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.78765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97639%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14531%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp"]}, "eus_latn_10mb": {"language_name": "Basque", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "eus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059583723, "dataset_raw_mb": 10.597770006073286, "dataset_scaled_mb": 10.00182409, "dataset_tokens": 2164224, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00182409, "tokenizer_training_raw_mb": 10.597770006073286, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1635548874670080.0, "train_compute_hours": 0.1546337117869894, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.69288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.18968%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.11744%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "glg_latn_10mb": {"language_name": "Galician", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "glg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059078607, "dataset_raw_mb": 10.591626999595531, "dataset_scaled_mb": 10.00079402, "dataset_tokens": 2284544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00079402, "tokenizer_training_raw_mb": 10.591626999595531, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1727710478991360.0, "train_compute_hours": 0.16334717255918313, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.77003%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [SLI_GalWeb.1.0](https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.22997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "SLI_GalWeb.1.0", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ron_latn_10mb": {"language_name": "Romanian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ron", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115120928, "dataset_raw_mb": 11.151796000876269, "dataset_scaled_mb": 10.00052615, "dataset_tokens": 2388480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00052615, "tokenizer_training_raw_mb": 11.151796000876269, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1804686144307200.0, "train_compute_hours": 0.170624871825408, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "urd_arab_10mb": {"language_name": "Urdu", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "urd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.707950617, "dataset_raw_mb": 17.082654981916516, "dataset_scaled_mb": 10.00184362, "dataset_tokens": 2564096, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00184362, "tokenizer_training_raw_mb": 17.082654981916516, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1940081187225600.0, "train_compute_hours": 0.18342585770132946, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 37.89138%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.70994%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 26.24866%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 0.15002%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "Anuvaad", "CCNet", "Earthlings", "OSCAR", "TICO", "W2C", "Workshop on NER for South and South East Asian Languages", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://ebible.org/find/"]}, "pap_latn_10mb": {"language_name": "Papiamento", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.001675271, "dataset_raw_mb": 10.017865000271177, "dataset_scaled_mb": 10.00111043, "dataset_tokens": 2404864, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00111043, "tokenizer_training_raw_mb": 10.017865000271177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1817082829209600.0, "train_compute_hours": 0.1717969220343622, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 68.24074%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.46139%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.28720%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.01066%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "war_latn_10mb": {"language_name": "Waray", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "war", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.093149717, "dataset_raw_mb": 10.932413994667646, "dataset_scaled_mb": 10.0008387, "dataset_tokens": 2921984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0008387, "tokenizer_training_raw_mb": 10.932413994667646, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2208895551406080.0, "train_compute_hours": 0.2088410339511203, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.84724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 25.81890%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.17011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.16375%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bel_cyrl_10mb": {"language_name": "Belarusian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bel", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.007901403, "dataset_raw_mb": 20.08014100482046, "dataset_scaled_mb": 10.00056127, "dataset_tokens": 2635264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00056127, "tokenizer_training_raw_mb": 20.08014100482046, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1991953565614080.0, "train_compute_hours": 0.18833015529442212, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.85869%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.04732%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 14.98527%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.10871%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ori_orya_10mb": {"language_name": "Odia", "language_script": "Oriya", "dataset_category": "10mb", "language_iso6393": "ori", "language_iso15924": "orya", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ory", "spv"], "language_byte_premium": 2.595114712, "dataset_raw_mb": 25.95364299752541, "dataset_scaled_mb": 10.00096176, "dataset_tokens": 2215424, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00096176, "tokenizer_training_raw_mb": 25.95364299752541, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1675683142041600.0, "train_compute_hours": 0.15842822433847856, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.28209%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 26.83885%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.83892%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.35162%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.43853%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.24999%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Workshop on NER for South and South East Asian Languages", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "fin_latn_10mb": {"language_name": "Finnish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.05886439, "dataset_raw_mb": 10.589035997483618, "dataset_scaled_mb": 10.0003703, "dataset_tokens": 1954816, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0003703, "tokenizer_training_raw_mb": 10.589035997483618, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1476832568279040.0, "train_compute_hours": 0.13962780645547287, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kab_latn_10mb": {"language_name": "Kabyle", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kab", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.028599141, "dataset_raw_mb": 10.287292063327802, "dataset_scaled_mb": 10.00126449, "dataset_tokens": 3166720, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00126449, "tokenizer_training_raw_mb": 10.287292063327802, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2393528677171200.0, "train_compute_hours": 0.22629725675073165, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 73.42841%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.37499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.19660%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "sme_latn_10mb": {"language_name": "Northern Sami", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sme", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.267947826, "dataset_raw_mb": 12.679712006181722, "dataset_scaled_mb": 10.00018435, "dataset_tokens": 2453504, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00018435, "tokenizer_training_raw_mb": 12.679712006181722, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1856403564134400.0, "train_compute_hours": 0.17551451879088875, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06677%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.99484%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97487%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.96230%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00123%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "orm_latn_10mb": {"language_name": "Oromo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "orm", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gax", "gaz", "hae", "orc"], "language_byte_premium": 1.264405846, "dataset_raw_mb": 12.64445100743895, "dataset_scaled_mb": 10.00031046, "dataset_tokens": 2964992, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00031046, "tokenizer_training_raw_mb": 12.64445100743895, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2242831476326400.0, "train_compute_hours": 0.21204952139813238, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "statmt/cc100", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 32.67081%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 27.67571%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.09955%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.69511%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.12785%: [eBible](https://ebible.org/find/)\n* 0.73097%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CC100", "CCNet", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "msa_latn_10mb": {"language_name": "Malay", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "msa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bjn", "btj", "bve", "bvu", "coa", "dup", "hji", "ind", "jak", "jax", "kvb", "kvr", "kxd", "lce", "lcf", "liw", "max", "meo", "mfa", "mfb", "min", "mqg", "msi", "mui", "orn", "ors", "pel", "pse", "tmw", "urk", "vkk", "vkt", "xmm", "zlm", "zmi", "zsm"], "language_byte_premium": 1.285705434, "dataset_raw_mb": 12.857749996639175, "dataset_scaled_mb": 10.00054107, "dataset_tokens": 2433536, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00054107, "tokenizer_training_raw_mb": 12.857749996639175, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1838777027788800.0, "train_compute_hours": 0.17384800990003202, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 82.84912%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.03127%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.11781%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.00181%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "TICO", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nso_latn_10mb": {"language_name": "Pedi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115696353, "dataset_raw_mb": 11.15699700089059, "dataset_scaled_mb": 10.00003, "dataset_tokens": 3006464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00003, "tokenizer_training_raw_mb": 11.15699700089059, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2274791679590400.0, "train_compute_hours": 0.21507121334309237, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 46.87464%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 28.49201%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.91655%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.71679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "asm_beng_10mb": {"language_name": "Assamese", "language_script": "Bengali", "dataset_category": "10mb", "language_iso6393": "asm", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.526329303, "dataset_raw_mb": 25.26684398794171, "dataset_scaled_mb": 10.00140558, "dataset_tokens": 2303488, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00140558, "tokenizer_training_raw_mb": 25.26684398794171, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1740920696340480.0, "train_compute_hours": 0.16459613856309993, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.77996%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 25.83555%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.99529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.32777%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.47875%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.58268%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CC100", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "ces_latn_10mb": {"language_name": "Czech", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ces", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035849214, "dataset_raw_mb": 10.360627014871563, "dataset_scaled_mb": 10.00206099, "dataset_tokens": 2145280, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00206099, "tokenizer_training_raw_mb": 10.360627014871563, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1622338657320960.0, "train_compute_hours": 0.1533847457830726, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "wol_latn_10mb": {"language_name": "Wolof", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "wol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078647211, "dataset_raw_mb": 10.786961006848387, "dataset_scaled_mb": 10.00045325, "dataset_tokens": 3275776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00045325, "tokenizer_training_raw_mb": 10.786961006848387, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2477709915586560.0, "train_compute_hours": 0.23425621020091114, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.24103%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 17.08456%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.36785%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.80354%: [eBible](https://ebible.org/find/)\n* 4.50302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "min_latn_10mb": {"language_name": "Minangkabau", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "min", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 0.949751483, "dataset_raw_mb": 9.49854199572638, "dataset_scaled_mb": 10.00108151, "dataset_tokens": 2451968, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00108151, "tokenizer_training_raw_mb": 9.49854199572638, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1854117925355520.0, "train_compute_hours": 0.17529842203361282, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.69385%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Minangkabau corpora](https://github.com/fajri91/minangNLP), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.18903%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 19.75253%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.74636%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.61822%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Minangkabau corpora", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/fajri91/minangNLP", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/indonlp"]}, "bew_latn_10mb": {"language_name": "Betawi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bew", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300925888, "dataset_raw_mb": 13.01170299451208, "dataset_scaled_mb": 10.00187875, "dataset_tokens": 2704384, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00187875, "tokenizer_training_raw_mb": 13.01170299451208, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2043980902563840.0, "train_compute_hours": 0.1932491035151267, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 51.56687%: [IndoNLP](https://huggingface.co/indonlp)\n* 46.73817%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.69496%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["IndoNLP", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/indonlp", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "gom_deva_10mb": {"language_name": "Goan Konkani", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "gom", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.736906273, "dataset_raw_mb": 17.370128999391934, "dataset_scaled_mb": 10.00061389, "dataset_tokens": 1950208, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00061389, "tokenizer_training_raw_mb": 17.370128999391934, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1473074823168000.0, "train_compute_hours": 0.13927252873588364, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.12288%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.53101%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.87562%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.47037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mlg_latn_10mb": {"language_name": "Malagasy", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "mlg", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bhr", "bmm", "bzc", "msh", "plt", "skg", "tdx", "tkg", "txy", "xmv", "xmw"], "language_byte_premium": 1.266754162, "dataset_raw_mb": 12.667934997837467, "dataset_scaled_mb": 10.00031054, "dataset_tokens": 3002880, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00031054, "tokenizer_training_raw_mb": 12.667934997837467, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2269561828147200.0, "train_compute_hours": 0.21457675466118983, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.72851%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.99233%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.22403%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.55221%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.26037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.24255%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TeDDi", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "aze_latn_10mb": {"language_name": "Azerbaijani", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "aze", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.29878361, "dataset_raw_mb": 12.987999006428204, "dataset_scaled_mb": 10.00012543, "dataset_tokens": 2409984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00012543, "tokenizer_training_raw_mb": 12.987999006428204, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1821499148206080.0, "train_compute_hours": 0.17221446492130213, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.37652%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 18.62348%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ven_latn_10mb": {"language_name": "Venda", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ven", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300531185, "dataset_raw_mb": 13.007801001661532, "dataset_scaled_mb": 10.00191395, "dataset_tokens": 2780672, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00191395, "tokenizer_training_raw_mb": 13.007801001661532, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2103368771174400.0, "train_compute_hours": 0.19886395654739783, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.17655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71722%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 9.05878%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.04745%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lim_latn_10mb": {"language_name": "Limburgan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lim", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.997886276, "dataset_raw_mb": 9.980460286118111, "dataset_scaled_mb": 10.00160091, "dataset_tokens": 2859008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00160091, "tokenizer_training_raw_mb": 9.980460286118111, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2163221515468800.0, "train_compute_hours": 0.20452276146250475, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 45.81871%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.97133%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.20996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tam_taml_10mb": {"language_name": "Tamil", "language_script": "Tamil", "dataset_category": "10mb", "language_iso6393": "tam", "language_iso15924": "taml", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.729099655, "dataset_raw_mb": 27.291728003289535, "dataset_scaled_mb": 10.00026802, "dataset_tokens": 2080768, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00026802, "tokenizer_training_raw_mb": 27.291728003289535, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1574378982604800.0, "train_compute_hours": 0.1488503765371811, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 36.08815%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.39717%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 19.80765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.41716%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.28987%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "OSCAR", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "fij_latn_10mb": {"language_name": "Fijian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.210673556, "dataset_raw_mb": 12.107523999046608, "dataset_scaled_mb": 10.00065124, "dataset_tokens": 3043840, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00065124, "tokenizer_training_raw_mb": 12.107523999046608, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2300553540403200.0, "train_compute_hours": 0.21750688018357528, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.40494%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 43.45086%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.42740%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.71680%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "sna_latn_10mb": {"language_name": "Shona", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sna", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119177764, "dataset_raw_mb": 11.192802001023834, "dataset_scaled_mb": 10.00091528, "dataset_tokens": 2586112, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00091528, "tokenizer_training_raw_mb": 11.192802001023834, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1954763510906880.0, "train_compute_hours": 0.18481400466755957, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.75185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 27.93918%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.33727%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.07291%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.53601%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.36278%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kur_arab_10mb": {"language_name": "Kurdish", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "kur", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.569891339, "dataset_raw_mb": 15.701771989440271, "dataset_scaled_mb": 10.00182089, "dataset_tokens": 2251776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00182089, "tokenizer_training_raw_mb": 15.701771989440271, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1702917109186560.0, "train_compute_hours": 0.16100307214127477, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 43.19057%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.23880%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.93282%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05661%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.32587%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.25533%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TICO", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nno_latn_10mb": {"language_name": "Norwegian Nynorsk", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nno", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 1.033289113, "dataset_raw_mb": 10.334695996759459, "dataset_scaled_mb": 10.00174672, "dataset_tokens": 2394112, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00174672, "tokenizer_training_raw_mb": 10.334695996759459, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1810070954311680.0, "train_compute_hours": 0.17113398113492248, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.90768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.53541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.55180%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00511%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "grn_latn_10mb": {"language_name": "Guarani", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "grn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gnw", "gug", "gui", "gun", "nhd"], "language_byte_premium": 0.992314188, "dataset_raw_mb": 9.924919997870335, "dataset_scaled_mb": 10.00179189, "dataset_tokens": 2616320, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00179189, "tokenizer_training_raw_mb": 9.924919997870335, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1978743348264960.0, "train_compute_hours": 0.18708118929050532, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.98971%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.31205%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.49453%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [GiossaMedia](https://github.com/sgongora27/giossa-gongora-guarani-2021), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.12356%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.05310%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.02706%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "GiossaMedia", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/sgongora27/giossa-gongora-guarani-2021", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "als_latn_10mb": {"language_name": "Tosk Albanian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "als", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["alb", "sqi"], "language_code_individuals": [], "language_byte_premium": 1.167247178, "dataset_raw_mb": 11.673438003869004, "dataset_scaled_mb": 10.00082778, "dataset_tokens": 2690560, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00082778, "tokenizer_training_raw_mb": 11.673438003869004, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2035187004211200.0, "train_compute_hours": 0.19241768039814983, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 85.38346%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.76858%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.84796%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "bxr_cyrl_10mb": {"language_name": "Russia Buriat", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bxr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["bua"], "language_code_individuals": [], "language_byte_premium": 1.588863716, "dataset_raw_mb": 15.890146056205175, "dataset_scaled_mb": 10.00094967, "dataset_tokens": 2441216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00094967, "tokenizer_training_raw_mb": 15.890146056205175, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1847106050457600.0, "train_compute_hours": 0.1746354811341731, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.66194%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.33383%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00423%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "luo_latn_10mb": {"language_name": "Luo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "luo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035745154, "dataset_raw_mb": 10.358067663362307, "dataset_scaled_mb": 10.00059486, "dataset_tokens": 2912256, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00059486, "tokenizer_training_raw_mb": 10.358067663362307, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2203510741401600.0, "train_compute_hours": 0.20833192464160583, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 88.95975%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.45137%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "mar_deva_10mb": {"language_name": "Marathi", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "mar", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.479356527, "dataset_raw_mb": 24.79854500799735, "dataset_scaled_mb": 10.00200848, "dataset_tokens": 2164736, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00200848, "tokenizer_training_raw_mb": 24.79854500799735, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1636517365678080.0, "train_compute_hours": 0.15472527820956394, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.53905%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 40.24253%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13450%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.08391%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Hindialect", "Indiccorp", "OSCAR", "TICO", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "nob_latn_10mb": {"language_name": "Norwegian Bokm\u00e5l", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nob", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 0.9976190383, "dataset_raw_mb": 9.976884995207797, "dataset_scaled_mb": 10.00069627, "dataset_tokens": 2123776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00069627, "tokenizer_training_raw_mb": 9.976884995207797, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1605525653422080.0, "train_compute_hours": 0.1517951526871785, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mal_mlym_10mb": {"language_name": "Malayalam", "language_script": "Malayalam", "dataset_category": "10mb", "language_iso6393": "mal", "language_iso15924": "mlym", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.884868527, "dataset_raw_mb": 28.84951406387912, "dataset_scaled_mb": 10.00028729, "dataset_tokens": 2520576, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00028729, "tokenizer_training_raw_mb": 28.84951406387912, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1905176771297280.0, "train_compute_hours": 0.18012580383174284, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 79.43378%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 20.26204%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30417%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "twi_latn_10mb": {"language_name": "Twi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "twi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aka"], "language_code_individuals": [], "language_byte_premium": 1.032409918, "dataset_raw_mb": 10.324420558883375, "dataset_scaled_mb": 10.00031129, "dataset_tokens": 3083264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00031129, "tokenizer_training_raw_mb": 10.324420558883375, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2332048867983360.0, "train_compute_hours": 0.2204846202456995, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 77.35013%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.40427%: [eBible](https://ebible.org/find/)\n* 7.58195%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.66365%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "sah_cyrl_10mb": {"language_name": "Yakut", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "sah", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.881497141, "dataset_raw_mb": 18.81691399935317, "dataset_scaled_mb": 10.00103247, "dataset_tokens": 2382848, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00103247, "tokenizer_training_raw_mb": 18.81691399935317, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1802090588405760.0, "train_compute_hours": 0.17037947381290824, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 39.11470%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.57614%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 19.21725%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 5.94791%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.13671%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00728%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fon_latn_10mb": {"language_name": "Fon", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fon", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.541191249, "dataset_raw_mb": 15.412457994642581, "dataset_scaled_mb": 10.00035395, "dataset_tokens": 3507200, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00035395, "tokenizer_training_raw_mb": 15.412457994642581, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2651418462781440.0, "train_compute_hours": 0.2506795637538816, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 81.63858%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.44701%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.91441%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [FFR](https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "FFR", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset", "https://wortschatz.uni-leipzig.de/en/download"]}, "yue_hant_10mb": {"language_name": "Yue Chinese", "language_script": "Han Traditional", "dataset_category": "10mb", "language_iso6393": "yue", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.862461402, "dataset_raw_mb": 8.625072996084302, "dataset_scaled_mb": 10.00053217, "dataset_tokens": 2089984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00053217, "tokenizer_training_raw_mb": 8.625072996084302, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1579105218723840.0, "train_compute_hours": 0.1492972206793449, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.60663%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 47.24904%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.14433%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "scn_latn_10mb": {"language_name": "Sicilian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "scn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.036272745, "dataset_raw_mb": 10.362731418924614, "dataset_scaled_mb": 10.00000383, "dataset_tokens": 2887680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00000383, "tokenizer_training_raw_mb": 10.362731418924614, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2184915714048000.0, "train_compute_hours": 0.20657384932817457, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.07639%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.21878%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.70271%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00212%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "snd_arab_10mb": {"language_name": "Sindhi", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "snd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.587969761, "dataset_raw_mb": 15.8829459927025, "dataset_scaled_mb": 10.00204562, "dataset_tokens": 2576896, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00204562, "tokenizer_training_raw_mb": 15.8829459927025, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1947248020684800.0, "train_compute_hours": 0.1841034492283811, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.13852%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.07857%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.09349%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 4.06666%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.62277%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "dyu_latn_10mb": {"language_name": "Dyula", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "dyu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.15445855, "dataset_raw_mb": 11.545061194645527, "dataset_scaled_mb": 10.00041205, "dataset_tokens": 3232768, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00041205, "tokenizer_training_raw_mb": 11.545061194645527, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2443773990666240.0, "train_compute_hours": 0.23104772275389907, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.41765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 31.58235%: [eBible](https://ebible.org/find/)\n* 0.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "eBible", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lzh_hant_10mb": {"language_name": "Literary Chinese", "language_script": "Han Traditional", "dataset_category": "10mb", "language_iso6393": "lzh", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 7.001552998999999, "dataset_scaled_mb": 10.00221857, "dataset_tokens": 1841152, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00221857, "tokenizer_training_raw_mb": 7.001552998999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1391682838855680.0, "train_compute_hours": 0.13157728658271886, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.82278%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 46.05529%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.12193%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "tel_latn_10mb": {"language_name": "Telugu", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tel", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.276370327, "dataset_raw_mb": 12.766032007661611, "dataset_scaled_mb": 10.0018245, "dataset_tokens": 2678272, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0018245, "tokenizer_training_raw_mb": 12.766032007661611, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2025889490534400.0, "train_compute_hours": 0.1915386427414342, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ast_latn_10mb": {"language_name": "Asturian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ast", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.748880208, "dataset_raw_mb": 17.489890128332608, "dataset_scaled_mb": 10.00062214, "dataset_tokens": 4232704, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00062214, "tokenizer_training_raw_mb": 17.489890128332608, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 3200630343598080.0, "train_compute_hours": 0.30260505066745486, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.70018%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.64321%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.37355%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.28306%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "quy_latn_10mb": {"language_name": "Ayacucho Quechua", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "quy", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.163741957, "dataset_raw_mb": 11.638228673233025, "dataset_scaled_mb": 10.00069526, "dataset_tokens": 2911232, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00069526, "tokenizer_training_raw_mb": 11.638228673233025, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2201883676508160.0, "train_compute_hours": 0.2081780930516806, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 86.82052%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.17948%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/"]}, "cat_latn_10mb": {"language_name": "Catalan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "cat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.092622079, "dataset_raw_mb": 10.928507014290542, "dataset_scaled_mb": 10.00209242, "dataset_tokens": 2442752, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00209242, "tokenizer_training_raw_mb": 10.928507014290542, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1846292518010880.0, "train_compute_hours": 0.1745585653392105, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pam_latn_10mb": {"language_name": "Pampanga", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "pam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.18679287, "dataset_raw_mb": 11.869399171969718, "dataset_scaled_mb": 10.00123903, "dataset_tokens": 3049472, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00123903, "tokenizer_training_raw_mb": 11.869399171969718, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2305938350407680.0, "train_compute_hours": 0.21801598949308976, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 91.78710%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.56188%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.58011%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.06645%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00446%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tsn_latn_10mb": {"language_name": "Tswana", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tsn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173940296, "dataset_raw_mb": 11.739854997450378, "dataset_scaled_mb": 10.00038506, "dataset_tokens": 3165184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00038506, "tokenizer_training_raw_mb": 11.739854997450378, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2394032292495360.0, "train_compute_hours": 0.22634487129047043, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 54.15070%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.72794%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.23011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.14046%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.74997%: [eBible](https://ebible.org/find/)\n* 0.00083%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "sqi_latn_10mb": {"language_name": "Albanian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sqi", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aae", "aat", "aln", "als"], "language_byte_premium": 1.335619892, "dataset_raw_mb": 13.35719295510462, "dataset_scaled_mb": 10.00074425, "dataset_tokens": 2828800, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00074425, "tokenizer_training_raw_mb": 13.35719295510462, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2139241678110720.0, "train_compute_hours": 0.202255576839559, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 62.65815%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 37.34185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "fuv_latn_10mb": {"language_name": "Nigerian Fulfulde", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fuv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["ful"], "language_code_individuals": [], "language_byte_premium": 1.110831498, "dataset_raw_mb": 11.109530507366687, "dataset_scaled_mb": 10.00109425, "dataset_tokens": 2944512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00109425, "tokenizer_training_raw_mb": 11.109530507366687, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2227335620198400.0, "train_compute_hours": 0.21058445863693964, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "ibo_latn_10mb": {"language_name": "Igbo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ibo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345020968, "dataset_raw_mb": 13.451702989529723, "dataset_scaled_mb": 10.00111025, "dataset_tokens": 3140096, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00111025, "tokenizer_training_raw_mb": 13.451702989529723, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2374778691256320.0, "train_compute_hours": 0.22452453080968846, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 31.78330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 31.30256%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.52382%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93791%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 4.92177%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.53064%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "oci_latn_10mb": {"language_name": "Occitan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "oci", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.014733462, "dataset_raw_mb": 10.148434996966191, "dataset_scaled_mb": 10.0010844, "dataset_tokens": 2745344, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0010844, "tokenizer_training_raw_mb": 10.148434996966191, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2074972614819840.0, "train_compute_hours": 0.19617922903751217, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.78812%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.07771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.38373%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.02047%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.72997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pan_guru_10mb": {"language_name": "Panjabi", "language_script": "Gurmukhi", "dataset_category": "10mb", "language_iso6393": "pan", "language_iso15924": "guru", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.220967698, "dataset_raw_mb": 22.209690017080387, "dataset_scaled_mb": 10.00000587, "dataset_tokens": 2234880, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00000587, "tokenizer_training_raw_mb": 22.209690017080387, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1689861850398720.0, "train_compute_hours": 0.1597687567649699, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.26136%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.38636%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.73324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.48906%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12998%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "srn_latn_10mb": {"language_name": "Sranan Tongo", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "srn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.060725469, "dataset_raw_mb": 10.608617000934347, "dataset_scaled_mb": 10.00128432, "dataset_tokens": 2716160, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00128432, "tokenizer_training_raw_mb": 10.608617000934347, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "22355", "model_parameters": "24407040", "train_compute_flops": 2052619842355200.0, "train_compute_hours": 0.19406587600449166, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.32395%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.83517%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.18414%: [eBible](https://ebible.org/find/)\n* 1.65674%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "arg_latn_10mb": {"language_name": "Aragonese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "arg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.192925647, "dataset_raw_mb": 11.929954498513085, "dataset_scaled_mb": 10.00058514, "dataset_tokens": 2978304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00058514, "tokenizer_training_raw_mb": 11.929954498513085, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2250966800793600.0, "train_compute_hours": 0.21281867934775855, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.30879%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 38.24094%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.44878%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00149%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "crh_latn_10mb": {"language_name": "Crimean Tatar", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "crh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.312089034, "dataset_raw_mb": 13.122057968031358, "dataset_scaled_mb": 10.0008899, "dataset_tokens": 2820096, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0008899, "tokenizer_training_raw_mb": 13.122057968031358, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2132384761774080.0, "train_compute_hours": 0.20160728656773122, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.04959%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.27361%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.67679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "epo_latn_10mb": {"language_name": "Esperanto", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "epo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9954937917, "dataset_raw_mb": 9.956139995663353, "dataset_scaled_mb": 10.00120752, "dataset_tokens": 2380800, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00120752, "tokenizer_training_raw_mb": 9.956139995663353, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1799146375741440.0, "train_compute_hours": 0.17010111188828161, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.12373%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.22117%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.49198%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 14.01395%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14917%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "mon_cyrl_10mb": {"language_name": "Mongolian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "mon", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.783772557, "dataset_raw_mb": 17.83882200147761, "dataset_scaled_mb": 10.00061467, "dataset_tokens": 2161152, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00061467, "tokenizer_training_raw_mb": 17.83882200147761, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1634076768337920.0, "train_compute_hours": 0.1544945308246761, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 63.59654%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.77330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 11.63016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ceb_latn_10mb": {"language_name": "Cebuano", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ceb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113380333, "dataset_raw_mb": 11.134130997832003, "dataset_scaled_mb": 10.0002943, "dataset_tokens": 2723328, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0002943, "tokenizer_training_raw_mb": 11.134130997832003, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2059980374016000.0, "train_compute_hours": 0.1947617808160582, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 36.46583%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 33.35484%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.15251%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.56324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.46359%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "swe_latn_10mb": {"language_name": "Swedish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "swe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.020981292, "dataset_raw_mb": 10.211991990161705, "dataset_scaled_mb": 10.00213429, "dataset_tokens": 2131968, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00213429, "tokenizer_training_raw_mb": 10.211991990161705, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1611723995873280.0, "train_compute_hours": 0.15238117779165558, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "azb_arab_10mb": {"language_name": "South Azerbaijani", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "azb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.490422093, "dataset_raw_mb": 14.906720367849962, "dataset_scaled_mb": 10.001677, "dataset_tokens": 2577408, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.001677, "tokenizer_training_raw_mb": 14.906720367849962, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1947906594570240.0, "train_compute_hours": 0.1841657143957318, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 46.02648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 42.37928%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.52167%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "OSCAR", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "roh_latn_10mb": {"language_name": "Romansh", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "roh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.273368995, "dataset_raw_mb": 12.736061001269759, "dataset_scaled_mb": 10.00186203, "dataset_tokens": 2795008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00186203, "tokenizer_training_raw_mb": 12.736061001269759, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2112821243412480.0, "train_compute_hours": 0.1997576448317254, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.92821%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71848%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.35087%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00179%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00065%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mon_latn_10mb": {"language_name": "Mongolian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "mon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.181843271, "dataset_raw_mb": 11.818732118174275, "dataset_scaled_mb": 10.00025334, "dataset_tokens": 2661376, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00025334, "tokenizer_training_raw_mb": 11.818732118174275, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2012834231746560.0, "train_compute_hours": 0.19030432736512934, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "bre_latn_10mb": {"language_name": "Breton", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bre", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01282485, "dataset_raw_mb": 10.128270002271567, "dataset_scaled_mb": 10.00002123, "dataset_tokens": 2734080, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00002123, "tokenizer_training_raw_mb": 10.128270002271567, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2067302166036480.0, "train_compute_hours": 0.19545402297072176, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.39389%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.93949%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.16716%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.31819%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.13530%: [eBible](https://ebible.org/find/)\n* 0.04596%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "azj_latn_10mb": {"language_name": "North Azerbaijani", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "azj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.076115956, "dataset_raw_mb": 10.763410084343422, "dataset_scaled_mb": 10.00209134, "dataset_tokens": 1880064, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00209134, "tokenizer_training_raw_mb": 10.763410084343422, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1422519592550400.0, "train_compute_hours": 0.13449276147749237, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "ltz_latn_10mb": {"language_name": "Luxembourgish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ltz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.225348993, "dataset_raw_mb": 12.25532300307957, "dataset_scaled_mb": 10.00149596, "dataset_tokens": 2828288, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00149596, "tokenizer_training_raw_mb": 12.25532300307957, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2138583104225280.0, "train_compute_hours": 0.2021933116722083, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.34921%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.85023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.94131%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.85925%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fil_latn_10mb": {"language_name": "Filipino", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "fil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.328161865, "dataset_raw_mb": 13.282859007084106, "dataset_scaled_mb": 10.00093389, "dataset_tokens": 2831360, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00093389, "tokenizer_training_raw_mb": 13.282859007084106, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2140055210557440.0, "train_compute_hours": 0.2023324926345216, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "div_thaa_10mb": {"language_name": "Dhivehi", "language_script": "Thaana", "dataset_category": "10mb", "language_iso6393": "div", "language_iso15924": "thaa", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.997934845, "dataset_raw_mb": 19.982258002556076, "dataset_scaled_mb": 10.00145628, "dataset_tokens": 1890304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00145628, "tokenizer_training_raw_mb": 19.982258002556076, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1428872893562880.0, "train_compute_hours": 0.1350934372095814, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.19489%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.14021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.85897%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.61771%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18821%: [eBible](https://ebible.org/find/)\n* 0.00002%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "yor_latn_10mb": {"language_name": "Yoruba", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "yor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.374948177, "dataset_raw_mb": 13.750605996376924, "dataset_scaled_mb": 10.00081765, "dataset_tokens": 3184128, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00081765, "tokenizer_training_raw_mb": 13.750605996376924, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2407242509844480.0, "train_compute_hours": 0.22759383729438723, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.33023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.88587%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.42418%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Menyo20K](https://github.com/uds-lsv/menyo-20k_MT), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 2.89553%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.95515%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.49782%: [eBible](https://ebible.org/find/)\n* 0.01122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Menyo20K", "OSCAR", "TeDDi", "W2C", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/uds-lsv/menyo-20k_MT", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zul_latn_10mb": {"language_name": "Zulu", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "zul", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.163840155, "dataset_raw_mb": 11.64015099960579, "dataset_scaled_mb": 10.00150317, "dataset_tokens": 2683904, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00150317, "tokenizer_training_raw_mb": 11.64015099960579, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2028485046435840.0, "train_compute_hours": 0.191784040753934, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 46.89015%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [isiZulu](https://zenodo.org/record/5035171#.YaippvHMJDZ), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 34.71066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.29050%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.78489%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.32380%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "isiZulu", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "TeDDi", "TICO", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://zenodo.org/record/5035171#.YaippvHMJDZ", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "kir_cyrl_10mb": {"language_name": "Kirghiz", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kir", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.963770708, "dataset_raw_mb": 19.639585996175708, "dataset_scaled_mb": 10.00095679, "dataset_tokens": 2309632, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00095679, "tokenizer_training_raw_mb": 19.639585996175708, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1746964080230400.0, "train_compute_hours": 0.1651675130399651, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.41839%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.62164%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.53594%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt)\n* 8.32078%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.10325%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "eng_latn_10mb": {"language_name": "English", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "eng", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.0, "dataset_raw_mb": 10.000887, "dataset_scaled_mb": 10.000887, "dataset_tokens": 2192384, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.000887, "tokenizer_training_raw_mb": 10.000887, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1656584499363840.0, "train_compute_hours": 0.15662253448530852, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kmb_latn_10mb": {"language_name": "Kimbundu", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kmb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.132798568, "dataset_raw_mb": 11.32892799848879, "dataset_scaled_mb": 10.00083185, "dataset_tokens": 3038208, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00083185, "tokenizer_training_raw_mb": 11.32892799848879, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2297957984501760.0, "train_compute_hours": 0.21726148217107552, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 68.73144%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.26856%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kaz_cyrl_10mb": {"language_name": "Kazakh", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kaz", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.764638392, "dataset_raw_mb": 17.65019998581547, "dataset_scaled_mb": 10.00216252, "dataset_tokens": 2080256, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00216252, "tokenizer_training_raw_mb": 17.65019998581547, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1573410491596800.0, "train_compute_hours": 0.14875881011460657, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 68.35707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.84524%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.79769%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "W2C", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb"]}, "kin_latn_10mb": {"language_name": "Kinyarwanda", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.133982717, "dataset_raw_mb": 11.341475005645384, "dataset_scaled_mb": 10.00145314, "dataset_tokens": 2482176, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00145314, "tokenizer_training_raw_mb": 11.341475005645384, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1878097762713600.0, "train_compute_hours": 0.17756560665655857, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.08100%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.87658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [KinyaSMT](https://github.com/pniyongabo/kinyarwandaSMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.34254%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.79880%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "BLOOM", "KinyaSMT", "Wortschatz Leipzig Data", "MoT", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/pniyongabo/kinyarwandaSMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "heb_hebr_10mb": {"language_name": "Hebrew", "language_script": "Hebrew", "dataset_category": "10mb", "language_iso6393": "heb", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.355477183, "dataset_raw_mb": 13.558063999427299, "dataset_scaled_mb": 10.00242879, "dataset_tokens": 2010112, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00242879, "tokenizer_training_raw_mb": 13.558063999427299, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1520066006876160.0, "train_compute_hours": 0.1437153315592006, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "alt_cyrl_10mb": {"language_name": "Southern Altai", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "alt", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.856580455, "dataset_raw_mb": 18.57018600561058, "dataset_scaled_mb": 10.00235996, "dataset_tokens": 2324992, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00235996, "tokenizer_training_raw_mb": 18.57018600561058, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1758043617361920.0, "train_compute_hours": 0.1662150329142179, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 58.22529%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.73797%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "ind_latn_10mb": {"language_name": "Indonesian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ind", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.178746238, "dataset_raw_mb": 11.789230004283581, "dataset_scaled_mb": 10.00149958, "dataset_tokens": 2166272, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00149958, "tokenizer_training_raw_mb": 11.789230004283581, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1638493087334400.0, "train_compute_hours": 0.15491207371161603, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lav_latn_10mb": {"language_name": "Latvian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lav", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ltg", "lvs"], "language_byte_premium": 1.288843709, "dataset_raw_mb": 12.890695994815015, "dataset_scaled_mb": 10.00175266, "dataset_tokens": 2522112, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00175266, "tokenizer_training_raw_mb": 12.890695994815015, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1907462410076160.0, "train_compute_hours": 0.18034190058901878, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aka_latn_10mb": {"language_name": "Akan", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "aka", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["fat", "twi"], "language_byte_premium": 1.574942782, "dataset_raw_mb": 15.750788995798198, "dataset_scaled_mb": 10.00086427, "dataset_tokens": 4636672, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00086427, "tokenizer_training_raw_mb": 15.750788995798198, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 3508261827379200.0, "train_compute_hours": 0.33169020913403346, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43728%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.29722%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.29716%: [eBible](https://ebible.org/find/)\n* 5.04766%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.84113%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Akuapem](https://zenodo.org/record/4432117#.Y00gXOxBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.07955%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Akuapem", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/4432117#.Y00gXOxBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "ton_latn_10mb": {"language_name": "Tonga (Tonga Islands)", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ton", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.270386963, "dataset_raw_mb": 12.704157004234899, "dataset_scaled_mb": 10.00022621, "dataset_tokens": 3600896, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00022621, "tokenizer_training_raw_mb": 12.704157004234899, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "48080", "model_parameters": "38038528", "train_compute_flops": 2722040827084800.0, "train_compute_hours": 0.25735658728801747, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.09550%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.64313%: [eBible](https://ebible.org/find/)\n* 1.76795%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.49248%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pbt_arab_10mb": {"language_name": "Southern Pashto", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "pbt", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["pus"], "language_code_individuals": [], "language_byte_premium": 1.736020956, "dataset_raw_mb": 17.36220897005544, "dataset_scaled_mb": 10.00115172, "dataset_tokens": 2789888, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00115172, "tokenizer_training_raw_mb": 17.36220897005544, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2108404924416000.0, "train_compute_hours": 0.19934010194478546, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "uzb_cyrl_10mb": {"language_name": "Uzbek", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "uzb", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.980686782, "dataset_raw_mb": 19.81051899820707, "dataset_scaled_mb": 10.00184339, "dataset_tokens": 2187264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00184339, "tokenizer_training_raw_mb": 19.81051899820707, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1654957434470400.0, "train_compute_hours": 0.15646870289538328, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "guj_gujr_10mb": {"language_name": "Gujarati", "language_script": "Gujarati", "dataset_category": "10mb", "language_iso6393": "guj", "language_iso15924": "gujr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.162862919, "dataset_raw_mb": 21.63316402142501, "dataset_scaled_mb": 10.00209668, "dataset_tokens": 2025472, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00209668, "tokenizer_training_raw_mb": 21.63316402142501, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1531145544007680.0, "train_compute_hours": 0.14476285143345338, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 86.12642%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 13.58996%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.28351%: [eBible](https://ebible.org/find/)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "XLSum", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "arb_arab_10mb": {"language_name": "Standard Arabic", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "arb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.465017978, "dataset_raw_mb": 14.653150001399137, "dataset_scaled_mb": 10.00202743, "dataset_tokens": 2046464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00202743, "tokenizer_training_raw_mb": 14.653150001399137, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1547299974021120.0, "train_compute_hours": 0.1462901793619968, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kbd_cyrl_10mb": {"language_name": "Kabardian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kbd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.783468787, "dataset_raw_mb": 17.836756996982615, "dataset_scaled_mb": 10.00116017, "dataset_tokens": 2855936, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00116017, "tokenizer_training_raw_mb": 17.836756996982615, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2158650237911040.0, "train_compute_hours": 0.20409056794795288, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 59.32058%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.38457%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.04549%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.24936%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nan_latn_10mb": {"language_name": "Min Nan Chinese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "nan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 1.14871192, "dataset_raw_mb": 11.488552999244305, "dataset_scaled_mb": 10.00124818, "dataset_tokens": 3804160, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00124818, "tokenizer_training_raw_mb": 11.488552999244305, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2877503003688960.0, "train_compute_hours": 0.2720548294396835, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 49.98250%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 42.06803%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.94947%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mkd_cyrl_10mb": {"language_name": "Macedonian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "mkd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.834986754, "dataset_raw_mb": 18.349961032575116, "dataset_scaled_mb": 10.00005095, "dataset_tokens": 2286592, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00005095, "tokenizer_training_raw_mb": 18.349961032575116, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1727865437552640.0, "train_compute_hours": 0.16336182318679507, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.44064%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.55936%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "guj_latn_10mb": {"language_name": "Gujarati", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "guj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.190454087, "dataset_raw_mb": 11.906877005195787, "dataset_scaled_mb": 10.00196239, "dataset_tokens": 2509312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00196239, "tokenizer_training_raw_mb": 11.906877005195787, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1897506322513920.0, "train_compute_hours": 0.17940059776495246, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400"], "dataset_readme_str": "* 99.66021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.33979%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "W2C", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mya_mymr_10mb": {"language_name": "Burmese", "language_script": "Burmese", "dataset_category": "10mb", "language_iso6393": "mya", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 5.0, "dataset_raw_mb": 50.001986, "dataset_scaled_mb": 10.0003972, "dataset_tokens": 4187136, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0003972, "tokenizer_training_raw_mb": 50.001986, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 3165880886231040.0, "train_compute_hours": 0.2993196474254802, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 56.80614%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.62193%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.24461%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 11.75396%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 4.08864%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.48472%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TeDDi", "TICO", "W2C", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "hin_latn_10mb": {"language_name": "Hindi", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.255306654, "dataset_raw_mb": 12.555048995586395, "dataset_scaled_mb": 10.00157926, "dataset_tokens": 2972160, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00157926, "tokenizer_training_raw_mb": 12.555048995586395, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2247712671006720.0, "train_compute_hours": 0.2125110161679081, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 74.45561%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.54439%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [IITB](https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "IITB", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "bos_latn_10mb": {"language_name": "Bosnian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "bos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.9699882939, "dataset_raw_mb": 9.701447016024265, "dataset_scaled_mb": 10.00161247, "dataset_tokens": 2363904, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00161247, "tokenizer_training_raw_mb": 9.701447016024265, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1786091116953600.0, "train_compute_hours": 0.16886679651197675, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 70.89860%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.71791%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.38348%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tel_telu_10mb": {"language_name": "Telugu", "language_script": "Telugu", "dataset_category": "10mb", "language_iso6393": "tel", "language_iso15924": "telu", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.619819498, "dataset_raw_mb": 26.200987969566818, "dataset_scaled_mb": 10.0010661, "dataset_tokens": 2165760, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0010661, "tokenizer_training_raw_mb": 26.200987969566818, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1637834513448960.0, "train_compute_hours": 0.15484980854426533, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.10762%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.59426%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29812%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "myv_cyrl_10mb": {"language_name": "Erzya", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "myv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.773695539, "dataset_raw_mb": 17.73891800185086, "dataset_scaled_mb": 10.00110651, "dataset_tokens": 2331648, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00110651, "tokenizer_training_raw_mb": 17.73891800185086, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1761956321034240.0, "train_compute_hours": 0.16658496126141906, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.71457%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.46832%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 13.56506%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.24591%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00155%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ben_beng_10mb": {"language_name": "Bengali", "language_script": "Bengali", "dataset_category": "10mb", "language_iso6393": "ben", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.430759774, "dataset_raw_mb": 24.307843027968797, "dataset_scaled_mb": 10.00010091, "dataset_tokens": 2024448, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00010091, "tokenizer_training_raw_mb": 24.307843027968797, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1529518479114240.0, "train_compute_hours": 0.14460901984352817, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ido_latn_10mb": {"language_name": "Ido", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ido", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.17576321, "dataset_raw_mb": 11.758163145211428, "dataset_scaled_mb": 10.00045166, "dataset_tokens": 2877952, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00045166, "tokenizer_training_raw_mb": 11.758163145211428, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2176431732817920.0, "train_compute_hours": 0.20577172746642156, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.56311%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 39.86649%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29534%: [Tatoeba](https://tatoeba.org/en/)\n* 0.27506%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kbp_latn_10mb": {"language_name": "Kabiy\u00e8", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "kbp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.441209593, "dataset_raw_mb": 14.41266500601989, "dataset_scaled_mb": 10.00039486, "dataset_tokens": 3401216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00039486, "tokenizer_training_raw_mb": 14.41266500601989, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2571498584801280.0, "train_compute_hours": 0.24312350256303014, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.53530%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.12198%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.69494%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.64778%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "tso_latn_10mb": {"language_name": "Tsonga", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "tso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.207440889, "dataset_raw_mb": 12.075028995417364, "dataset_scaled_mb": 10.00051357, "dataset_tokens": 3090432, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00051357, "tokenizer_training_raw_mb": 12.075028995417364, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2336930062663680.0, "train_compute_hours": 0.22094611501547523, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 57.07934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 29.98123%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.39404%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.54534%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "hin_deva_10mb": {"language_name": "Hindi", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "hin", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.370151128, "dataset_raw_mb": 23.70644200019762, "dataset_scaled_mb": 10.00208034, "dataset_tokens": 2381312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00208034, "tokenizer_training_raw_mb": 23.70644200019762, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1799804949626880.0, "train_compute_hours": 0.1701633770556323, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bew_cyrl_10mb": {"language_name": "Betawi", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "bew", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740538562, "dataset_raw_mb": 17.40876731237211, "dataset_scaled_mb": 10.0019429, "dataset_tokens": 2305536, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.0019429, "tokenizer_training_raw_mb": 17.40876731237211, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1743864909004800.0, "train_compute_hours": 0.16487450048772656, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 52.29436%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 47.70564%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lmo_latn_10mb": {"language_name": "Lombard", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lmo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9431924587, "dataset_raw_mb": 9.432356804944199, "dataset_scaled_mb": 10.00045825, "dataset_tokens": 2956288, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00045825, "tokenizer_training_raw_mb": 9.432356804944199, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2235974559989760.0, "train_compute_hours": 0.2114012311263046, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.48817%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.05952%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.79259%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.65971%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "vol_latn_10mb": {"language_name": "Volap\u00fck", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "vol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129514541, "dataset_raw_mb": 11.296028261155536, "dataset_scaled_mb": 10.00078162, "dataset_tokens": 2876928, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00078162, "tokenizer_training_raw_mb": 11.296028261155536, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2174804667924480.0, "train_compute_hours": 0.20561789587649631, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.27628%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.97146%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.60410%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14817%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kom_cyrl_10mb": {"language_name": "Komi", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "kom", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["koi", "kpv"], "language_byte_premium": 1.614292695, "dataset_raw_mb": 16.144893997934712, "dataset_scaled_mb": 10.00121852, "dataset_tokens": 2624512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00121852, "tokenizer_training_raw_mb": 16.144893997934712, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1984941690716160.0, "train_compute_hours": 0.1876672143949824, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.95323%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 25.40122%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.73519%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.04414%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.86405%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00218%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nep_deva_10mb": {"language_name": "Nepali", "language_script": "Devanagari", "dataset_category": "10mb", "language_iso6393": "nep", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dty", "npi"], "language_byte_premium": 2.629966681, "dataset_raw_mb": 26.30385400625249, "dataset_scaled_mb": 10.00159211, "dataset_tokens": 2264064, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00159211, "tokenizer_training_raw_mb": 26.30385400625249, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1712214622863360.0, "train_compute_hours": 0.16188210979799042, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 53.66687%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.99086%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 16.12038%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.97982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.24207%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Tatoeba", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "hat_latn_10mb": {"language_name": "Haitian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "hat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9657950702, "dataset_raw_mb": 9.659182003792948, "dataset_scaled_mb": 10.00127491, "dataset_tokens": 2456064, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00127491, "tokenizer_training_raw_mb": 9.659182003792948, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1857217096581120.0, "train_compute_hours": 0.17559143458585136, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 43.18761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CMU_Haitian_Creole](http://www.speech.cs.cmu.edu/haitian/text/), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 35.83733%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39761%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.81760%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.62603%: [eBible](https://ebible.org/find/)\n* 0.13368%: [CMU Haitian Creole](http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht)\n* 0.00014%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CMU_Haitian_Creole", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible", "CMU Haitian Creole"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "http://www.speech.cs.cmu.edu/haitian/text/", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht"]}, "slv_latn_10mb": {"language_name": "Slovenian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "slv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9721500484, "dataset_raw_mb": 9.722032998632013, "dataset_scaled_mb": 10.00054777, "dataset_tokens": 2060800, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00054777, "tokenizer_training_raw_mb": 9.722032998632013, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1556752446259200.0, "train_compute_hours": 0.14718386764632438, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "rus_latn_10mb": {"language_name": "Russian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "rus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182252535, "dataset_raw_mb": 11.823422998882249, "dataset_scaled_mb": 10.00075927, "dataset_tokens": 2831360, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00075927, "tokenizer_training_raw_mb": 11.823422998882249, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2140055210557440.0, "train_compute_hours": 0.2023324926345216, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "swa_latn_10mb": {"language_name": "Swahili", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "swa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["swc", "swh"], "language_byte_premium": 1.258733422, "dataset_raw_mb": 12.58967099566127, "dataset_scaled_mb": 10.00185645, "dataset_tokens": 2701312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00185645, "tokenizer_training_raw_mb": 12.58967099566127, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2042508796231680.0, "train_compute_hours": 0.1931099225528134, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 57.90197%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.61041%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 5.07304%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.23078%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18380%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "arz_arab_10mb": {"language_name": "Egyptian Arabic", "language_script": "Arabic", "dataset_category": "10mb", "language_iso6393": "arz", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.551170046, "dataset_raw_mb": 15.513407258936715, "dataset_scaled_mb": 10.00110033, "dataset_tokens": 2653184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00110033, "tokenizer_training_raw_mb": 15.513407258936715, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2006635889295360.0, "train_compute_hours": 0.18971830226065223, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 84.18988%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.34266%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.36232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.09533%: [eBible](https://ebible.org/find/)\n* 0.00981%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "ADD", "AraBench", "DART", "Habibi", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "QADI", "Tatoeba", "TeDDi", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "uzn_cyrl_10mb": {"language_name": "Northern Uzbek", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "uzn", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 2.010315146, "dataset_raw_mb": 20.1069311339186, "dataset_scaled_mb": 10.00188014, "dataset_tokens": 2224128, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00188014, "tokenizer_training_raw_mb": 20.1069311339186, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1682849975500800.0, "train_compute_hours": 0.1591058158655302, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "wln_latn_10mb": {"language_name": "Walloon", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "wln", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.218380386, "dataset_raw_mb": 12.185677997077354, "dataset_scaled_mb": 10.00153822, "dataset_tokens": 3185664, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00153822, "tokenizer_training_raw_mb": 12.185677997077354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2409528148623360.0, "train_compute_hours": 0.22780993405166314, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.85998%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 12.46237%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 5.48451%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.19218%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "por_latn_10mb": {"language_name": "Portuguese", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "por", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.097878434, "dataset_raw_mb": 10.979252003276532, "dataset_scaled_mb": 10.00042597, "dataset_tokens": 2304000, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00042597, "tokenizer_training_raw_mb": 10.979252003276532, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1741889187348480.0, "train_compute_hours": 0.16468770498567448, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "est_latn_10mb": {"language_name": "Estonian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "est", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ekk", "vro"], "language_byte_premium": 0.9677856419, "dataset_raw_mb": 9.679328991954858, "dataset_scaled_mb": 10.00152159, "dataset_tokens": 1967104, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00152159, "tokenizer_training_raw_mb": 9.679328991954858, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1486130081955840.0, "train_compute_hours": 0.14050684411218853, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "che_cyrl_10mb": {"language_name": "Chechen", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "che", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.831603287, "dataset_raw_mb": 18.318361002622073, "dataset_scaled_mb": 10.00127109, "dataset_tokens": 2823168, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00127109, "tokenizer_training_raw_mb": 18.318361002622073, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2133856868106240.0, "train_compute_hours": 0.20174646753004452, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.55150%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.11809%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.84904%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.36799%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.11311%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00027%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tyv_cyrl_10mb": {"language_name": "Tuvinian", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "tyv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.860256075, "dataset_raw_mb": 18.60435900374002, "dataset_scaled_mb": 10.00096667, "dataset_tokens": 2348032, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00096667, "tokenizer_training_raw_mb": 18.60435900374002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1774353005936640.0, "train_compute_hours": 0.16775701147037325, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.97941%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 35.15487%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.54558%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.31541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00397%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00076%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_cyrl_10mb": {"language_name": "Azerbaijani", "language_script": "Cyrillic", "dataset_category": "10mb", "language_iso6393": "aze", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.818803281, "dataset_raw_mb": 18.1918350000709, "dataset_scaled_mb": 10.00209049, "dataset_tokens": 2152960, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00209049, "tokenizer_training_raw_mb": 18.1918350000709, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1627878425886720.0, "train_compute_hours": 0.15390850572019898, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "sco_latn_10mb": {"language_name": "Scots", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "sco", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.193629502, "dataset_raw_mb": 11.936641984223643, "dataset_scaled_mb": 10.00029068, "dataset_tokens": 3004928, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00029068, "tokenizer_training_raw_mb": 11.936641984223643, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2272506040811520.0, "train_compute_hours": 0.21485511658581646, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 95.84087%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.15532%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00381%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "spa_latn_10mb": {"language_name": "Spanish", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "spa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.083831966, "dataset_raw_mb": 10.838559002613051, "dataset_scaled_mb": 10.00022083, "dataset_tokens": 2274304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00022083, "tokenizer_training_raw_mb": 10.838559002613051, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1718567923875840.0, "train_compute_hours": 0.16248278553007944, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ita_latn_10mb": {"language_name": "Italian", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "ita", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066886932, "dataset_raw_mb": 10.669130003152965, "dataset_scaled_mb": 10.00024434, "dataset_tokens": 2230784, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00024434, "tokenizer_training_raw_mb": 10.669130003152965, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 1686762679173120.0, "train_compute_hours": 0.15947574421273136, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lin_latn_10mb": {"language_name": "Lingala", "language_script": "Latin", "dataset_category": "10mb", "language_iso6393": "lin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.139208008, "dataset_raw_mb": 11.393604009944381, "dataset_scaled_mb": 10.00133771, "dataset_tokens": 2863616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.00133771, "tokenizer_training_raw_mb": 11.393604009944381, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "39087104", "train_compute_flops": 2164190006476800.0, "train_compute_hours": 0.2046143278850793, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 49.58207%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.44683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.44468%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Lingala_Song_Lyrics](https://github.com/espoirMur/songs_lyrics_webscrap), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.78539%: [eBible](https://ebible.org/find/)\n* 1.74103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "Lingala_Song_Lyrics", "MoT", "TICO", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/espoirMur/songs_lyrics_webscrap", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "kaa_latn_100mb": {"language_name": "Kara-Kalpak", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "kaa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22736123, "dataset_raw_mb": 122.73825406730366, "dataset_scaled_mb": 100.0017363, "dataset_tokens": 23462912, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.736123, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.19739164000256e+17, "train_compute_hours": 11.320793687296932, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 97.52670%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.65677%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81649%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "hmn_latn_100mb": {"language_name": "Hmong", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hmn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cqd", "hea", "hma", "hmc", "hmd", "hme", "hmg", "hmh", "hmi", "hmj", "hml", "hmm", "hmp", "hmq", "hms", "hmw", "hmy", "hmz", "hnj", "hrm", "huj", "mmr", "muq", "mww", "sfm"], "language_byte_premium": 1.18983824, "dataset_raw_mb": 118.98485094938495, "dataset_scaled_mb": 100.0008631, "dataset_tokens": 28903424, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 118.983824, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.47512939249664e+17, "train_compute_hours": 13.946677892695506, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4"], "dataset_readme_str": "* 54.54885%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 45.45115%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4"]}, "jav_latn_100mb": {"language_name": "Javanese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "jav", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.146845795, "dataset_raw_mb": 114.68486907856324, "dataset_scaled_mb": 100.0002525, "dataset_tokens": 24784896, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.6845795, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.26544776265728e+17, "train_compute_hours": 11.964233392396103, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.56951%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 38.81475%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.78232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 6.69194%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.05217%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.08930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "knc_arab_100mb": {"language_name": "Central Kanuri", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "knc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 2.50197962, "dataset_raw_mb": 250.1981063642241, "dataset_scaled_mb": 100.0000577, "dataset_tokens": 107119104, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 250.19796200000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.46667089297408e+17, "train_compute_hours": 51.68488844266403, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "isl_latn_100mb": {"language_name": "Icelandic", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "isl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.154319145, "dataset_raw_mb": 115.43229715679658, "dataset_scaled_mb": 100.0003315, "dataset_tokens": 23691264, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.4319145, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2090504904704e+17, "train_compute_hours": 11.431022818992874, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 80.12926%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.11564%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 8.86357%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.85697%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03456%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Ndc without informant codes", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "zsm_latn_100mb": {"language_name": "Standard Malay", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "zsm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.143822408, "dataset_raw_mb": 114.3843193540798, "dataset_scaled_mb": 100.0018172, "dataset_tokens": 21624832, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.38224079999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.103436251136e+17, "train_compute_hours": 10.432488192558546, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 91.23035%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.76965%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "srp_cyrl_100mb": {"language_name": "Serbian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "srp", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.424878645, "dataset_raw_mb": 142.48893786108326, "dataset_scaled_mb": 100.0007533, "dataset_tokens": 18430976, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 142.4878645, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.40520669184e+16, "train_compute_hours": 8.892195417739638, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaa_cyrl_100mb": {"language_name": "Kara-Kalpak", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "kaa", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.920886208, "dataset_raw_mb": 192.09324706234332, "dataset_scaled_mb": 100.0024084, "dataset_tokens": 21218304, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 192.0886208, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.08284121317376e+17, "train_compute_hours": 10.237771470006459, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 99.51383%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.48617%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "uzb_latn_100mb": {"language_name": "Uzbek", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "uzb", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.229496706, "dataset_raw_mb": 122.95036698693427, "dataset_scaled_mb": 100.0005664, "dataset_tokens": 26109440, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.94967059999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3328767844352e+17, "train_compute_hours": 12.601744143750983, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 76.42069%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.51914%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.42511%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.63507%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "uig_arab_100mb": {"language_name": "Uighur", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "uig", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.308625158, "dataset_raw_mb": 230.86376892173575, "dataset_scaled_mb": 100.0005428, "dataset_tokens": 26183680, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 230.86251579999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.33633106509824e+17, "train_compute_hours": 12.634402797292452, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 65.05923%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.33662%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.64713%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.70495%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 2.67723%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.57484%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "WikiMatrix", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "bul_cyrl_100mb": {"language_name": "Bulgarian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "bul", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.812315686, "dataset_raw_mb": 181.23306412290407, "dataset_scaled_mb": 100.0008252, "dataset_tokens": 22427136, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 181.2315686, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.14438332547072e+17, "train_compute_hours": 10.819624168086808, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lit_latn_100mb": {"language_name": "Lithuanian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lit", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.030036841, "dataset_raw_mb": 103.00381790178565, "dataset_scaled_mb": 100.0001299, "dataset_tokens": 20116480, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.0036841, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.02646484434944e+17, "train_compute_hours": 9.704758528394706, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "uzn_latn_100mb": {"language_name": "Northern Uzbek", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "uzn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 1.645764497, "dataset_raw_mb": 164.5772454271343, "dataset_scaled_mb": 100.0004835, "dataset_tokens": 35738624, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 164.5764497, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.82412148211712e+17, "train_compute_hours": 17.246239467289136, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "kur_latn_100mb": {"language_name": "Kurdish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "kur", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.288322619, "dataset_raw_mb": 128.83328895079185, "dataset_scaled_mb": 100.0007972, "dataset_tokens": 27687424, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.8322619, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.41291314675712e+17, "train_compute_hours": 13.358451569340044, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 71.61672%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.40823%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.33884%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.36208%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.27236%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00178%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "BLOOM", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "TICO", "W2C", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "kor_hang_100mb": {"language_name": "Korean", "language_script": "Hangul", "dataset_category": "100mb", "language_iso6393": "kor", "language_iso15924": "hang", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.293311204, "dataset_raw_mb": 129.33258804955432, "dataset_scaled_mb": 100.0011348, "dataset_tokens": 22707712, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 129.3311204, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.15877529059328e+17, "train_compute_hours": 10.955693656518285, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pnb_arab_100mb": {"language_name": "Western Panjabi", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "pnb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["lah"], "language_code_individuals": [], "language_byte_premium": 1.414617177, "dataset_raw_mb": 141.46259928942473, "dataset_scaled_mb": 100.0006232, "dataset_tokens": 24761344, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 141.4617177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.26391136550912e+17, "train_compute_hours": 11.94970745572259, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.15181%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.73099%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.11696%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00023%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kal_latn_100mb": {"language_name": "Kalaallisut", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "kal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.341175059, "dataset_raw_mb": 134.12067201196177, "dataset_scaled_mb": 100.0023607, "dataset_tokens": 21407744, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.1175059, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.09301592489984e+17, "train_compute_hours": 10.33396874450758, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.53226%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.83885%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.54939%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.07923%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tir_ethi_100mb": {"language_name": "Tigrinya", "language_script": "Ge'ez", "dataset_category": "100mb", "language_iso6393": "tir", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.762985532, "dataset_raw_mb": 176.3016689243307, "dataset_scaled_mb": 100.0017673, "dataset_tokens": 22338048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.29855320000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.14032546021376e+17, "train_compute_hours": 10.78125889656646, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 50.52196%: [Tigrinya Language Modeling Dataset](https://zenodo.org/record/5139094)\n* 20.99445%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.46805%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.08565%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 4.86709%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.06280%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Tigrinya Language Modeling Dataset", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://zenodo.org/record/5139094", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "nya_latn_100mb": {"language_name": "Nyanja", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "nya", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.21429492, "dataset_raw_mb": 121.4322710353539, "dataset_scaled_mb": 100.0022886, "dataset_tokens": 25321984, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.429492, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2926900699136e+17, "train_compute_hours": 12.221797024637674, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.17651%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.50780%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.70728%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.45060%: [eBible](https://ebible.org/find/)\n* 0.15782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "hye_armn_100mb": {"language_name": "Armenian", "language_script": "Armenian", "dataset_category": "100mb", "language_iso6393": "hye", "language_iso15924": "armn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.724226492, "dataset_raw_mb": 172.42616179420952, "dataset_scaled_mb": 100.0020372, "dataset_tokens": 20388352, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 172.42264920000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.04082545442816e+17, "train_compute_hours": 9.840531569138967, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 52.58958%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 24.27630%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.13412%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "sot_latn_100mb": {"language_name": "Southern Sotho", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "sot", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.166011747, "dataset_raw_mb": 116.60277097008164, "dataset_scaled_mb": 100.001369, "dataset_tokens": 28273152, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.6011747, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.44338241060864e+17, "train_compute_hours": 13.646524609390779, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 34.03982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.19565%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.65729%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.10724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "pes_arab_100mb": {"language_name": "Iranian Persian", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "pes", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.59732627, "dataset_raw_mb": 159.73472109473997, "dataset_scaled_mb": 100.001311, "dataset_tokens": 21595648, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 159.73262699999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10209582301184e+17, "train_compute_hours": 10.419815053930124, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dan_latn_100mb": {"language_name": "Danish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "dan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.021065783, "dataset_raw_mb": 102.10722208197619, "dataset_scaled_mb": 100.0006305, "dataset_tokens": 20802048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.10657830000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.06210507751424e+17, "train_compute_hours": 10.041720732861906, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hrv_latn_100mb": {"language_name": "Croatian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hrv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.989672823, "dataset_raw_mb": 98.96816399951801, "dataset_scaled_mb": 100.0008909, "dataset_tokens": 21938176, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 98.96728230000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1195840987136e+17, "train_compute_hours": 10.585158751474037, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.01986%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.11618%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.57132%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.27830%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.96930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "pus_arab_100mb": {"language_name": "Pushto", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "pus", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pbt", "pbu", "pst"], "language_byte_premium": 1.585665259, "dataset_raw_mb": 158.5665339868928, "dataset_scaled_mb": 100.0000051, "dataset_tokens": 23777792, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 158.5665259, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.21363355271168e+17, "train_compute_hours": 11.474353589274067, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.56138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95008%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.68420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.73176%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/)\n* 4.07251%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00007%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Tatoeba", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "rus_cyrl_100mb": {"language_name": "Russian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "rus", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.822744411, "dataset_raw_mb": 182.2787500677876, "dataset_scaled_mb": 100.002364, "dataset_tokens": 22044160, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 182.2744411, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.12532729757696e+17, "train_compute_hours": 10.639458086182168, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lao_laoo_100mb": {"language_name": "Lao", "language_script": "Lao", "dataset_category": "100mb", "language_iso6393": "lao", "language_iso15924": "laoo", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.706914388, "dataset_raw_mb": 270.6961618242242, "dataset_scaled_mb": 100.0017448, "dataset_tokens": 23282688, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 270.6914388, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18819416047616e+17, "train_compute_hours": 11.233835699047331, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.18671%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.72107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.03480%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.64542%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 0.41139%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00061%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://dumps.wikimedia.org/"]}, "srp_latn_100mb": {"language_name": "Serbian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "srp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.826257686, "dataset_raw_mb": 82.62577289653997, "dataset_scaled_mb": 100.0000052, "dataset_tokens": 20746752, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 82.6257686, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05894866976768e+17, "train_compute_hours": 10.011878332348976, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.93765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.75254%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30981%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "hun_latn_100mb": {"language_name": "Hungarian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01995535, "dataset_raw_mb": 101.99709808157388, "dataset_scaled_mb": 100.0015325, "dataset_tokens": 19105280, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.995535, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.7500337864704e+16, "train_compute_hours": 9.218213761753834, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "plt_latn_100mb": {"language_name": "Plateau Malagasy", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "plt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.151182211, "dataset_raw_mb": 115.11959607203283, "dataset_scaled_mb": 100.0011944, "dataset_tokens": 26322944, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.1182211, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.34385627561984e+17, "train_compute_hours": 12.705550242223943, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 95.80021%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.19979%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TeDDi](https://github.com/MorphDiv/TeDDi_sample)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "TeDDi"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/MorphDiv/TeDDi_sample"]}, "san_deva_100mb": {"language_name": "Sanskrit", "language_script": "Devanagari", "dataset_category": "100mb", "language_iso6393": "san", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 2.542780452, "dataset_raw_mb": 254.2815560169701, "dataset_scaled_mb": 100.0013807, "dataset_tokens": 26692608, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 254.2780452, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.36230349307904e+17, "train_compute_hours": 12.879960298201834, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 39.60135%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 37.07458%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.73656%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.30509%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 5.49767%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 0.78475%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "CCNet", "Hindialect", "Wortschatz Leipzig Data", "OSCAR", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://ebible.org/find/"]}, "yid_hebr_100mb": {"language_name": "Yiddish", "language_script": "Hebrew", "dataset_category": "100mb", "language_iso6393": "yid", "language_iso15924": "hebr", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ydd", "yih"], "language_byte_premium": 1.546105069, "dataset_raw_mb": 154.61108900855848, "dataset_scaled_mb": 100.0003765, "dataset_tokens": 19216384, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 154.6105069, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.805244792832e+16, "train_compute_hours": 9.270413258677529, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.13900%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.11646%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.12670%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.66901%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.94883%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "bos_cyrl_100mb": {"language_name": "Bosnian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "bos", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.148940187, "dataset_raw_mb": 114.89491498823988, "dataset_scaled_mb": 100.0007801, "dataset_tokens": 23256576, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.8940187, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18720125075456e+17, "train_compute_hours": 11.224448188952204, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "lat_latn_100mb": {"language_name": "Latin", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.87538342, "dataset_raw_mb": 87.5399789669954, "dataset_scaled_mb": 100.00187, "dataset_tokens": 18896384, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 87.538342, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.6460134285312e+16, "train_compute_hours": 9.119867241520408, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.83255%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.81393%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.54910%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.52148%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.21200%: [eBible](https://ebible.org/find/)\n* 0.07094%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "zho_hans_100mb": {"language_name": "Chinese", "language_script": "Han Simplified", "dataset_category": "100mb", "language_iso6393": "zho", "language_iso15924": "hans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.9359663918, "dataset_raw_mb": 93.59749193897956, "dataset_scaled_mb": 100.0009111, "dataset_tokens": 20636160, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 93.59663918, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0535556022272e+17, "train_compute_hours": 9.960889330148074, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "xho_latn_100mb": {"language_name": "Xhosa", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "xho", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.198781752, "dataset_raw_mb": 119.87853903026172, "dataset_scaled_mb": 100.0003035, "dataset_tokens": 26783744, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.87817519999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.36745617195008e+17, "train_compute_hours": 12.928676534800758, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 41.02036%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.08328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.57971%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536)\n* 0.21721%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09945%: [Lacuna Project: IsiXhosa](https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "CORP.NCHLT", "Mburisano_Covid", "Wikipedia 2023/08", "Lacuna Project: IsiXhosa"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://dumps.wikimedia.org/", "https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho"]}, "gsw_latn_100mb": {"language_name": "Swiss German", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "gsw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.142566025, "dataset_raw_mb": 114.25743006057192, "dataset_scaled_mb": 100.0007243, "dataset_tokens": 29948928, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.2566025, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.5283232243712e+17, "train_compute_hours": 14.449601394054984, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.51719%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.37695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 11.72500%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 2.37834%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00253%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ell_grek_100mb": {"language_name": "Modern Greek", "language_script": "Greek", "dataset_category": "100mb", "language_iso6393": "ell", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.967262375, "dataset_raw_mb": 196.72695594421936, "dataset_scaled_mb": 100.0003652, "dataset_tokens": 23875584, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 196.7262375, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.21862423052288e+17, "train_compute_hours": 11.521538179489049, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tuk_latn_100mb": {"language_name": "Turkmen", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "tuk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.78535505, "dataset_raw_mb": 178.53632197847088, "dataset_scaled_mb": 100.0004576, "dataset_tokens": 30660608, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 178.535505, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.56497204477952e+17, "train_compute_hours": 14.796099332460917, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 83.09655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.30174%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23979%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.35026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01166%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "khk_cyrl_100mb": {"language_name": "Halh Mongolian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "khk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["mon"], "language_code_individuals": [], "language_byte_premium": 1.801810967, "dataset_raw_mb": 180.18198373153905, "dataset_scaled_mb": 100.0004923, "dataset_tokens": 21806592, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 180.1810967, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.11278789296128e+17, "train_compute_hours": 10.520903715270284, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "vec_latn_100mb": {"language_name": "Venetian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "vec", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9963413972, "dataset_raw_mb": 99.6355050066166, "dataset_scaled_mb": 100.0013703, "dataset_tokens": 26904064, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.63413972000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.37300340178944e+17, "train_compute_hours": 12.981123071463797, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.87927%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.55145%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.45405%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.10163%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01360%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hau_latn_100mb": {"language_name": "Hausa", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hau", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176529442, "dataset_raw_mb": 117.65479499846522, "dataset_scaled_mb": 100.0015731, "dataset_tokens": 27744768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.65294420000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.41624984600576e+17, "train_compute_hours": 13.389998544054459, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 31.25887%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.98045%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.12632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/)\n* 8.31691%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 3.07782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23963%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "TeDDi", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "tur_latn_100mb": {"language_name": "Turkish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "tur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.044489573, "dataset_raw_mb": 104.45179298474173, "dataset_scaled_mb": 100.0027149, "dataset_tokens": 18708992, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 104.44895729999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.5471143944192e+16, "train_compute_hours": 9.026362700178154, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_arab_100mb": {"language_name": "Azerbaijani", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "aze", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.198662364, "dataset_raw_mb": 119.86840406101906, "dataset_scaled_mb": 100.0018084, "dataset_tokens": 21145600, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.8662364, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07963777286144e+17, "train_compute_hours": 10.207484397962705, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.47341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 22.81190%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.00426%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.71043%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ell_latn_100mb": {"language_name": "Modern Greek", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ell", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238882746, "dataset_raw_mb": 123.8901520029133, "dataset_scaled_mb": 100.0015154, "dataset_tokens": 24510976, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 123.8882746, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.25088073187328e+17, "train_compute_hours": 11.826508737711013, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mri_latn_100mb": {"language_name": "Maori", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "mri", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182543083, "dataset_raw_mb": 118.25623797380285, "dataset_scaled_mb": 100.0016318, "dataset_tokens": 30222848, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 118.2543083, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.54288241639424e+17, "train_compute_hours": 14.587251936818271, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 42.28220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 39.00578%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 17.44951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.04465%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.21786%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "MC4", "NLLB_seed", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "fra_latn_100mb": {"language_name": "French", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "fra", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173979035, "dataset_raw_mb": 117.39909896285133, "dataset_scaled_mb": 100.0010183, "dataset_tokens": 25136640, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.39790349999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.28298045800448e+17, "train_compute_hours": 12.129997057496903, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nor_latn_100mb": {"language_name": "Norwegian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "nor", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["nno", "nob"], "language_byte_premium": 1.125315993, "dataset_raw_mb": 112.5324502639539, "dataset_scaled_mb": 100.0007562, "dataset_tokens": 25547264, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 112.53159930000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.30393607897088e+17, "train_compute_hours": 12.328122928451958, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.92843%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.07157%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "tgl_latn_100mb": {"language_name": "Tagalog", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "tgl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.117593432, "dataset_raw_mb": 111.76080870026739, "dataset_scaled_mb": 100.0013113, "dataset_tokens": 24543232, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.7593432, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2525294845952e+17, "train_compute_hours": 11.84209694526371, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.29659%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.79155%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.61906%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29280%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TICO", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "zho_hant_100mb": {"language_name": "Chinese", "language_script": "Han Traditional", "dataset_category": "100mb", "language_iso6393": "zho", "language_iso15924": "hant", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.989382544, "dataset_raw_mb": 98.94046200927043, "dataset_scaled_mb": 100.0022313, "dataset_tokens": 24095232, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 98.9382544, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.23016811249664e+17, "train_compute_hours": 11.63068033633187, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 36.42318%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.54996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 29.93702%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.08984%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "CCNet", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "lus_latn_100mb": {"language_name": "Lushai", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.168842547, "dataset_raw_mb": 116.8858789236033, "dataset_scaled_mb": 100.0013896, "dataset_tokens": 29448192, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.88425469999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.50309809160192e+17, "train_compute_hours": 14.211109229690882, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 54.32199%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 39.88323%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.79478%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download"]}, "lug_latn_100mb": {"language_name": "Ganda", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.217417428, "dataset_raw_mb": 121.74230305550039, "dataset_scaled_mb": 100.0004602, "dataset_tokens": 28298240, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.74174280000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.44424467431424e+17, "train_compute_hours": 13.654676920789178, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 40.49929%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.20499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Makerere MT Corpus](https://zenodo.org/record/5089560#.Y00i3uxBw-S), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.27645%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.29489%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.85655%: [eBible](https://ebible.org/find/)\n* 0.49599%: [Makerere Radio Corpus](https://zenodo.org/record/5855017)\n* 0.37185%: [Makerere Parallel Corpus](https://zenodo.org/record/4764039)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CC100", "Wortschatz Leipzig Data", "Makerere MT Corpus", "TICO", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible", "Makerere Radio Corpus", "Makerere Parallel Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://zenodo.org/record/5089560#.Y00i3uxBw-S", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://zenodo.org/record/5855017", "https://zenodo.org/record/4764039"]}, "fry_latn_100mb": {"language_name": "Western Frisian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "fry", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23134294, "dataset_raw_mb": 123.13441799623406, "dataset_scaled_mb": 100.0001007, "dataset_tokens": 28158976, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 123.134294, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.43755559829504e+17, "train_compute_hours": 13.591434747516743, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.10455%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.78619%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.66588%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.44227%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00110%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "deu_latn_100mb": {"language_name": "German", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "deu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.053648018, "dataset_raw_mb": 105.36660912244528, "dataset_scaled_mb": 100.0017153, "dataset_tokens": 21072896, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.36480180000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07550935875584e+17, "train_compute_hours": 10.168452119146124, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "sun_latn_100mb": {"language_name": "Sundanese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "sun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.096980852, "dataset_raw_mb": 109.69998297687395, "dataset_scaled_mb": 100.00173, "dataset_tokens": 24632320, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.6980852, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.25741041975296e+17, "train_compute_hours": 11.888243968573441, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.54837%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 32.02343%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.03343%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.55888%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81131%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.02459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "MC4", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "slk_latn_100mb": {"language_name": "Slovak", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "slk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.041506091, "dataset_raw_mb": 104.15107694453609, "dataset_scaled_mb": 100.0004492, "dataset_tokens": 21123584, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 104.1506091, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07817976332288e+17, "train_compute_hours": 10.19369958050723, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "chv_cyrl_100mb": {"language_name": "Chuvash", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "chv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.796969484, "dataset_raw_mb": 179.6978880353432, "dataset_scaled_mb": 100.0005229, "dataset_tokens": 32877056, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 179.6969484, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.67833620578304e+17, "train_compute_hours": 15.867905945585107, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.68880%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.72870%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.16329%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.06333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.35083%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00505%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "TIL", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tat_cyrl_100mb": {"language_name": "Tatar", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "tat", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.854541191, "dataset_raw_mb": 185.45586403780663, "dataset_scaled_mb": 100.0009409, "dataset_tokens": 23312896, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 185.4541191, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18966523461632e+17, "train_compute_hours": 11.247744036372481, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 42.95952%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.38275%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.13842%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.01257%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.69512%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.81162%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "tgk_cyrl_100mb": {"language_name": "Tajik", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "tgk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.746052186, "dataset_raw_mb": 174.60725798895325, "dataset_scaled_mb": 100.001168, "dataset_tokens": 21698048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 174.6052186, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10758034276352e+17, "train_compute_hours": 10.471668695218735, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.68005%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.31925%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.47599%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.49619%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.02853%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nld_latn_100mb": {"language_name": "Dutch", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "nld", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.051605721, "dataset_raw_mb": 105.16303811541576, "dataset_scaled_mb": 100.002345, "dataset_tokens": 21698560, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.16057210000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10760647196672e+17, "train_compute_hours": 10.471915734958081, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "jpn_jpan_100mb": {"language_name": "Japanese", "language_script": "Japanese", "dataset_category": "100mb", "language_iso6393": "jpn", "language_iso15924": "jpan", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321973899, "dataset_raw_mb": 132.19901698547488, "dataset_scaled_mb": 100.0012308, "dataset_tokens": 21925376, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 132.19738990000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.11909809553408e+17, "train_compute_hours": 10.580563812322211, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fas_arab_100mb": {"language_name": "Persian", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "fas", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pes", "prs"], "language_byte_premium": 1.590775443, "dataset_raw_mb": 159.0788632119198, "dataset_scaled_mb": 100.0008291, "dataset_tokens": 24438272, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 159.0775443, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2475910651904e+17, "train_compute_hours": 11.795406434527418, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "vie_latn_100mb": {"language_name": "Vietnamese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "vie", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.349331266, "dataset_raw_mb": 134.934296875007, "dataset_scaled_mb": 100.0008673, "dataset_tokens": 26223104, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.9331266, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3386774675456e+17, "train_compute_hours": 12.656586965885674, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ckb_arab_100mb": {"language_name": "Central Kurdish", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "ckb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.651286029, "dataset_raw_mb": 165.13218602555432, "dataset_scaled_mb": 100.0021699, "dataset_tokens": 22688768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 165.1286029, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.15773796122624e+17, "train_compute_hours": 10.945886178866271, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.06661%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.31684%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.60071%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 15.36201%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.39332%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.26051%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "TICO", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kan_knda_100mb": {"language_name": "Kannada", "language_script": "Kannada", "dataset_category": "100mb", "language_iso6393": "kan", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.641985282, "dataset_raw_mb": 264.2041440039154, "dataset_scaled_mb": 100.0021256, "dataset_tokens": 21264896, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 264.1985282, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.08514842181632e+17, "train_compute_hours": 10.259585078990662, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 49.18479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 40.78353%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.59325%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.16622%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.00209%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.27013%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "TeDDi", "W2C", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "cos_latn_100mb": {"language_name": "Corsican", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "cos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176702158, "dataset_raw_mb": 117.67128695197444, "dataset_scaled_mb": 100.0009103, "dataset_tokens": 30479360, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.67021580000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.55572230684672e+17, "train_compute_hours": 14.708647264732626, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 60.95981%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.95148%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.08866%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00005%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "kat_geor_100mb": {"language_name": "Georgian", "language_script": "Georgian", "dataset_category": "100mb", "language_iso6393": "kat", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 4.338956454, "dataset_raw_mb": 433.90076102965924, "dataset_scaled_mb": 100.001179, "dataset_tokens": 35512320, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 433.8956454, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8127396012032e+17, "train_compute_hours": 17.138628956830257, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "allenai/nllb"], "dataset_readme_str": "* 60.04062%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.58420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.78020%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 7.59499%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb"]}, "ydd_hebr_100mb": {"language_name": "Eastern Yiddish", "language_script": "Hebrew", "dataset_category": "100mb", "language_iso6393": "ydd", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": ["yid"], "language_code_individuals": [], "language_byte_premium": 1.806848485, "dataset_raw_mb": 180.68801445991542, "dataset_scaled_mb": 100.0017522, "dataset_tokens": 23550976, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 180.6848485, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20222554259456e+17, "train_compute_hours": 11.366496039075841, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "afr_latn_100mb": {"language_name": "Afrikaans", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "afr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.03722598, "dataset_raw_mb": 103.72391299509745, "dataset_scaled_mb": 100.0012678, "dataset_tokens": 23968768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.722598, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2231341309952e+17, "train_compute_hours": 11.564177238500074, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.06252%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.34080%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 13.07209%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.39061%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.13398%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "Earthlings", "Mburisano_Covid", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://repo.sadilar.org/handle/20.500.12185/536", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "prs_arab_100mb": {"language_name": "Dari", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "prs", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.66404538, "dataset_raw_mb": 166.40628191955824, "dataset_scaled_mb": 100.001048, "dataset_tokens": 23078400, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 166.404538, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1781919014912e+17, "train_compute_hours": 11.139268886825892, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.28229%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.71771%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tico-19.github.io/"]}, "som_latn_100mb": {"language_name": "Somali", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "som", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.42230286, "dataset_raw_mb": 142.23166591823477, "dataset_scaled_mb": 100.0009702, "dataset_tokens": 30258688, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 142.230286, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.54405561761792e+17, "train_compute_hours": 14.598344021114881, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 38.68291%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 30.77395%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.30160%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TICO](https://tico-19.github.io/)\n* 7.64665%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.41754%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.17735%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "Wortschatz Leipzig Data", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "sin_sinh_100mb": {"language_name": "Sinhala", "language_script": "Sinhala", "dataset_category": "100mb", "language_iso6393": "sin", "language_iso15924": "sinh", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.446318518, "dataset_raw_mb": 244.6323359264347, "dataset_scaled_mb": 100.0001979, "dataset_tokens": 23275008, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 244.6318518, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18822551552e+17, "train_compute_hours": 11.234132146734547, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.49015%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.08102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.71980%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.70904%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "amh_ethi_100mb": {"language_name": "Amharic", "language_script": "Ge'ez", "dataset_category": "100mb", "language_iso6393": "amh", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.720917143, "dataset_raw_mb": 172.09531910513945, "dataset_scaled_mb": 100.0020947, "dataset_tokens": 21179904, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 172.0917143, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.08147204292608e+17, "train_compute_hours": 10.224826587664758, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 35.99036%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [OSCAR](https://oscar-project.org/), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 33.89884%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.09942%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.48807%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.48971%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03361%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "OSCAR", "Parallel Corpora for Ethiopian Languages", "TICO", "W2C", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://oscar-project.org/", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "smo_latn_100mb": {"language_name": "Samoan", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "smo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.178001518, "dataset_raw_mb": 117.80147304650261, "dataset_scaled_mb": 100.0011216, "dataset_tokens": 32361984, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.80015180000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.65171577356288e+17, "train_compute_hours": 15.616221859139957, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 43.50048%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.24768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.13951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.11208%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00025%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "fao_latn_100mb": {"language_name": "Faroese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "fao", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155644359, "dataset_raw_mb": 115.56487897404725, "dataset_scaled_mb": 100.0003834, "dataset_tokens": 24129024, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.5644359, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.23197625335808e+17, "train_compute_hours": 11.647775486294575, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 47.79193%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 42.53753%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.12327%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.54727%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "Ndc without informant codes", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "pol_latn_100mb": {"language_name": "Polish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "pol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.077373477, "dataset_raw_mb": 107.73786613211713, "dataset_scaled_mb": 100.0004812, "dataset_tokens": 21622272, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 107.73734770000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10404767449088e+17, "train_compute_hours": 10.43826892245923, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "gla_latn_100mb": {"language_name": "Scottish Gaelic", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "gla", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9936126744, "dataset_raw_mb": 99.36307104572657, "dataset_scaled_mb": 100.0018152, "dataset_tokens": 22136320, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.36126744, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.12968826159104e+17, "train_compute_hours": 10.680689018678924, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.49478%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.15694%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.70049%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 0.49333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.15447%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "haw_latn_100mb": {"language_name": "Hawaiian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "haw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.114669379, "dataset_raw_mb": 111.46854001429844, "dataset_scaled_mb": 100.0014373, "dataset_tokens": 33245184, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.46693789999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.69670503563264e+17, "train_compute_hours": 16.04157488234496, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.98608%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.94220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.78601%: [Ulukau](https://ulukau.org/index.php?l=en)\n* 1.02324%: [eBible](https://ebible.org/find/)\n* 0.26199%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00047%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "Ulukau", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ulukau.org/index.php?l=en", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "grc_grek_100mb": {"language_name": "Ancient Greek", "language_script": "Greek", "dataset_category": "100mb", "language_iso6393": "grc", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.76699504, "dataset_raw_mb": 176.6999849760499, "dataset_scaled_mb": 100.0002722, "dataset_tokens": 23190528, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.69950400000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18383058354176e+17, "train_compute_hours": 11.19258006257664, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 97.28398%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.44105%: [eBible](https://ebible.org/find/)\n* 0.26854%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00642%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "khm_khmr_100mb": {"language_name": "Central Khmer", "language_script": "Khmer", "dataset_category": "100mb", "language_iso6393": "khm", "language_iso15924": "khmr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.903484241, "dataset_raw_mb": 390.35026693491017, "dataset_scaled_mb": 100.0004721, "dataset_tokens": 31746048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 390.3484241, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.62061679591424e+17, "train_compute_hours": 15.322195161370997, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 55.59600%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.62172%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.83408%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.36741%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/)\n* 1.58079%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "ukr_cyrl_100mb": {"language_name": "Ukrainian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "ukr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.751370357, "dataset_raw_mb": 175.1405669880508, "dataset_scaled_mb": 100.0020163, "dataset_tokens": 21571072, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 175.1370357, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1012675272704e+17, "train_compute_hours": 10.411983894192874, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tha_thai_100mb": {"language_name": "Thai", "language_script": "Thai", "dataset_category": "100mb", "language_iso6393": "tha", "language_iso15924": "thai", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.741647235, "dataset_raw_mb": 274.1679309531002, "dataset_scaled_mb": 100.0011699, "dataset_tokens": 20609536, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 274.1647235, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05170304172032e+17, "train_compute_hours": 9.94337421262848, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 99.89966%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09995%: [eBible](https://ebible.org/find/)\n* 0.00039%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["OSCAR 2021/09", "eBible", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/", "https://tatoeba.org/en/"]}, "gle_latn_100mb": {"language_name": "Irish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "gle", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.975875656, "dataset_raw_mb": 197.588417992758, "dataset_scaled_mb": 100.0004314, "dataset_tokens": 41451520, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 197.5875656, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.115420291072e+17, "train_compute_hours": 20.000337297408002, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.77543%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.32257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.85422%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.21381%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.83397%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "mlt_latn_100mb": {"language_name": "Maltese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "mlt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.08846667, "dataset_raw_mb": 108.8476641443164, "dataset_scaled_mb": 100.0009161, "dataset_tokens": 28329472, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 108.84666700000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.44574710349824e+17, "train_compute_hours": 13.668881705801542, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 94.92627%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MaCoCu](https://macocu.eu/), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23828%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.42133%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.41412%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MaCoCu", "MC4", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://macocu.eu/", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "bak_cyrl_100mb": {"language_name": "Bashkir", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "bak", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.27194034, "dataset_raw_mb": 227.19721903316267, "dataset_scaled_mb": 100.0014019, "dataset_tokens": 29721088, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 227.194034, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.51677150363648e+17, "train_compute_hours": 14.340385125290357, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 29.93711%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 29.35888%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.91103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.93919%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.89615%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Languages of Russia", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "que_latn_100mb": {"language_name": "Quechua", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "que", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["qub", "qud", "quf", "qug", "quh", "quk", "qul", "qup", "qur", "qus", "quw", "qux", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvj", "qvl", "qvm", "qvn", "qvo", "qvp", "qvs", "qvw", "qvz", "qwa", "qwc", "qwh", "qws", "qxa", "qxc", "qxh", "qxl", "qxn", "qxo", "qxp", "qxr", "qxt", "qxu", "qxw"], "language_byte_premium": 1.21478134, "dataset_raw_mb": 121.47913607312736, "dataset_scaled_mb": 100.0008249, "dataset_tokens": 29125632, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.47813400000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.48688753393664e+17, "train_compute_hours": 14.057845775400962, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "Llamacha/monolingual-quechua-iic", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.54127%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.31458%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 7.98999%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Quechua-IIC](https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.28328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.76909%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09735%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00445%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM", "CC100", "Earthlings", "OSCAR", "Quechua-IIC", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bod_tibt_100mb": {"language_name": "Tibetan", "language_script": "Tibetan", "dataset_category": "100mb", "language_iso6393": "bod", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.616914857, "dataset_raw_mb": 261.6927949425029, "dataset_scaled_mb": 100.0005003, "dataset_tokens": 17793536, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 261.6914857, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.0806558588928e+16, "train_compute_hours": 8.585347357498648, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 35.45938%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 27.32632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.94564%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.69915%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.63626%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.93326%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "MoT", "OSCAR", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "cym_latn_100mb": {"language_name": "Welsh", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "cym", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.026556848, "dataset_raw_mb": 102.65776306433878, "dataset_scaled_mb": 100.0020245, "dataset_tokens": 23621120, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.6556848, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20605608378368e+17, "train_compute_hours": 11.402712064863884, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.76968%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.88536%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.87431%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.28932%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.18133%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "eus_latn_100mb": {"language_name": "Basque", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "eus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059583723, "dataset_raw_mb": 105.96011499734922, "dataset_scaled_mb": 100.0016447, "dataset_tokens": 20982784, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.95837230000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07132868624384e+17, "train_compute_hours": 10.128925760850851, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.69288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.18968%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.11744%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ron_latn_100mb": {"language_name": "Romanian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ron", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115120928, "dataset_raw_mb": 111.5136650089964, "dataset_scaled_mb": 100.0014099, "dataset_tokens": 23056896, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.5120928, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.17684363460608e+17, "train_compute_hours": 11.126521636275667, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "urd_arab_100mb": {"language_name": "Urdu", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "urd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.707950617, "dataset_raw_mb": 170.79598997116037, "dataset_scaled_mb": 100.0005435, "dataset_tokens": 24801792, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 170.79506170000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.26549218230272e+17, "train_compute_hours": 11.96465335995299, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 37.89138%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.70994%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 26.24866%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 0.15002%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "Anuvaad", "CCNet", "Earthlings", "OSCAR", "TICO", "W2C", "Workshop on NER for South and South East Asian Languages", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://ebible.org/find/"]}, "pap_latn_100mb": {"language_name": "Papiamento", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "pap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.001675271, "dataset_raw_mb": 100.16936497378724, "dataset_scaled_mb": 100.0018348, "dataset_tokens": 23498240, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 100.16752710000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.19936700776448e+17, "train_compute_hours": 11.339469891591449, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 68.24074%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.46139%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.28720%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.01066%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "war_latn_100mb": {"language_name": "Waray", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "war", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.093149717, "dataset_raw_mb": 109.31611491597404, "dataset_scaled_mb": 100.0010458, "dataset_tokens": 27957248, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.3149717, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.42700985188352e+17, "train_compute_hours": 13.491729508716917, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.84724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 25.81890%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.17011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.16375%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bel_cyrl_100mb": {"language_name": "Belarusian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "bel", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.007901403, "dataset_raw_mb": 200.79394507236853, "dataset_scaled_mb": 100.0018949, "dataset_tokens": 25410048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 200.7901403, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.29669045092352e+17, "train_compute_hours": 12.259618808731464, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.85869%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.04732%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 14.98527%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.10871%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ori_orya_100mb": {"language_name": "Odia", "language_script": "Oriya", "dataset_category": "100mb", "language_iso6393": "ori", "language_iso15924": "orya", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ory", "spv"], "language_byte_premium": 2.595114712, "dataset_raw_mb": 259.5150280642243, "dataset_scaled_mb": 100.0013706, "dataset_tokens": 21361664, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 259.5114712, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.09001106653184e+17, "train_compute_hours": 10.305559174482852, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.28209%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 26.83885%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.83892%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.35162%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.43853%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.24999%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Workshop on NER for South and South East Asian Languages", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "fin_latn_100mb": {"language_name": "Finnish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "fin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.05886439, "dataset_raw_mb": 105.88651100277853, "dataset_scaled_mb": 100.000068, "dataset_tokens": 18592768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.88643900000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.4870433562624e+16, "train_compute_hours": 8.969568264102634, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "orm_latn_100mb": {"language_name": "Oromo", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "orm", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gax", "gaz", "hae", "orc"], "language_byte_premium": 1.264405846, "dataset_raw_mb": 126.44245503556799, "dataset_scaled_mb": 100.0014793, "dataset_tokens": 28832256, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 126.44058460000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.47118126989312e+17, "train_compute_hours": 13.909350188080408, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "statmt/cc100", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 32.67081%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 27.67571%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.09955%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.69511%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.12785%: [eBible](https://ebible.org/find/)\n* 0.73097%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CC100", "CCNet", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "msa_latn_100mb": {"language_name": "Malay", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "msa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bjn", "btj", "bve", "bvu", "coa", "dup", "hji", "ind", "jak", "jax", "kvb", "kvr", "kxd", "lce", "lcf", "liw", "max", "meo", "mfa", "mfb", "min", "mqg", "msi", "mui", "orn", "ors", "pel", "pse", "tmw", "urk", "vkk", "vkt", "xmm", "zlm", "zmi", "zsm"], "language_byte_premium": 1.285705434, "dataset_raw_mb": 128.5721439746948, "dataset_scaled_mb": 100.0012449, "dataset_tokens": 23629824, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.5705434, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20608221298688e+17, "train_compute_hours": 11.40295910460323, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 82.84912%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.03127%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.11781%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.00181%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "TICO", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "asm_beng_100mb": {"language_name": "Assamese", "language_script": "Bengali", "dataset_category": "100mb", "language_iso6393": "asm", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.526329303, "dataset_raw_mb": 252.63658690903316, "dataset_scaled_mb": 100.0014474, "dataset_tokens": 22117376, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 252.63293029999997, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.128650932224e+17, "train_compute_hours": 10.67088154102691, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.77996%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 25.83555%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.99529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.32777%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.47875%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.58268%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CC100", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "ces_latn_100mb": {"language_name": "Czech", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ces", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035849214, "dataset_raw_mb": 103.58716908920945, "dataset_scaled_mb": 100.0021699, "dataset_tokens": 20621312, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.5849214, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05288146878464e+17, "train_compute_hours": 9.95451570487296, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mlg_latn_100mb": {"language_name": "Malagasy", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "mlg", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bhr", "bmm", "bzc", "msh", "plt", "skg", "tdx", "tkg", "txy", "xmv", "xmw"], "language_byte_premium": 1.266754162, "dataset_raw_mb": 126.67709604269423, "dataset_scaled_mb": 100.0013261, "dataset_tokens": 29208576, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 126.6754162, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.49120407830528e+17, "train_compute_hours": 14.09865674034083, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.72851%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.99233%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.22403%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.55221%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.26037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.24255%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TeDDi", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "aze_latn_100mb": {"language_name": "Azerbaijani", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "aze", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.29878361, "dataset_raw_mb": 129.87919403980746, "dataset_scaled_mb": 100.0006414, "dataset_tokens": 23309312, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 129.878361, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.19005978558464e+17, "train_compute_hours": 11.251474336436598, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.37652%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 18.62348%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lim_latn_100mb": {"language_name": "Limburgan", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lim", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.997886276, "dataset_raw_mb": 99.78893385129811, "dataset_scaled_mb": 100.0003069, "dataset_tokens": 27896320, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.7886276, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.42415131705344e+17, "train_compute_hours": 13.464703361232525, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 45.81871%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.97133%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.20996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tam_taml_100mb": {"language_name": "Tamil", "language_script": "Tamil", "dataset_category": "100mb", "language_iso6393": "tam", "language_iso15924": "taml", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.729099655, "dataset_raw_mb": 272.91165208358683, "dataset_scaled_mb": 100.000618, "dataset_tokens": 20064256, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 272.9099655, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.02445289570304e+17, "train_compute_hours": 9.685736468465107, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 36.08815%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.39717%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 19.80765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.41716%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.28987%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "OSCAR", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "sna_latn_100mb": {"language_name": "Shona", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "sna", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119177764, "dataset_raw_mb": 111.91820303056365, "dataset_scaled_mb": 100.0003812, "dataset_tokens": 24935936, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.91777640000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.27248958291968e+17, "train_compute_hours": 12.030810602149703, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.75185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 27.93918%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.33727%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.07291%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.53601%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.36278%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kur_arab_100mb": {"language_name": "Kurdish", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "kur", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.569891339, "dataset_raw_mb": 156.98976499631826, "dataset_scaled_mb": 100.000402, "dataset_tokens": 21803008, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 156.9891339, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.11327128322048e+17, "train_compute_hours": 10.525473950448175, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 43.19057%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.23880%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.93282%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05661%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.32587%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.25533%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TICO", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nno_latn_100mb": {"language_name": "Norwegian Nynorsk", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "nno", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 1.033289113, "dataset_raw_mb": 103.33035604483779, "dataset_scaled_mb": 100.0013982, "dataset_tokens": 23257088, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.32891129999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.187310993408e+17, "train_compute_hours": 11.225485755857456, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.90768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.53541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.55180%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00511%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "als_latn_100mb": {"language_name": "Tosk Albanian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "als", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["alb", "sqi"], "language_code_individuals": [], "language_byte_premium": 1.167247178, "dataset_raw_mb": 116.72473309093803, "dataset_scaled_mb": 100.0000131, "dataset_tokens": 26042880, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.7247178, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3294799880192e+17, "train_compute_hours": 12.569628977636073, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 85.38346%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.76858%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.84796%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "mar_deva_100mb": {"language_name": "Marathi", "language_script": "Devanagari", "dataset_category": "100mb", "language_iso6393": "mar", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.479356527, "dataset_raw_mb": 247.93851610885307, "dataset_scaled_mb": 100.0011549, "dataset_tokens": 20673536, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 247.93565270000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05489341743104e+17, "train_compute_hours": 9.973537764802561, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.53905%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 40.24253%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13450%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.08391%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Hindialect", "Indiccorp", "OSCAR", "TICO", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "nob_latn_100mb": {"language_name": "Norwegian Bokm\u00e5l", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "nob", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 0.9976190383, "dataset_raw_mb": 99.7627089085639, "dataset_scaled_mb": 100.000807, "dataset_tokens": 20589568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.76190383, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05126145818624e+17, "train_compute_hours": 9.939199241033542, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mal_mlym_100mb": {"language_name": "Malayalam", "language_script": "Malayalam", "dataset_category": "100mb", "language_iso6393": "mal", "language_iso15924": "mlym", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.884868527, "dataset_raw_mb": 288.489678717209, "dataset_scaled_mb": 100.0009796, "dataset_tokens": 24480768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 288.4868527, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.24942533525504e+17, "train_compute_hours": 11.81274862422947, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 79.43378%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 20.26204%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30417%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "sah_cyrl_100mb": {"language_name": "Yakut", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "sah", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.881497141, "dataset_raw_mb": 188.151023057561, "dataset_scaled_mb": 100.0006957, "dataset_tokens": 22938624, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 188.1497141, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.17063533592576e+17, "train_compute_hours": 11.067824994207186, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 39.11470%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.57614%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 19.21725%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 5.94791%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.13671%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00728%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "scn_latn_100mb": {"language_name": "Sicilian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "scn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.036272745, "dataset_raw_mb": 103.62758424192349, "dataset_scaled_mb": 100.0002989, "dataset_tokens": 28128768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.6272745, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.43535813230592e+17, "train_compute_hours": 13.57065870543779, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.07639%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.21878%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.70271%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00212%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "snd_arab_100mb": {"language_name": "Sindhi", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "snd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.587969761, "dataset_raw_mb": 158.79830999459924, "dataset_scaled_mb": 100.00084, "dataset_tokens": 24961536, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 158.7969761, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2742951108608e+17, "train_compute_hours": 12.047881048138473, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.13852%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.07857%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.09349%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 4.06666%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.62277%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "ast_latn_100mb": {"language_name": "Asturian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ast", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.748880208, "dataset_raw_mb": 174.88906715502844, "dataset_scaled_mb": 100.0005983, "dataset_tokens": 41352704, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 174.88802080000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.11029635432448e+17, "train_compute_hours": 19.95189280452236, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.70018%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.64321%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.37355%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.28306%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "quy_latn_100mb": {"language_name": "Ayacucho Quechua", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "quy", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.163741957, "dataset_raw_mb": 116.37490569896798, "dataset_scaled_mb": 100.0006101, "dataset_tokens": 28205056, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.3741957, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4398236131328e+17, "train_compute_hours": 13.612877796891928, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 86.82052%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.17948%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/"]}, "cat_latn_100mb": {"language_name": "Catalan", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "cat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.092622079, "dataset_raw_mb": 109.26335307720102, "dataset_scaled_mb": 100.0010481, "dataset_tokens": 23899136, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.2622079, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.22016062767104e+17, "train_compute_hours": 11.536064116162562, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tsn_latn_100mb": {"language_name": "Tswana", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "tsn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173940296, "dataset_raw_mb": 117.3957199566322, "dataset_scaled_mb": 100.0014399, "dataset_tokens": 30919680, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.39402960000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.57802619469824e+17, "train_compute_hours": 14.919520386237906, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 54.15070%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.72794%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.23011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.14046%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.74997%: [eBible](https://ebible.org/find/)\n* 0.00083%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "sqi_latn_100mb": {"language_name": "Albanian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "sqi", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aae", "aat", "aln", "als"], "language_byte_premium": 1.335619892, "dataset_raw_mb": 133.5646986385129, "dataset_scaled_mb": 100.0020286, "dataset_tokens": 27465728, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 133.5619892, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.40192581681152e+17, "train_compute_hours": 13.254571358945281, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 62.65815%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 37.34185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "ibo_latn_100mb": {"language_name": "Igbo", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ibo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345020968, "dataset_raw_mb": 134.50302096390712, "dataset_scaled_mb": 100.0006871, "dataset_tokens": 30829568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.5020968, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.57384552218624e+17, "train_compute_hours": 14.879994027942635, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 31.78330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 31.30256%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.52382%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93791%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 4.92177%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.53064%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "oci_latn_100mb": {"language_name": "Occitan", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "oci", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.014733462, "dataset_raw_mb": 101.47461096378703, "dataset_scaled_mb": 100.0012464, "dataset_tokens": 26584576, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.4733462, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3571246850048e+17, "train_compute_hours": 12.830997021863565, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.78812%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.07771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.38373%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.02047%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.72997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pan_guru_100mb": {"language_name": "Panjabi", "language_script": "Gurmukhi", "dataset_category": "100mb", "language_iso6393": "pan", "language_iso15924": "guru", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.220967698, "dataset_raw_mb": 222.10041329750854, "dataset_scaled_mb": 100.0016405, "dataset_tokens": 21575680, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 222.0967698, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1009278476288e+17, "train_compute_hours": 10.408772377581382, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.26136%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.38636%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.73324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.48906%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12998%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "epo_latn_100mb": {"language_name": "Esperanto", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "epo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9954937917, "dataset_raw_mb": 99.54988796687694, "dataset_scaled_mb": 100.0005111, "dataset_tokens": 23139328, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.54937917, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18113404977152e+17, "train_compute_hours": 11.16708556147619, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.12373%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.22117%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.49198%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 14.01395%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14917%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "mon_cyrl_100mb": {"language_name": "Mongolian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "mon", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.783772557, "dataset_raw_mb": 178.38129095027847, "dataset_scaled_mb": 100.0022622, "dataset_tokens": 20547584, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 178.3772557, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.04870079627264e+17, "train_compute_hours": 9.914989346577688, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 63.59654%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.77330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 11.63016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ceb_latn_100mb": {"language_name": "Cebuano", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ceb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113380333, "dataset_raw_mb": 111.33902097969342, "dataset_scaled_mb": 100.0008871, "dataset_tokens": 25974272, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.3380333, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.32581144788992e+17, "train_compute_hours": 12.534944598231972, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 36.46583%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 33.35484%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.15251%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.56324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.46359%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "swe_latn_100mb": {"language_name": "Swedish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "swe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.020981292, "dataset_raw_mb": 102.09958787597189, "dataset_scaled_mb": 100.0014287, "dataset_tokens": 20646400, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.09812920000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05374373249024e+17, "train_compute_hours": 9.96266801627136, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "azb_arab_100mb": {"language_name": "South Azerbaijani", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "azb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.490422093, "dataset_raw_mb": 149.0442717460923, "dataset_scaled_mb": 100.0013838, "dataset_tokens": 25029120, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 149.0422093, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.27782777913344e+17, "train_compute_hours": 12.081280820897979, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 46.02648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 42.37928%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.52167%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "OSCAR", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "fil_latn_100mb": {"language_name": "Filipino", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "fil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.328161865, "dataset_raw_mb": 132.81691805155523, "dataset_scaled_mb": 100.0005508, "dataset_tokens": 27497472, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 132.8161865, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.40354582740992e+17, "train_compute_hours": 13.2698878227847, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "bre_latn_100mb": {"language_name": "Breton", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "bre", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01282485, "dataset_raw_mb": 101.28258699146241, "dataset_scaled_mb": 100.0001007, "dataset_tokens": 26627072, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.28248500000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.35895895506944e+17, "train_compute_hours": 12.848339211565616, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.39389%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.93949%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.16716%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.31819%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.13530%: [eBible](https://ebible.org/find/)\n* 0.04596%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "azj_latn_100mb": {"language_name": "North Azerbaijani", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "azj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.076115956, "dataset_raw_mb": 107.61438381644199, "dataset_scaled_mb": 100.002591, "dataset_tokens": 18049024, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 107.6115956, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.2160835190784e+16, "train_compute_hours": 8.713388054401397, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "ltz_latn_100mb": {"language_name": "Luxembourgish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ltz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.225348993, "dataset_raw_mb": 122.53603507598162, "dataset_scaled_mb": 100.0009269, "dataset_tokens": 27672064, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.5348993, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.41212404482048e+17, "train_compute_hours": 13.350990969211812, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.34921%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.85023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.94131%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.85925%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kir_cyrl_100mb": {"language_name": "Kirghiz", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "kir", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.963770708, "dataset_raw_mb": 196.38141505356023, "dataset_scaled_mb": 100.0022122, "dataset_tokens": 22320128, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 196.37707079999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.13932732465152e+17, "train_compute_hours": 10.771821978523462, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.41839%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.62164%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.53594%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt)\n* 8.32078%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.10325%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "div_thaa_100mb": {"language_name": "Dhivehi", "language_script": "Thaana", "dataset_category": "100mb", "language_iso6393": "div", "language_iso15924": "thaa", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.997934845, "dataset_raw_mb": 199.79830092153085, "dataset_scaled_mb": 100.0024107, "dataset_tokens": 18036224, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 199.7934845, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.2029927882752e+16, "train_compute_hours": 8.70101136346019, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.19489%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.14021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.85897%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.61771%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18821%: [eBible](https://ebible.org/find/)\n* 0.00002%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "yor_latn_100mb": {"language_name": "Yoruba", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "yor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.374948177, "dataset_raw_mb": 137.49675596444513, "dataset_scaled_mb": 100.0014097, "dataset_tokens": 31010816, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 137.4948177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.58317887356928e+17, "train_compute_hours": 14.96823662283683, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.33023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.88587%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.42418%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Menyo20K](https://github.com/uds-lsv/menyo-20k_MT), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 2.89553%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.95515%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.49782%: [eBible](https://ebible.org/find/)\n* 0.01122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Menyo20K", "OSCAR", "TeDDi", "W2C", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/uds-lsv/menyo-20k_MT", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zul_latn_100mb": {"language_name": "Zulu", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "zul", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.163840155, "dataset_raw_mb": 116.38643698582649, "dataset_scaled_mb": 100.0020806, "dataset_tokens": 26162688, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.38401549999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.33502199201792e+17, "train_compute_hours": 12.622026106351244, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 46.89015%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [isiZulu](https://zenodo.org/record/5035171#.YaippvHMJDZ), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 34.71066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.29050%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.78489%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.32380%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "isiZulu", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "TeDDi", "TICO", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://zenodo.org/record/5035171#.YaippvHMJDZ", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "eng_latn_100mb": {"language_name": "English", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "eng", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.0, "dataset_raw_mb": 100.001044, "dataset_scaled_mb": 100.001044, "dataset_tokens": 21390848, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 100.0, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.092070047744e+17, "train_compute_hours": 10.325025905943274, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaz_cyrl_100mb": {"language_name": "Kazakh", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "kaz", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.764638392, "dataset_raw_mb": 176.4678627519976, "dataset_scaled_mb": 100.0022801, "dataset_tokens": 19991552, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.4638392, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.02032448159744e+17, "train_compute_hours": 9.646704189648524, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 68.35707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.84524%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.79769%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "W2C", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb"]}, "kin_latn_100mb": {"language_name": "Kinyarwanda", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "kin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.133982717, "dataset_raw_mb": 113.39981697824847, "dataset_scaled_mb": 100.0013627, "dataset_tokens": 23874560, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 113.39827170000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.21848835866624e+17, "train_compute_hours": 11.520253572844451, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.08100%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.87658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [KinyaSMT](https://github.com/pniyongabo/kinyarwandaSMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.34254%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.79880%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "BLOOM", "KinyaSMT", "Wortschatz Leipzig Data", "MoT", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/pniyongabo/kinyarwandaSMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "heb_hebr_100mb": {"language_name": "Hebrew", "language_script": "Hebrew", "dataset_category": "100mb", "language_iso6393": "heb", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.355477183, "dataset_raw_mb": 135.54965093936755, "dataset_scaled_mb": 100.0014258, "dataset_tokens": 19294720, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 135.5477183, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.8518070329344e+16, "train_compute_hours": 9.314435740228888, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ind_latn_100mb": {"language_name": "Indonesian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ind", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.178746238, "dataset_raw_mb": 117.87676994327552, "dataset_scaled_mb": 100.0018207, "dataset_tokens": 21039104, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.8746238, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07378483134464e+17, "train_compute_hours": 10.152147496349324, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lav_latn_100mb": {"language_name": "Latvian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lav", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ltg", "lvs"], "language_byte_premium": 1.288843709, "dataset_raw_mb": 128.88438585058702, "dataset_scaled_mb": 100.0000116, "dataset_tokens": 24353792, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.8843709, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.24319613321216e+17, "train_compute_hours": 11.753854350369513, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "pbt_arab_100mb": {"language_name": "Southern Pashto", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "pbt", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["pus"], "language_code_individuals": [], "language_byte_premium": 1.736020956, "dataset_raw_mb": 173.60429461774496, "dataset_scaled_mb": 100.0012667, "dataset_tokens": 26925568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 173.60209559999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.37444312088576e+17, "train_compute_hours": 12.994734961101733, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "uzb_cyrl_100mb": {"language_name": "Uzbek", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "uzb", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.980686782, "dataset_raw_mb": 198.0733120167265, "dataset_scaled_mb": 100.0023395, "dataset_tokens": 21101568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 198.0686782, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0768105930752e+17, "train_compute_hours": 10.180754698165527, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "guj_gujr_100mb": {"language_name": "Gujarati", "language_script": "Gujarati", "dataset_category": "100mb", "language_iso6393": "guj", "language_iso15924": "gujr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.162862919, "dataset_raw_mb": 216.28875821258657, "dataset_scaled_mb": 100.0011403, "dataset_tokens": 19382784, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 216.2862919, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.8917063262208e+16, "train_compute_hours": 9.35215870842694, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 86.12642%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 13.58996%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.28351%: [eBible](https://ebible.org/find/)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "XLSum", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "arb_arab_100mb": {"language_name": "Standard Arabic", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "arb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.465017978, "dataset_raw_mb": 146.50200700456728, "dataset_scaled_mb": 100.0001428, "dataset_tokens": 19608064, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 146.50179780000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.00043754504192e+17, "train_compute_hours": 9.4586822440327, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mkd_cyrl_100mb": {"language_name": "Macedonian", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "mkd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.834986754, "dataset_raw_mb": 183.50164844553885, "dataset_scaled_mb": 100.0016202, "dataset_tokens": 22130688, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 183.4986754, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.12999397326848e+17, "train_compute_hours": 10.683579383629267, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.44064%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.55936%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "guj_latn_100mb": {"language_name": "Gujarati", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "guj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.190454087, "dataset_raw_mb": 119.04766603903977, "dataset_scaled_mb": 100.0018962, "dataset_tokens": 24248320, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.0454087, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.237479063552e+17, "train_compute_hours": 11.699802055400728, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400"], "dataset_readme_str": "* 99.66021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.33979%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "W2C", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mya_mymr_100mb": {"language_name": "Burmese", "language_script": "Burmese", "dataset_category": "100mb", "language_iso6393": "mya", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 5.0, "dataset_raw_mb": 500.002259, "dataset_scaled_mb": 100.0004518, "dataset_tokens": 41403392, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 500.0, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.11295892013056e+17, "train_compute_hours": 19.97706615396166, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 56.80614%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.62193%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.24461%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 11.75396%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 4.08864%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.48472%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TeDDi", "TICO", "W2C", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "hin_latn_100mb": {"language_name": "Hindi", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.255306654, "dataset_raw_mb": 125.53176002740229, "dataset_scaled_mb": 100.000872, "dataset_tokens": 28580352, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 125.5306654, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.45889270562816e+17, "train_compute_hours": 13.793167398666242, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 74.45561%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.54439%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [IITB](https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "IITB", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "bos_latn_100mb": {"language_name": "Bosnian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "bos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.9699882939, "dataset_raw_mb": 96.99923921005417, "dataset_scaled_mb": 100.0004225, "dataset_tokens": 22827520, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.99882939, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.16513775157248e+17, "train_compute_hours": 11.015847833048904, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 70.89860%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.71791%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.38348%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tel_telu_100mb": {"language_name": "Telugu", "language_script": "Telugu", "dataset_category": "100mb", "language_iso6393": "tel", "language_iso15924": "telu", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.619819498, "dataset_raw_mb": 261.98237866445186, "dataset_scaled_mb": 100.0001637, "dataset_tokens": 20939776, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 261.9819498, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.06862692663296e+17, "train_compute_hours": 10.103381851802531, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.10762%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.59426%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29812%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ben_beng_100mb": {"language_name": "Bengali", "language_script": "Bengali", "dataset_category": "100mb", "language_iso6393": "ben", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.430759774, "dataset_raw_mb": 243.07610841795184, "dataset_scaled_mb": 100.0000539, "dataset_tokens": 19471872, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 243.07597740000003, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.9405418070016e+16, "train_compute_hours": 9.398330435710605, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hin_deva_100mb": {"language_name": "Hindi", "language_script": "Devanagari", "dataset_category": "100mb", "language_iso6393": "hin", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.370151128, "dataset_raw_mb": 237.0185310319568, "dataset_scaled_mb": 100.0014422, "dataset_tokens": 22815232, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 237.0151128, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1647614910464e+17, "train_compute_hours": 11.012290460802328, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lmo_latn_100mb": {"language_name": "Lombard", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "lmo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9431924587, "dataset_raw_mb": 94.32043806526781, "dataset_scaled_mb": 100.001264, "dataset_tokens": 28895232, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 94.31924587, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.47512939249664e+17, "train_compute_hours": 13.946677892695506, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.48817%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.05952%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.79259%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.65971%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nep_deva_100mb": {"language_name": "Nepali", "language_script": "Devanagari", "dataset_category": "100mb", "language_iso6393": "nep", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dty", "npi"], "language_byte_premium": 2.629966681, "dataset_raw_mb": 263.00056386964457, "dataset_scaled_mb": 100.0014813, "dataset_tokens": 21534720, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 262.9966681, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.09924512694272e+17, "train_compute_hours": 10.392863018367535, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 53.66687%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.99086%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 16.12038%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.97982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.24207%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Tatoeba", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "hat_latn_100mb": {"language_name": "Haitian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "hat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9657950702, "dataset_raw_mb": 96.58119001448932, "dataset_scaled_mb": 100.0017426, "dataset_tokens": 23884288, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.57950702, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.21866865016832e+17, "train_compute_hours": 11.521958147045936, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 43.18761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CMU_Haitian_Creole](http://www.speech.cs.cmu.edu/haitian/text/), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 35.83733%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39761%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.81760%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.62603%: [eBible](https://ebible.org/find/)\n* 0.13368%: [CMU Haitian Creole](http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht)\n* 0.00014%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CMU_Haitian_Creole", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible", "CMU Haitian Creole"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "http://www.speech.cs.cmu.edu/haitian/text/", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht"]}, "slv_latn_100mb": {"language_name": "Slovenian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "slv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9721500484, "dataset_raw_mb": 97.21728492072351, "dataset_scaled_mb": 100.0023454, "dataset_tokens": 19798528, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 97.21500483999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.01072461234176e+17, "train_compute_hours": 9.555941789413005, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "swa_latn_100mb": {"language_name": "Swahili", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "swa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["swc", "swh"], "language_byte_premium": 1.258733422, "dataset_raw_mb": 125.87427504733904, "dataset_scaled_mb": 100.0007411, "dataset_tokens": 26009088, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 125.8733422, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.32767184715776e+17, "train_compute_hours": 12.552533827673368, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 57.90197%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.61041%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 5.07304%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.23078%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18380%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "arz_arab_100mb": {"language_name": "Egyptian Arabic", "language_script": "Arabic", "dataset_category": "100mb", "language_iso6393": "arz", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.551170046, "dataset_raw_mb": 155.11875959379006, "dataset_scaled_mb": 100.0011314, "dataset_tokens": 24773632, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 155.1170046, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.26428240019456e+17, "train_compute_hours": 11.953215420021296, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 84.18988%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.34266%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.36232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.09533%: [eBible](https://ebible.org/find/)\n* 0.00981%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "ADD", "AraBench", "DART", "Habibi", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "QADI", "Tatoeba", "TeDDi", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "uzn_cyrl_100mb": {"language_name": "Northern Uzbek", "language_script": "Cyrillic", "dataset_category": "100mb", "language_iso6393": "uzn", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 2.010315146, "dataset_raw_mb": 201.0347122072712, "dataset_scaled_mb": 100.0015906, "dataset_tokens": 21524480, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 201.03151459999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.09905699667968e+17, "train_compute_hours": 10.391084332244247, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "wln_latn_100mb": {"language_name": "Walloon", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "wln", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.218380386, "dataset_raw_mb": 121.8382999425928, "dataset_scaled_mb": 100.0002145, "dataset_tokens": 28432384, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.8380386, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.45125775245312e+17, "train_compute_hours": 13.7209823868295, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.85998%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 12.46237%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 5.48451%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.19218%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "por_latn_100mb": {"language_name": "Portuguese", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "por", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.097878434, "dataset_raw_mb": 109.7887700093983, "dataset_scaled_mb": 100.000844, "dataset_tokens": 22529024, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.78784340000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.14973197336576e+17, "train_compute_hours": 10.870193202730823, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "est_latn_100mb": {"language_name": "Estonian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "est", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ekk", "vro"], "language_byte_premium": 0.9677856419, "dataset_raw_mb": 96.77930793326581, "dataset_scaled_mb": 100.0007685, "dataset_tokens": 18955264, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.77856419000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.672769732608e+16, "train_compute_hours": 9.145164110829382, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "glg_latn_100mb": {"language_name": "Galician", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "glg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059078607, "dataset_raw_mb": 105.9099769508725, "dataset_scaled_mb": 100.0019982, "dataset_tokens": 22216704, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.9078607, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1337983852544e+17, "train_compute_hours": 10.719548369677964, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.77003%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [SLI_GalWeb.1.0](https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.22997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "SLI_GalWeb.1.0", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "spa_latn_100mb": {"language_name": "Spanish", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "spa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.083831966, "dataset_raw_mb": 108.38492997246321, "dataset_scaled_mb": 100.0015993, "dataset_tokens": 22170112, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 108.38319659999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.13150424121344e+17, "train_compute_hours": 10.697858280563434, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ita_latn_100mb": {"language_name": "Italian", "language_script": "Latin", "dataset_category": "100mb", "language_iso6393": "ita", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066886932, "dataset_raw_mb": 106.6903720532762, "dataset_scaled_mb": 100.0015736, "dataset_tokens": 21613056, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 106.6886932, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10307566813184e+17, "train_compute_hours": 10.429079044155579, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hmn_latn_full": {"language_name": "Hmong", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hmn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cqd", "hea", "hma", "hmc", "hmd", "hme", "hmg", "hmh", "hmi", "hmj", "hml", "hmm", "hmp", "hmq", "hms", "hmw", "hmy", "hmz", "hnj", "hrm", "huj", "mmr", "muq", "mww", "sfm"], "language_byte_premium": 1.18983824, "dataset_raw_mb": 411.64505182027375, "dataset_scaled_mb": 345.9672399, "dataset_tokens": 100051968, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 118.983824, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.10651379482624e+17, "train_compute_hours": 48.27976678744809, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4"], "dataset_readme_str": "* 54.54885%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 45.45115%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4"]}, "knc_arab_full": {"language_name": "Central Kanuri", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "knc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 2.50197962, "dataset_raw_mb": 554.5632721189596, "dataset_scaled_mb": 221.6497959, "dataset_tokens": 237422592, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 250.19796200000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.211661320454144e+18, "train_compute_hours": 114.55707029748271, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "mos_latn_full": {"language_name": "Mossi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141292925, "dataset_raw_mb": 11.830385281602279, "dataset_scaled_mb": 10.36577466, "dataset_tokens": 3537920, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.36577466, "tokenizer_training_raw_mb": 11.830385281602279, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8049530986496e+16, "train_compute_hours": 1.706501111450531, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.01233%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "tlh_latn_full": {"language_name": "Klingon", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tlh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.143065822, "dataset_raw_mb": 6.755707000054284, "dataset_scaled_mb": 5.910164463, "dataset_tokens": 1741312, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.910164463, "tokenizer_training_raw_mb": 6.755707000054284, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "46820", "model_parameters": "121625088", "train_compute_flops": 8878964539392000.0, "train_compute_hours": 0.8394657382697892, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 93.86315%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.89344%: [Tatoeba](https://tatoeba.org/en/)\n* 1.24341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "uzn_latn_full": {"language_name": "Northern Uzbek", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "uzn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 1.645764497, "dataset_raw_mb": 239.6054344692336, "dataset_scaled_mb": 145.589138, "dataset_tokens": 52049408, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 164.5764497, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.65611450580992e+17, "train_compute_hours": 25.11235532765743, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "otq_latn_full": {"language_name": "Quer\u00e9taro Otomi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "otq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.252835364, "dataset_raw_mb": 21.899736995895047, "dataset_scaled_mb": 17.48013955, "dataset_tokens": 5702656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 17.48013955, "tokenizer_training_raw_mb": 21.899736995895047, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.9087551586304e+16, "train_compute_hours": 2.7500957863414692, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 95.35934%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.64066%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "mad_latn_full": {"language_name": "Madurese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mad", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.285380683, "dataset_raw_mb": 9.376250997345448, "dataset_scaled_mb": 7.294532368, "dataset_tokens": 2044416, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.294532368, "tokenizer_training_raw_mb": 9.376250997345448, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0425813368832e+16, "train_compute_hours": 0.9857132639622983, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 45.95275%: [IndoNLP](https://huggingface.co/indonlp)\n* 25.66820%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13860%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.75067%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 8.48839%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/)\n* 0.00140%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["IndoNLP", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data", "Tatoeba"], "dataset_links": ["https://huggingface.co/indonlp", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/"]}, "nya_latn_full": {"language_name": "Nyanja", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nya", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.21429492, "dataset_raw_mb": 539.3000409620712, "dataset_scaled_mb": 444.1260785, "dataset_tokens": 112440832, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.429492, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.73810628165632e+17, "train_compute_hours": 54.25118666293248, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.17651%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.50780%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.70728%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.45060%: [eBible](https://ebible.org/find/)\n* 0.15782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "ady_cyrl_full": {"language_name": "Adyghe", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "ady", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.806684783, "dataset_raw_mb": 27.497886010633398, "dataset_scaled_mb": 15.22007949, "dataset_tokens": 4124160, "train_batch_size": 8, "tokenizer_training_scaled_mb": 15.22007949, "tokenizer_training_raw_mb": 27.497886010633398, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.104315379712e+16, "train_compute_hours": 1.9895345408186182, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 75.48292%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.94295%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.57302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00111%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "bgp_latn_full": {"language_name": "Eastern Balochi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bgp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["bal"], "language_code_individuals": [], "language_byte_premium": 1.288416545, "dataset_raw_mb": 7.2630140013336755, "dataset_scaled_mb": 5.637162942, "dataset_tokens": 1735680, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.637162942, "tokenizer_training_raw_mb": 7.2630140013336755, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8853096628224000.0, "train_compute_hours": 0.8370200448502692, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "uig_latn_full": {"language_name": "Uighur", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "uig", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.191355626, "dataset_raw_mb": 9.382463087260318, "dataset_scaled_mb": 7.875451194, "dataset_tokens": 1662976, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.875451194, "tokenizer_training_raw_mb": 9.382463087260318, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8486765199360000.0, "train_compute_hours": 0.8023850733940364, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 97.79454%: [eBible](https://ebible.org/find/)\n* 2.20546%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["eBible", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "WikiMatrix"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "quz_latn_full": {"language_name": "Cusco Quechua", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "quz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.297913163, "dataset_raw_mb": 12.096775758069075, "dataset_scaled_mb": 9.320173416, "dataset_tokens": 2070528, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.320173416, "tokenizer_training_raw_mb": 12.096775758069075, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.056664977408e+16, "train_compute_hours": 0.9990287059130183, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 99.99007%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.00993%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm)\n", "dataset_names": ["AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM"], "dataset_links": ["https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm"]}, "aym_latn_full": {"language_name": "Aymara", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "aym", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ayc", "ayr"], "language_byte_premium": 1.213786361, "dataset_raw_mb": 37.306028016610654, "dataset_scaled_mb": 30.73525063, "dataset_tokens": 9201152, "train_batch_size": 8, "tokenizer_training_scaled_mb": 30.73525063, "tokenizer_training_raw_mb": 37.306028016610654, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.6943465177088e+16, "train_compute_hours": 4.438291253106502, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.19759%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 24.44245%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 7.18196%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.90026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.73814%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.53960%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/"]}, "afb_arab_full": {"language_name": "Gulf Arabic", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "afb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.374141124, "dataset_raw_mb": 19.58100623999951, "dataset_scaled_mb": 14.24963266, "dataset_tokens": 3247616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 14.24963266, "tokenizer_training_raw_mb": 19.58100623999951, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.656983420928e+16, "train_compute_hours": 1.5666025070592002, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.98461%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/)\n* 0.01539%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "ADD", "AraBench", "DART", "Habibi", "QADI", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/"]}, "tcy_knda_full": {"language_name": "Tulu", "language_script": "Kannada", "dataset_category": "full", "language_iso6393": "tcy", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.669467794, "dataset_raw_mb": 14.112610998976608, "dataset_scaled_mb": 5.28667588, "dataset_tokens": 1210368, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.28667588, "tokenizer_training_raw_mb": 14.112610998976608, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6176943636480000.0, "train_compute_hours": 0.5840019438126546, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 55.60953%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 44.39047%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "sot_latn_full": {"language_name": "Southern Sotho", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sot", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.166011747, "dataset_raw_mb": 388.1817779270005, "dataset_scaled_mb": 332.9141228, "dataset_tokens": 94144000, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.6011747, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.80449939963904e+17, "train_compute_hours": 45.42435796022365, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 34.03982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.19565%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.65729%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.10724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "iba_latn_full": {"language_name": "Iban", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "iba", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.298224214, "dataset_raw_mb": 25.93965398930456, "dataset_scaled_mb": 19.98087365, "dataset_tokens": 5278208, "train_batch_size": 8, "tokenizer_training_scaled_mb": 19.98087365, "tokenizer_training_raw_mb": 25.93965398930456, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.6929018109952e+16, "train_compute_hours": 2.5460162576681893, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.98818%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.00622%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00559%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "ful_latn_full": {"language_name": "Fulah", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ful", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ffm", "fub", "fuc", "fue", "fuf", "fuh", "fui", "fuq", "fuv"], "language_byte_premium": 1.258383084, "dataset_raw_mb": 30.233360993150672, "dataset_scaled_mb": 24.02556215, "dataset_tokens": 7806464, "train_batch_size": 8, "tokenizer_training_scaled_mb": 24.02556215, "tokenizer_training_raw_mb": 30.233360993150672, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.983527673856e+16, "train_compute_hours": 3.7662443461911277, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "cawoylel/FulaSpeechCorpora", "allenai/MADLAD-400"], "dataset_readme_str": "* 66.44435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.91335%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.61833%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.98971%: [Fula Speech Corpora](https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora)\n* 0.75836%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.27589%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Fula Speech Corpora", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cawoylel/FulaSpeechCorpora", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "lao_laoo_full": {"language_name": "Lao", "language_script": "Lao", "dataset_category": "full", "language_iso6393": "lao", "language_iso15924": "laoo", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.706914388, "dataset_raw_mb": 1442.7377807076762, "dataset_scaled_mb": 532.9824198, "dataset_tokens": 124077056, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 270.6914388, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.33209361924096e+17, "train_compute_hours": 59.8670669455509, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.18671%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.72107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.03480%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.64542%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 0.41139%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00061%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://dumps.wikimedia.org/"]}, "plt_latn_full": {"language_name": "Plateau Malagasy", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "plt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.151182211, "dataset_raw_mb": 426.60240607198676, "dataset_scaled_mb": 370.5776566, "dataset_tokens": 97517568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.1182211, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.9766725582848e+17, "train_compute_hours": 47.05217691469266, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 95.80021%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.19979%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TeDDi](https://github.com/MorphDiv/TeDDi_sample)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "TeDDi"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/MorphDiv/TeDDi_sample"]}, "mam_latn_full": {"language_name": "Mam", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.34382757, "dataset_raw_mb": 12.452221999939054, "dataset_scaled_mb": 9.266234953, "dataset_tokens": 3580416, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.266234953, "tokenizer_training_raw_mb": 12.452221999939054, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8264574328832e+16, "train_compute_hours": 1.726832481998662, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 47.02182%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.57200%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm)\n* 17.40618%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://ebible.org/find/"]}, "ctd_latn_full": {"language_name": "Tedim Chin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ctd", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.295581309, "dataset_raw_mb": 48.06304500917115, "dataset_scaled_mb": 37.0976678, "dataset_tokens": 11405824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 37.0976678, "tokenizer_training_raw_mb": 48.06304500917115, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.8200448499712e+16, "train_compute_hours": 5.502587858154589, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "san_deva_full": {"language_name": "Sanskrit", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "san", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 2.542780452, "dataset_raw_mb": 303.45206699421016, "dataset_scaled_mb": 119.3386817, "dataset_tokens": 31856128, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 254.2780452, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.62598373425152e+17, "train_compute_hours": 15.372937123832553, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 39.60135%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 37.07458%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.73656%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.30509%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 5.49767%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 0.78475%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "CCNet", "Hindialect", "Wortschatz Leipzig Data", "OSCAR", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://ebible.org/find/"]}, "lfn_latn_full": {"language_name": "Lingua Franca Nova", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lfn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.301456056, "dataset_raw_mb": 6.6593551025883215, "dataset_scaled_mb": 5.116849756, "dataset_tokens": 1593344, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.116849756, "tokenizer_training_raw_mb": 6.6593551025883215, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8131408035840000.0, "train_compute_hours": 0.7687876688430546, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 50.98661%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 49.01339%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "hif_latn_full": {"language_name": "Fiji Hindi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hif", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.281123121, "dataset_raw_mb": 35.86688899820071, "dataset_scaled_mb": 27.99644188, "dataset_tokens": 8768000, "train_batch_size": 8, "tokenizer_training_scaled_mb": 27.99644188, "tokenizer_training_raw_mb": 35.86688899820071, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.4738683011072e+16, "train_compute_hours": 4.229839121046807, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 74.06143%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.82293%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.11449%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00116%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "abk_cyrl_full": {"language_name": "Abkhazian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "abk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.014757281, "dataset_raw_mb": 50.860956363987185, "dataset_scaled_mb": 25.24421023, "dataset_tokens": 6408192, "train_batch_size": 8, "tokenizer_training_scaled_mb": 25.24421023, "tokenizer_training_raw_mb": 50.860956363987185, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.2693904211968e+16, "train_compute_hours": 3.0910600345860657, "dataset_hugging_face": ["cis-lmu/Glot500", "Nart/abkhaz_text", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.32220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Nart/abkhaz](https://huggingface.co/datasets/Nart/abkhaz_text), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.67618%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Nart/abkhaz", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/Nart/abkhaz_text", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pck_latn_full": {"language_name": "Paite Chin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pck", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321489762, "dataset_raw_mb": 9.171149997255899, "dataset_scaled_mb": 6.940008361, "dataset_tokens": 2163712, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.940008361, "tokenizer_training_raw_mb": 9.171149997255899, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1037498015744e+16, "train_compute_hours": 1.0435452669430691, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "shn_mymr_full": {"language_name": "Shan", "language_script": "Burmese", "dataset_category": "full", "language_iso6393": "shn", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.819448606, "dataset_raw_mb": 32.86908699132068, "dataset_scaled_mb": 11.65798409, "dataset_tokens": 2238976, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.65798409, "tokenizer_training_raw_mb": 32.86908699132068, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1418723090432e+16, "train_compute_hours": 1.079588364913571, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 67.30922%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 32.69078%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "pon_latn_full": {"language_name": "Pohnpeian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pon", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.897776051, "dataset_raw_mb": 6.299712851929576, "dataset_scaled_mb": 7.017020386, "dataset_tokens": 1412608, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.017020386, "tokenizer_training_raw_mb": 6.299712851929576, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "21317", "model_parameters": "102750720", "train_compute_flops": 7206172950528000.0, "train_compute_hours": 0.6813108971408292, "dataset_hugging_face": [], "dataset_readme_str": "* 80.41176%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["eBible"], "dataset_links": ["https://ebible.org/find/"]}, "gsw_latn_full": {"language_name": "Swiss German", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "gsw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.142566025, "dataset_raw_mb": 147.17201702472414, "dataset_scaled_mb": 128.808326, "dataset_tokens": 38605824, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.2566025, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.97044502003712e+17, "train_compute_hours": 18.629662007623683, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.51719%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.37695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/)\n* 11.72500%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 2.37834%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00253%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kpv_cyrl_full": {"language_name": "Komi-Zyrian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "kpv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["kom"], "language_code_individuals": [], "language_byte_premium": 1.671354249, "dataset_raw_mb": 8.674338765955815, "dataset_scaled_mb": 5.190006111, "dataset_tokens": 1355776, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.190006111, "tokenizer_training_raw_mb": 8.674338765955815, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6919013007360000.0, "train_compute_hours": 0.6541612297867637, "dataset_hugging_face": [], "dataset_readme_str": "* 99.99709%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00291%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Tatoeba"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "kik_latn_full": {"language_name": "Kikuyu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kik", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.292947561, "dataset_raw_mb": 10.802740565780962, "dataset_scaled_mb": 8.355126605, "dataset_tokens": 2418176, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.355126605, "tokenizer_training_raw_mb": 10.802740565780962, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2337948459008e+16, "train_compute_hours": 1.166496945215302, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 56.84771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.17631%: [eBible](https://ebible.org/find/)\n* 3.33430%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.64167%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "ewe_latn_full": {"language_name": "Ewe", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ewe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078257744, "dataset_raw_mb": 68.22042200335976, "dataset_scaled_mb": 63.26912316, "dataset_tokens": 18470400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 63.26912316, "tokenizer_training_raw_mb": 68.22042200335976, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.4247774650368e+16, "train_compute_hours": 8.910698694216611, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 56.57066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.38723%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.00459%: [eBible](https://ebible.org/find/)\n* 3.36620%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.60581%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.06551%: [Ewe Language Corpus](https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "Ewe Language Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://www.kaggle.com/datasets/yvicherita/ewe-language-corpus"]}, "nde_latn_full": {"language_name": "North Ndebele", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nde", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.969100135, "dataset_raw_mb": 9.857028689902354, "dataset_scaled_mb": 10.17132114, "dataset_tokens": 1766912, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.17132114, "tokenizer_training_raw_mb": 9.857028689902354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9003862130688000.0, "train_compute_hours": 0.8512742378105019, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 65.86973%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MoT](https://github.com/bltlab/mot)\n* 34.13027%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CORP.NCHLT", "Mburisano_Covid", "MoT", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://github.com/bltlab/mot", "https://ebible.org/find/"]}, "tuk_latn_full": {"language_name": "Turkmen", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tuk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.78535505, "dataset_raw_mb": 332.8594018562501, "dataset_scaled_mb": 186.4387713, "dataset_tokens": 57201664, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 178.535505, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.91945768026112e+17, "train_compute_hours": 27.60214534065059, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 83.09655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.30174%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23979%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.35026%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01166%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "ayr_latn_full": {"language_name": "Central Aymara", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ayr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aym"], "language_code_individuals": [], "language_byte_premium": 1.097680523, "dataset_raw_mb": 31.139537018979354, "dataset_scaled_mb": 28.36848825, "dataset_tokens": 7641088, "train_batch_size": 8, "tokenizer_training_scaled_mb": 28.36848825, "tokenizer_training_raw_mb": 31.139537018979354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.8985816342528e+16, "train_compute_hours": 3.6859317269299203, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain"]}, "tum_latn_full": {"language_name": "Tumbuka", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tum", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.20719527, "dataset_raw_mb": 44.295869178886164, "dataset_scaled_mb": 36.69320969, "dataset_tokens": 9842688, "train_batch_size": 8, "tokenizer_training_scaled_mb": 36.69320969, "tokenizer_training_raw_mb": 44.295869178886164, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.023078023168e+16, "train_compute_hours": 4.749091949177019, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 82.53031%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05112%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.41857%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "bar_latn_full": {"language_name": "Bavarian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bar", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129561954, "dataset_raw_mb": 31.268273228659677, "dataset_scaled_mb": 27.68176913, "dataset_tokens": 7961600, "train_batch_size": 8, "tokenizer_training_scaled_mb": 27.68176913, "tokenizer_training_raw_mb": 31.268273228659677, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.0625162551296e+16, "train_compute_hours": 3.8409244593952585, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 64.07962%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 35.91237%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00802%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "rmc_latn_full": {"language_name": "Carpathian Romani", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "rmc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["rom"], "language_code_individuals": [], "language_byte_premium": 1.024618866, "dataset_raw_mb": 5.749587000484519, "dataset_scaled_mb": 5.611439718, "dataset_tokens": 1241600, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.611439718, "tokenizer_training_raw_mb": 5.749587000484519, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "32412", "model_parameters": "110615040", "train_compute_flops": 6328754307072000.0, "train_compute_hours": 0.5983549526686255, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 54.85237%: [eBible](https://ebible.org/find/)\n* 45.14763%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["eBible", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "vec_latn_full": {"language_name": "Venetian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "vec", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9963413972, "dataset_raw_mb": 150.14663903707597, "dataset_scaled_mb": 150.6979831, "dataset_tokens": 40523776, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.63413972000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.06832501522432e+17, "train_compute_hours": 19.555072871211756, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.87927%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.55145%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.45405%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.10163%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01360%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "cjk_latn_full": {"language_name": "Chokwe", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cjk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.170350075, "dataset_raw_mb": 14.161795276318346, "dataset_scaled_mb": 12.10047795, "dataset_tokens": 3622400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 12.10047795, "tokenizer_training_raw_mb": 14.161795276318346, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8473085370368e+16, "train_compute_hours": 1.7465462531984293, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "oss_cyrl_full": {"language_name": "Ossetian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "oss", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.846676453, "dataset_raw_mb": 71.28734400298458, "dataset_scaled_mb": 38.6030503, "dataset_tokens": 14059008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 38.6030503, "tokenizer_training_raw_mb": 71.28734400298458, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.1734853173248e+16, "train_compute_hours": 6.782204300016176, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 71.94447%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.35828%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.22885%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.45171%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.01669%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ltg_latn_full": {"language_name": "Latgalian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ltg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.002770005, "dataset_raw_mb": 18.752839998848287, "dataset_scaled_mb": 18.70103803, "dataset_tokens": 4046848, "train_batch_size": 8, "tokenizer_training_scaled_mb": 18.70103803, "tokenizer_training_raw_mb": 18.752839998848287, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.065252220928e+16, "train_compute_hours": 1.9526020997864728, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 65.03249%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.87600%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.09151%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tbz_latn_full": {"language_name": "Ditammari", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tbz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.330093026, "dataset_raw_mb": 6.806791000588384, "dataset_scaled_mb": 5.117530028, "dataset_tokens": 1868800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.117530028, "tokenizer_training_raw_mb": 6.806791000588384, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "33542", "model_parameters": "112187904", "train_compute_flops": 9532455911424000.0, "train_compute_hours": 0.9012503770800874, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 53.71611%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 46.28389%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "gaz_latn_full": {"language_name": "West Central Oromo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "gaz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["orm"], "language_code_individuals": [], "language_byte_premium": 1.332217666, "dataset_raw_mb": 105.30489411275167, "dataset_scaled_mb": 79.04481137, "dataset_tokens": 25565184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 79.04481137, "tokenizer_training_raw_mb": 105.30489411275167, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.30458930905088e+17, "train_compute_hours": 12.334298921935593, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 96.08435%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.91565%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "ell_latn_full": {"language_name": "Modern Greek", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ell", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238882746, "dataset_raw_mb": 466.33540194332517, "dataset_scaled_mb": 376.4160922, "dataset_tokens": 92225536, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 123.8882746, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.7066011140096e+17, "train_compute_hours": 44.49877416881804, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mri_latn_full": {"language_name": "Maori", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mri", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182543083, "dataset_raw_mb": 532.3461498944638, "dataset_scaled_mb": 450.1706175, "dataset_tokens": 136011776, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 118.2543083, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.9415886389248e+17, "train_compute_hours": 65.62956531347085, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 42.28220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 39.00578%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 17.44951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.04465%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.21786%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "MC4", "NLLB_seed", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "udm_cyrl_full": {"language_name": "Udmurt", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "udm", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740439775, "dataset_raw_mb": 90.09717201470468, "dataset_scaled_mb": 51.76690013, "dataset_tokens": 10932736, "train_batch_size": 8, "tokenizer_training_scaled_mb": 51.76690013, "tokenizer_training_raw_mb": 90.09717201470468, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.577670361088e+16, "train_compute_hours": 5.273433795937746, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.17663%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.77909%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 4.96507%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.07691%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00230%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "dik_latn_full": {"language_name": "Southwestern Dinka", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "dik", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["din"], "language_code_individuals": [], "language_byte_premium": 1.123839426, "dataset_raw_mb": 13.047528502424674, "dataset_scaled_mb": 11.60978001, "dataset_tokens": 3753984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.60978001, "tokenizer_training_raw_mb": 13.047528502424674, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.9148525273088e+16, "train_compute_hours": 1.8104060258192292, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 92.46146%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.53854%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "iku_cans_full": {"language_name": "Inuktitut", "language_script": "Unified Canadian Aboriginal Syllabics", "dataset_category": "full", "language_iso6393": "iku", "language_iso15924": "cans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ike", "ikt"], "language_byte_premium": 2.158751414, "dataset_raw_mb": 160.62538400377773, "dataset_scaled_mb": 74.40661438, "dataset_tokens": 13798400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 74.40661438, "tokenizer_training_raw_mb": 160.62538400377773, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.0412454199296e+16, "train_compute_hours": 6.65717748793344, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 50.41059%: [Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0](https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60)\n* 42.46450%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.06461%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.06031%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Nunavut Hansard Inuktitut\u2013English Parallel Corpus 3.0", "Glot500", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://nrc-digital-repository.canada.ca/eng/view/object/?id=c7e34fa7-7629-43c2-bd6d-19b32bf64f60", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "srd_latn_full": {"language_name": "Sardinian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "srd", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["sdc", "sdn", "src", "sro"], "language_byte_premium": 1.109372772, "dataset_raw_mb": 27.415600352127917, "dataset_scaled_mb": 24.71270347, "dataset_tokens": 6834176, "train_batch_size": 8, "tokenizer_training_scaled_mb": 24.71270347, "tokenizer_training_raw_mb": 27.415600352127917, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.4867853918208e+16, "train_compute_hours": 3.296597097721484, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 44.15271%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 33.42496%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.42165%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00068%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "dar_cyrl_full": {"language_name": "Dargwa", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "dar", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.015493724, "dataset_raw_mb": 34.24500458822022, "dataset_scaled_mb": 16.99087632, "dataset_tokens": 4506624, "train_batch_size": 8, "tokenizer_training_scaled_mb": 16.99087632, "tokenizer_training_raw_mb": 34.24500458822022, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.2983769718784e+16, "train_compute_hours": 2.1730109552304873, "dataset_hugging_face": [], "dataset_readme_str": "* 100.00000%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["Languages of Russia"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download"]}, "lus_latn_full": {"language_name": "Lushai", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.168842547, "dataset_raw_mb": 248.9963719742163, "dataset_scaled_mb": 213.0281556, "dataset_tokens": 62735360, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.88425469999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.20160604225536e+17, "train_compute_hours": 30.26972985405068, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 54.32199%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 39.88323%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.79478%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download"]}, "fry_latn_full": {"language_name": "Western Frisian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fry", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23134294, "dataset_raw_mb": 582.1918311142244, "dataset_scaled_mb": 472.8104675, "dataset_tokens": 133072384, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 123.134294, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.79115497734144e+17, "train_compute_hours": 64.20728342213725, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.10455%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.78619%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.66588%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.44227%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00110%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ina_latn_full": {"language_name": "Interlingua", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ina", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.238452493, "dataset_raw_mb": 23.720226314749326, "dataset_scaled_mb": 19.15311766, "dataset_tokens": 5581824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 19.15311766, "tokenizer_training_raw_mb": 23.720226314749326, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.8480308903936e+16, "train_compute_hours": 2.6926837509175856, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06085%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.53752%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.40163%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lua_latn_full": {"language_name": "Luba-Lulua", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.185565668, "dataset_raw_mb": 24.687544207794474, "dataset_scaled_mb": 20.82343043, "dataset_tokens": 6322688, "train_batch_size": 8, "tokenizer_training_scaled_mb": 20.82343043, "tokenizer_training_raw_mb": 24.687544207794474, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.2259375562752e+16, "train_compute_hours": 3.049977325932917, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "zza_latn_full": {"language_name": "Zaza", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "zza", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["diq", "kiu"], "language_byte_premium": 1.199780688, "dataset_raw_mb": 56.12093799452181, "dataset_scaled_mb": 46.77599711, "dataset_tokens": 14813184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 46.77599711, "tokenizer_training_raw_mb": 56.12093799452181, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.5587604185088e+16, "train_compute_hours": 7.146464395681048, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 35.78443%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.44703%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 19.35033%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.39981%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.01839%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kum_cyrl_full": {"language_name": "Kumyk", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "kum", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.964075989, "dataset_raw_mb": 21.68064298605451, "dataset_scaled_mb": 11.03859683, "dataset_tokens": 2208768, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.03859683, "tokenizer_training_raw_mb": 21.68064298605451, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1256983322624e+16, "train_compute_hours": 1.0642966050480873, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 84.31257%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.68506%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00238%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "chv_cyrl_full": {"language_name": "Chuvash", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "chv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.796969484, "dataset_raw_mb": 460.67370989156666, "dataset_scaled_mb": 256.3614541, "dataset_tokens": 84293120, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 179.6969484, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.30162720653312e+17, "train_compute_hours": 40.669929952676775, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.68880%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.72870%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.16329%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.06333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.35083%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00505%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "TIL", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kea_latn_full": {"language_name": "Kabuverdianu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kea", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.781652324, "dataset_raw_mb": 12.305318588191426, "dataset_scaled_mb": 15.7427007, "dataset_tokens": 3247616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 15.7427007, "tokenizer_training_raw_mb": 12.305318588191426, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.656983420928e+16, "train_compute_hours": 1.5666025070592002, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.94529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.05471%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "vls_latn_full": {"language_name": "Vlaams", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "vls", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.208907177, "dataset_raw_mb": 10.269010209727243, "dataset_scaled_mb": 8.494457147, "dataset_tokens": 2484736, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.494457147, "tokenizer_training_raw_mb": 10.269010209727243, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2672924844032e+16, "train_compute_hours": 1.1981674397993891, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 59.71361%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.28639%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tpi_latn_full": {"language_name": "Tok Pisin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tpi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176877977, "dataset_raw_mb": 19.553309043649552, "dataset_scaled_mb": 16.61455939, "dataset_tokens": 5102592, "train_batch_size": 8, "tokenizer_training_scaled_mb": 16.61455939, "tokenizer_training_raw_mb": 19.553309043649552, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.6034615484416e+16, "train_compute_hours": 2.4614545548902402, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.07802%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.84121%: [eBible](https://ebible.org/find/)\n* 19.34190%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.73886%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Glot500", "BLOOM", "Earthlings", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kat_latn_full": {"language_name": "Georgian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.200493595, "dataset_raw_mb": 6.683025049677142, "dataset_scaled_mb": 5.566897714, "dataset_tokens": 1422336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.566897714, "tokenizer_training_raw_mb": 6.683025049677142, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7253989392384000.0, "train_compute_hours": 0.6858317243708509, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "lbe_cyrl_full": {"language_name": "Lak", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "lbe", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.012712204, "dataset_raw_mb": 20.616377013438974, "dataset_scaled_mb": 10.24308243, "dataset_tokens": 2470912, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.24308243, "tokenizer_training_raw_mb": 20.616377013438974, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2594798526464e+16, "train_compute_hours": 1.1907809515929602, "dataset_hugging_face": [], "dataset_readme_str": "* 99.11772%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.88228%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/"]}, "ckb_arab_full": {"language_name": "Central Kurdish", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "ckb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.651286029, "dataset_raw_mb": 1385.217201558295, "dataset_scaled_mb": 838.8717504, "dataset_tokens": 190565888, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 165.1286029, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.72543052873728e+17, "train_compute_hours": 91.94952499897066, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.06661%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.31684%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.60071%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 15.36201%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.39332%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.26051%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "TICO", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "cos_latn_full": {"language_name": "Corsican", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176702158, "dataset_raw_mb": 487.15348693927746, "dataset_scaled_mb": 413.9989747, "dataset_tokens": 126150656, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.67021580000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.43842379874304e+17, "train_compute_hours": 60.87237046084329, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 60.95981%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.95148%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.08866%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00005%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "ydd_hebr_full": {"language_name": "Eastern Yiddish", "language_script": "Hebrew", "dataset_category": "full", "language_iso6393": "ydd", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": ["yid"], "language_code_individuals": [], "language_byte_premium": 1.806848485, "dataset_raw_mb": 217.1798320373088, "dataset_scaled_mb": 120.1981427, "dataset_tokens": 28306432, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 180.6848485, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.44508080881664e+17, "train_compute_hours": 13.662582192448234, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "bho_deva_full": {"language_name": "Bhojpuri", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "bho", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.515275311, "dataset_raw_mb": 59.71688999342624, "dataset_scaled_mb": 23.74169131, "dataset_tokens": 6156800, "train_batch_size": 8, "tokenizer_training_scaled_mb": 23.74169131, "tokenizer_training_raw_mb": 59.71688999342624, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.140338286592e+16, "train_compute_hours": 2.9690471073233455, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.96562%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 39.06917%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.96521%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "prs_arab_full": {"language_name": "Dari", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "prs", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.66404538, "dataset_raw_mb": 270.7392890680128, "dataset_scaled_mb": 162.6994626, "dataset_tokens": 37549568, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 166.404538, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.91613547118592e+17, "train_compute_hours": 18.116189909394155, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.28229%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.71771%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://tico-19.github.io/"]}, "glk_arab_full": {"language_name": "Gilaki", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "glk", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.682310821, "dataset_raw_mb": 166.0888207376491, "dataset_scaled_mb": 98.72659598, "dataset_tokens": 25519104, "train_batch_size": 8, "tokenizer_training_scaled_mb": 98.72659598, "tokenizer_training_raw_mb": 166.0888207376491, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.30218019651584e+17, "train_compute_hours": 12.311521857967943, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 50.16168%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 48.65632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.13696%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "dov_latn_full": {"language_name": "Dombe", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "dov", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.989047107, "dataset_raw_mb": 7.34544499961471, "dataset_scaled_mb": 7.426789834, "dataset_tokens": 1389056, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.426789834, "tokenizer_training_raw_mb": 7.34544499961471, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7081275359232000.0, "train_compute_hours": 0.6695023976001164, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 89.36384%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.63616%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "fao_latn_full": {"language_name": "Faroese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fao", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155644359, "dataset_raw_mb": 462.6551831005347, "dataset_scaled_mb": 400.3439116, "dataset_tokens": 96587776, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.5644359, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.92938915217408e+17, "train_compute_hours": 46.605133802373125, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 47.79193%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 42.53753%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.12327%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.54727%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "Ndc without informant codes", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "din_latn_full": {"language_name": "Dinka", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "din", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dib", "dik", "dip", "diw", "dks"], "language_byte_premium": 1.241578901, "dataset_raw_mb": 14.510183005729429, "dataset_scaled_mb": 11.6868795, "dataset_tokens": 4125696, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.6868795, "tokenizer_training_raw_mb": 14.510183005729429, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.1039757000704e+16, "train_compute_hours": 1.9892133891574693, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 73.62398%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.66747%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.00269%: [eBible](https://ebible.org/find/)\n* 2.70586%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "frr_latn_full": {"language_name": "Northern Frisian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "frr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.169852216, "dataset_raw_mb": 6.640207120796018, "dataset_scaled_mb": 5.676107657, "dataset_tokens": 1594368, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.676107657, "tokenizer_training_raw_mb": 6.640207120796018, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8131930619904000.0, "train_compute_hours": 0.7688370767909237, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 51.70479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 48.23694%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.05826%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kac_latn_full": {"language_name": "Kachin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kac", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345469973, "dataset_raw_mb": 17.145970000984132, "dataset_scaled_mb": 12.74348023, "dataset_tokens": 4453888, "train_batch_size": 8, "tokenizer_training_scaled_mb": 12.74348023, "tokenizer_training_raw_mb": 17.145970000984132, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "47643", "model_parameters": "123197952", "train_compute_flops": 2.2716467970048e+16, "train_compute_hours": 2.1477387898954476, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 57.50265%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 42.49735%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb"]}, "haw_latn_full": {"language_name": "Hawaiian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "haw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.114669379, "dataset_raw_mb": 290.86755804860354, "dataset_scaled_mb": 260.9451408, "dataset_tokens": 86747136, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.46693789999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.42752032047104e+17, "train_compute_hours": 41.86019212081711, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.98608%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.94220%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.78601%: [Ulukau](https://ulukau.org/index.php?l=en)\n* 1.02324%: [eBible](https://ebible.org/find/)\n* 0.26199%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00047%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "Ulukau", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ulukau.org/index.php?l=en", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "grc_grek_full": {"language_name": "Ancient Greek", "language_script": "Greek", "dataset_category": "full", "language_iso6393": "grc", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.76699504, "dataset_raw_mb": 363.0314690558369, "dataset_scaled_mb": 205.4513232, "dataset_tokens": 47620608, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.69950400000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.43066912768e+17, "train_compute_hours": 22.980871752610913, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 97.28398%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.44105%: [eBible](https://ebible.org/find/)\n* 0.26854%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00642%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "bjn_latn_full": {"language_name": "Banjar", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bjn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.166394033, "dataset_raw_mb": 108.67582861923184, "dataset_scaled_mb": 93.17248335, "dataset_tokens": 25012224, "train_batch_size": 8, "tokenizer_training_scaled_mb": 93.17248335, "tokenizer_training_raw_mb": 108.67582861923184, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.27636976959488e+17, "train_compute_hours": 12.067496003442503, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 90.26394%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.99946%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 3.53524%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12237%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.07900%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://ebible.org/find/"]}, "bqc_latn_full": {"language_name": "Boko (Benin)", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bqc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.976139974, "dataset_raw_mb": 6.638001443666431, "dataset_scaled_mb": 6.800255722, "dataset_tokens": 1806336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.800255722, "tokenizer_training_raw_mb": 6.638001443666431, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "16704", "model_parameters": "99604992", "train_compute_flops": 9218382888960000.0, "train_compute_hours": 0.8715562004107638, "dataset_hugging_face": [], "dataset_readme_str": "* 51.76714%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["eBible"], "dataset_links": ["https://ebible.org/find/"]}, "bsb_latn_full": {"language_name": "Brunei Bisaya", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bsb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.308722105, "dataset_raw_mb": 11.370552334069938, "dataset_scaled_mb": 8.688286299, "dataset_tokens": 2460672, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.688286299, "tokenizer_training_raw_mb": 11.370552334069938, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2552991801344e+16, "train_compute_hours": 1.1868283157634327, "dataset_hugging_face": [], "dataset_readme_str": "", "dataset_names": [], "dataset_links": []}, "run_latn_full": {"language_name": "Rundi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "run", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119232514, "dataset_raw_mb": 101.38722325417693, "dataset_scaled_mb": 90.58638128, "dataset_tokens": 23721984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 90.58638128, "tokenizer_training_raw_mb": 101.38722325417693, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.21052417753088e+17, "train_compute_hours": 11.444955860291957, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 82.69665%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.93706%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.36629%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "bak_cyrl_full": {"language_name": "Bashkir", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "bak", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.27194034, "dataset_raw_mb": 905.0447339015249, "dataset_scaled_mb": 398.3576144, "dataset_tokens": 118369280, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 227.194034, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.04131216850944e+17, "train_compute_hours": 57.11786050227107, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 29.93711%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 29.35888%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.91103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.93919%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.89615%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "TIL", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Languages of Russia", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kmr_latn_full": {"language_name": "Northern Kurdish", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kmr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kur"], "language_code_individuals": [], "language_byte_premium": 1.034619587, "dataset_raw_mb": 55.57345142015274, "dataset_scaled_mb": 53.71389844, "dataset_tokens": 12299264, "train_batch_size": 8, "tokenizer_training_scaled_mb": 53.71389844, "tokenizer_training_raw_mb": 55.57345142015274, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.2761823502336e+16, "train_compute_hours": 5.933845131129949, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 98.85869%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.14131%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [TICO](https://tico-19.github.io/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "TICO"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://tico-19.github.io/"]}, "nds_latn_full": {"language_name": "Low German", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nds", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.135906023, "dataset_raw_mb": 85.59402256925513, "dataset_scaled_mb": 75.35308453, "dataset_tokens": 20312064, "train_batch_size": 8, "tokenizer_training_scaled_mb": 75.35308453, "tokenizer_training_raw_mb": 85.59402256925513, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0365977493504e+17, "train_compute_hours": 9.800560539312874, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.35251%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.73725%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63050%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.27974%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ace_latn_full": {"language_name": "Achinese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ace", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241956888, "dataset_raw_mb": 88.291587493599, "dataset_scaled_mb": 71.09070238, "dataset_tokens": 21666816, "train_batch_size": 8, "tokenizer_training_scaled_mb": 71.09070238, "tokenizer_training_raw_mb": 88.291587493599, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.10567813677056e+17, "train_compute_hours": 10.453684202194387, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 94.09064%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.78765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97639%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14531%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp"]}, "orm_latn_full": {"language_name": "Oromo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "orm", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gax", "gaz", "hae", "orc"], "language_byte_premium": 1.264405846, "dataset_raw_mb": 174.36663301694668, "dataset_scaled_mb": 137.9040073, "dataset_tokens": 39742976, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 126.44058460000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.02881243414528e+17, "train_compute_hours": 19.18149937737356, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "statmt/cc100", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 32.67081%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 27.67571%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.09955%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.69511%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.12785%: [eBible](https://ebible.org/find/)\n* 0.73097%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CC100", "CCNet", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "mrj_cyrl_full": {"language_name": "Western Mari", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "mrj", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["chm"], "language_code_individuals": [], "language_byte_premium": 1.51492647, "dataset_raw_mb": 13.245989001820059, "dataset_scaled_mb": 8.743651434, "dataset_tokens": 1812992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.743651434, "tokenizer_training_raw_mb": 13.245989001820059, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9244773384192000.0, "train_compute_hours": 0.8740513017781528, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.44714%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90362%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.64924%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nso_latn_full": {"language_name": "Pedi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115696353, "dataset_raw_mb": 66.27341201120218, "dataset_scaled_mb": 59.4009399, "dataset_tokens": 17516544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 59.4009399, "tokenizer_training_raw_mb": 66.27341201120218, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.9383823474688e+16, "train_compute_hours": 8.450834219425047, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 46.87464%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 28.49201%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.91655%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.71679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "min_latn_full": {"language_name": "Minangkabau", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "min", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 0.949751483, "dataset_raw_mb": 71.29816799798617, "dataset_scaled_mb": 75.07034132, "dataset_tokens": 17732608, "train_batch_size": 8, "tokenizer_training_scaled_mb": 75.07034132, "tokenizer_training_raw_mb": 71.29816799798617, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.0480727425024e+16, "train_compute_hours": 8.55454150200227, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.69385%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Minangkabau corpora](https://github.com/fajri91/minangNLP), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.18903%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 19.75253%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.74636%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 4.61822%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Minangkabau corpora", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/fajri91/minangNLP", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/indonlp"]}, "mlg_latn_full": {"language_name": "Malagasy", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mlg", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bhr", "bmm", "bzc", "msh", "plt", "skg", "tdx", "tkg", "txy", "xmv", "xmw"], "language_byte_premium": 1.266754162, "dataset_raw_mb": 913.0752279686493, "dataset_scaled_mb": 720.7990748, "dataset_tokens": 210497024, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 126.6754162, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.074300621815808e+18, "train_compute_hours": 101.57024060804004, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.72851%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.99233%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.22403%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.55221%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.26037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.24255%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TeDDi", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ven_latn_full": {"language_name": "Venda", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ven", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300531185, "dataset_raw_mb": 15.367358007945619, "dataset_scaled_mb": 11.81621647, "dataset_tokens": 3268608, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.81621647, "tokenizer_training_raw_mb": 15.367358007945619, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.668088332288e+16, "train_compute_hours": 1.577101695981382, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.17655%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71722%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 9.05878%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.04745%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lim_latn_full": {"language_name": "Limburgan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lim", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.997886276, "dataset_raw_mb": 142.0122011718688, "dataset_scaled_mb": 142.3130116, "dataset_tokens": 39700480, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.7886276, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.02614202957824e+17, "train_compute_hours": 19.15625191601245, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 45.81871%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 40.97133%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.20996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CC100", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "bak_latn_full": {"language_name": "Bashkir", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bak", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.19129188, "dataset_raw_mb": 8.972382725871157, "dataset_scaled_mb": 7.531640966, "dataset_tokens": 1793024, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.531640966, "tokenizer_training_raw_mb": 8.972382725871157, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9145743704064000.0, "train_compute_hours": 0.86468849565696, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "TIL", "Wikipedia Hugging Face", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "fij_latn_full": {"language_name": "Fijian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.210673556, "dataset_raw_mb": 42.95121400529566, "dataset_scaled_mb": 35.47712246, "dataset_tokens": 10642944, "train_batch_size": 8, "tokenizer_training_scaled_mb": 35.47712246, "tokenizer_training_raw_mb": 42.95121400529566, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.4301448798208e+16, "train_compute_hours": 5.133955159103302, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.40494%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 43.45086%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.42740%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.71680%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "bbc_latn_full": {"language_name": "Batak Toba", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bbc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.332303573, "dataset_raw_mb": 8.630488002135204, "dataset_scaled_mb": 6.477868991, "dataset_tokens": 1846784, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.477868991, "tokenizer_training_raw_mb": 8.630488002135204, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9421929381888000.0, "train_compute_hours": 0.8908005961057747, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 98.53788%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.46212%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/indonlp"]}, "grn_latn_full": {"language_name": "Guarani", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "grn", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["gnw", "gug", "gui", "gun", "nhd"], "language_byte_premium": 0.992314188, "dataset_raw_mb": 59.915068003100664, "dataset_scaled_mb": 60.37913065, "dataset_tokens": 15366656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 60.37913065, "tokenizer_training_raw_mb": 59.915068003100664, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.8414000095232e+16, "train_compute_hours": 7.413687281731026, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.98971%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.31205%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.49453%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [GiossaMedia](https://github.com/sgongora27/giossa-gongora-guarani-2021), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.12356%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.05310%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 0.02706%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "GiossaMedia", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "AmericasNLP (excluding AmericasNLI)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/sgongora27/giossa-gongora-guarani-2021", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "als_latn_full": {"language_name": "Tosk Albanian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "als", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["alb", "sqi"], "language_code_individuals": [], "language_byte_premium": 1.167247178, "dataset_raw_mb": 392.5468180865508, "dataset_scaled_mb": 336.301364, "dataset_tokens": 87609344, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.7247178, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.47118744485888e+17, "train_compute_hours": 42.27304493321123, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 85.38346%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.76858%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.84796%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "lez_cyrl_full": {"language_name": "Lezghian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "lez", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.829893752, "dataset_raw_mb": 17.168559741232546, "dataset_scaled_mb": 9.38227136, "dataset_tokens": 2358784, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.38227136, "tokenizer_training_raw_mb": 17.168559741232546, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2034849701888e+16, "train_compute_hours": 1.1378403354512292, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.87415%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 21.89044%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.23541%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bxr_cyrl_full": {"language_name": "Russia Buriat", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "bxr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["bua"], "language_code_individuals": [], "language_byte_premium": 1.588863716, "dataset_raw_mb": 40.31826259332194, "dataset_scaled_mb": 25.37553233, "dataset_tokens": 6060544, "train_batch_size": 8, "tokenizer_training_scaled_mb": 25.37553233, "tokenizer_training_raw_mb": 40.31826259332194, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.0921560358912e+16, "train_compute_hours": 2.923492979388044, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.66194%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 1.33383%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00423%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "twi_latn_full": {"language_name": "Twi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "twi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aka"], "language_code_individuals": [], "language_byte_premium": 1.032409918, "dataset_raw_mb": 64.81990523522025, "dataset_scaled_mb": 62.78504701, "dataset_tokens": 18900480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 62.78504701, "tokenizer_training_raw_mb": 64.81990523522025, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.6442627719168e+16, "train_compute_hours": 9.118212075266793, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 77.35013%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.40427%: [eBible](https://ebible.org/find/)\n* 7.58195%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.66365%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "sah_cyrl_full": {"language_name": "Yakut", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "sah", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.881497141, "dataset_raw_mb": 387.69310187436866, "dataset_scaled_mb": 206.0556423, "dataset_tokens": 47289344, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 188.1497141, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.41359630630912e+17, "train_compute_hours": 22.81945598692259, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 39.11470%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 31.57614%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 19.21725%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 5.94791%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.13671%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00728%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fon_latn_full": {"language_name": "Fon", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fon", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.541191249, "dataset_raw_mb": 62.94311600345822, "dataset_scaled_mb": 40.84056151, "dataset_tokens": 13993984, "train_batch_size": 8, "tokenizer_training_scaled_mb": 40.84056151, "tokenizer_training_raw_mb": 62.94311600345822, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.1406931673088e+16, "train_compute_hours": 6.751200812728321, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 81.63858%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.44701%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.91441%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [FFR](https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "FFR", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/bonaventuredossou/ffr-v1/tree/master/FFR-Dataset", "https://wortschatz.uni-leipzig.de/en/download"]}, "kjh_cyrl_full": {"language_name": "Khakas", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "kjh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.926386897, "dataset_raw_mb": 11.889085000040977, "dataset_scaled_mb": 6.171701551, "dataset_tokens": 1271808, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.171701551, "tokenizer_training_raw_mb": 11.889085000040977, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6490494074880000.0, "train_compute_hours": 0.6136467125341092, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400"], "dataset_readme_str": "* 35.38390%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 32.53557%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.07691%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00363%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Tatoeba", "MADLAD-400 (CommonCrawl)", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "snd_arab_full": {"language_name": "Sindhi", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "snd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.587969761, "dataset_raw_mb": 729.0990529438783, "dataset_scaled_mb": 459.139129, "dataset_tokens": 114626048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 158.7969761, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.84994711011328e+17, "train_compute_hours": 55.30859085925283, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.13852%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.07857%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.09349%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/)\n* 4.06666%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.62277%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "tel_latn_full": {"language_name": "Telugu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tel", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.276370327, "dataset_raw_mb": 25.549920997800896, "dataset_scaled_mb": 20.01763944, "dataset_tokens": 5266432, "train_batch_size": 8, "tokenizer_training_scaled_mb": 20.01763944, "tokenizer_training_raw_mb": 25.549920997800896, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.6870749986816e+16, "train_compute_hours": 2.5405072714807857, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ast_latn_full": {"language_name": "Asturian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ast", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.748880208, "dataset_raw_mb": 394.68745778102857, "dataset_scaled_mb": 225.6800986, "dataset_tokens": 93333504, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 174.88802080000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.76364639043584e+17, "train_compute_hours": 45.038111327757036, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.70018%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.64321%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.37355%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.28306%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "quy_latn_full": {"language_name": "Ayacucho Quechua", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "quy", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["que"], "language_code_individuals": [], "language_byte_premium": 1.163741957, "dataset_raw_mb": 143.81483688665918, "dataset_scaled_mb": 123.5796613, "dataset_tokens": 34850816, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.3741957, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.77864621686784e+17, "train_compute_hours": 16.816291504932305, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 86.82052%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.17948%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/"]}, "hne_deva_full": {"language_name": "Chhattisgarhi", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "hne", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.165410702, "dataset_raw_mb": 18.6421030039668, "dataset_scaled_mb": 8.609037993, "dataset_tokens": 2106880, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.609037993, "tokenizer_training_raw_mb": 18.6421030039668, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0749292904448e+16, "train_compute_hours": 1.0162967836932655, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "allenai/MADLAD-400"], "dataset_readme_str": "* 77.35712%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.41578%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md)\n* 7.22709%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Hindialect", "NLLB_seed", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "srn_latn_full": {"language_name": "Sranan Tongo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "srn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.060725469, "dataset_raw_mb": 12.168331002868737, "dataset_scaled_mb": 11.47170626, "dataset_tokens": 3098112, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.47170626, "tokenizer_training_raw_mb": 12.168331002868737, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "27432", "model_parameters": "107469312", "train_compute_flops": 1.5797454962688e+16, "train_compute_hours": 1.4935775601086838, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.32395%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.83517%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.18414%: [eBible](https://ebible.org/find/)\n* 1.65674%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "arg_latn_full": {"language_name": "Aragonese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "arg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.192925647, "dataset_raw_mb": 51.08599986199692, "dataset_scaled_mb": 42.82412738, "dataset_tokens": 12469760, "train_batch_size": 8, "tokenizer_training_scaled_mb": 42.82412738, "tokenizer_training_raw_mb": 51.08599986199692, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.3624348499968e+16, "train_compute_hours": 6.015392949087884, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.30879%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 38.24094%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.44878%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00149%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mag_deva_full": {"language_name": "Magahi", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "mag", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.555403846, "dataset_raw_mb": 23.194194996500745, "dataset_scaled_mb": 9.076528171, "dataset_tokens": 2488832, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.076528171, "tokenizer_training_raw_mb": 23.194194996500745, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2693828206592e+16, "train_compute_hours": 1.200143757714153, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 77.72701%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.27299%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kas_deva_full": {"language_name": "Kashmiri", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "kas", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.525878379, "dataset_raw_mb": 17.57951283870269, "dataset_scaled_mb": 6.95976219, "dataset_tokens": 1990656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.95976219, "tokenizer_training_raw_mb": 17.57951283870269, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.015903420416e+16, "train_compute_hours": 0.9604905065751274, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "azb_arab_full": {"language_name": "South Azerbaijani", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "azb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.490422093, "dataset_raw_mb": 160.31578392065677, "dataset_scaled_mb": 107.5640147, "dataset_tokens": 26922496, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 149.0422093, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.37411911876608e+17, "train_compute_hours": 12.991671668333849, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 46.02648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 42.37928%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.52167%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "OSCAR", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "roh_latn_full": {"language_name": "Romansh", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "roh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.273368995, "dataset_raw_mb": 110.4428150367891, "dataset_scaled_mb": 86.73276597, "dataset_tokens": 23623680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 86.73276597, "tokenizer_training_raw_mb": 110.4428150367891, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20550737051648e+17, "train_compute_hours": 11.39752423033763, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.92821%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.71848%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.35087%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00179%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00065%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ltz_latn_full": {"language_name": "Luxembourgish", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ltz", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.225348993, "dataset_raw_mb": 709.5590321340954, "dataset_scaled_mb": 579.0668913, "dataset_tokens": 160200192, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.5348993, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.17543835615232e+17, "train_compute_hours": 77.29505354907648, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 61.34921%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.85023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.94131%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.85925%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "azj_latn_full": {"language_name": "North Azerbaijani", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "azj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["aze"], "language_code_individuals": [], "language_byte_premium": 1.076115956, "dataset_raw_mb": 161.22798816759314, "dataset_scaled_mb": 149.8239918, "dataset_tokens": 27041792, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 107.6115956, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3804580634624e+17, "train_compute_hours": 13.051603509099056, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "div_thaa_full": {"language_name": "Dhivehi", "language_script": "Thaana", "dataset_category": "full", "language_iso6393": "div", "language_iso15924": "thaa", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.997934845, "dataset_raw_mb": 1266.7387598446423, "dataset_scaled_mb": 634.0240589, "dataset_tokens": 114510336, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 199.7934845, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.84395829673984e+17, "train_compute_hours": 55.251969350994855, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.19489%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.14021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.85897%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.61771%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18821%: [eBible](https://ebible.org/find/)\n* 0.00002%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "yor_latn_full": {"language_name": "Yoruba", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "yor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.374948177, "dataset_raw_mb": 690.9792977855947, "dataset_scaled_mb": 502.5493392, "dataset_tokens": 155829248, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 137.4948177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.95252228489216e+17, "train_compute_hours": 75.18748342079861, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.33023%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.88587%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.42418%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Menyo20K](https://github.com/uds-lsv/menyo-20k_MT), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 2.89553%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.95515%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.49782%: [eBible](https://ebible.org/find/)\n* 0.01122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Menyo20K", "OSCAR", "TeDDi", "W2C", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/uds-lsv/menyo-20k_MT", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zul_latn_full": {"language_name": "Zulu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "zul", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.163840155, "dataset_raw_mb": 889.3352547501321, "dataset_scaled_mb": 764.1386585, "dataset_tokens": 199965696, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 116.38401549999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.0204890955776e+18, "train_compute_hours": 96.482605400064, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 46.89015%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [isiZulu](https://zenodo.org/record/5035171#.YaippvHMJDZ), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [MC4](https://huggingface.co/datasets/allenai/c4), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 34.71066%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.29050%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.78489%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.32380%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "AfroMAFT", "CORP.NCHLT", "isiZulu", "Wortschatz Leipzig Data", "Mburisano_Covid", "MC4", "TeDDi", "TICO", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://repo.sadilar.org/handle/20.500.12185/7", "https://zenodo.org/record/5035171#.YaippvHMJDZ", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/allenai/c4", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "vep_latn_full": {"language_name": "Veps", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "vep", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173646093, "dataset_raw_mb": 8.087713701741503, "dataset_scaled_mb": 6.891100946, "dataset_tokens": 1751552, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.891100946, "tokenizer_training_raw_mb": 8.087713701741503, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8931222945792000.0, "train_compute_hours": 0.8444065330566982, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 54.55117%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 45.44728%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00154%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "bpy_beng_full": {"language_name": "Bishnupriya", "language_script": "Bengali", "dataset_category": "full", "language_iso6393": "bpy", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.330506839, "dataset_raw_mb": 21.65858669530776, "dataset_scaled_mb": 9.293509177, "dataset_tokens": 2019328, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.293509177, "tokenizer_training_raw_mb": 21.65858669530776, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.030535774208e+16, "train_compute_hours": 0.9743247319784728, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.33593%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 26.65812%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.00595%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mya_mymr_full": {"language_name": "Burmese", "language_script": "Burmese", "dataset_category": "full", "language_iso6393": "mya", "language_iso15924": "mymr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 5.0, "dataset_raw_mb": 3810.7010309999996, "dataset_scaled_mb": 762.1402062, "dataset_tokens": 315374592, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 500.0, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.60946223906816e+18, "train_compute_hours": 152.16733896644422, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 56.80614%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.62193%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.24461%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 11.75396%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 4.08864%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.48472%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TeDDi", "TICO", "W2C", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "guj_latn_full": {"language_name": "Gujarati", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "guj", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.190454087, "dataset_raw_mb": 120.95021202348862, "dataset_scaled_mb": 101.6000645, "dataset_tokens": 24635392, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.0454087, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.25773442187264e+17, "train_compute_hours": 11.891307261341325, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400"], "dataset_readme_str": "* 99.66021%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 0.33979%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "W2C", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kbp_latn_full": {"language_name": "Kabiy\u00e8", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kbp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.441209593, "dataset_raw_mb": 20.083003005589156, "dataset_scaled_mb": 13.93482468, "dataset_tokens": 4698624, "train_batch_size": 8, "tokenizer_training_scaled_mb": 13.93482468, "tokenizer_training_raw_mb": 20.083003005589156, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.396178579456e+16, "train_compute_hours": 2.265477929667491, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 48.53530%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.12198%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.69494%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.64778%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "kom_cyrl_full": {"language_name": "Komi", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "kom", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["koi", "kpv"], "language_byte_premium": 1.614292695, "dataset_raw_mb": 29.387639999488385, "dataset_scaled_mb": 18.20465402, "dataset_tokens": 4716032, "train_batch_size": 8, "tokenizer_training_scaled_mb": 18.20465402, "tokenizer_training_raw_mb": 29.387639999488385, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.4054283173888e+16, "train_compute_hours": 2.27422313644032, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.95323%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 25.40122%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.73519%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.04414%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.86405%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00218%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hat_latn_full": {"language_name": "Haitian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9657950702, "dataset_raw_mb": 749.2594279507356, "dataset_scaled_mb": 775.7954571, "dataset_tokens": 185333248, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.57950702, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.45806868283392e+17, "train_compute_hours": 89.42174027406617, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 43.18761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CMU_Haitian_Creole](http://www.speech.cs.cmu.edu/haitian/text/), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 35.83733%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39761%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.81760%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.62603%: [eBible](https://ebible.org/find/)\n* 0.13368%: [CMU Haitian Creole](http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht)\n* 0.00014%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "CMU_Haitian_Creole", "Earthlings", "Wortschatz Leipzig Data", "MC4", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible", "CMU Haitian Creole"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "http://www.speech.cs.cmu.edu/haitian/text/", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "http://www.speech.cs.cmu.edu/haitian/text/newswire-all.ht"]}, "bcl_latn_full": {"language_name": "Central Bikol", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bcl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["bik"], "language_code_individuals": [], "language_byte_premium": 1.218848876, "dataset_raw_mb": 11.568451338017548, "dataset_scaled_mb": 9.491292617, "dataset_tokens": 2638336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.491292617, "tokenizer_training_raw_mb": 11.568451338017548, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3456800940032e+16, "train_compute_hours": 1.2722793616030255, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 54.73555%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 45.26445%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "arz_arab_full": {"language_name": "Egyptian Arabic", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "arz", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.551170046, "dataset_raw_mb": 189.83468016763783, "dataset_scaled_mb": 122.3816052, "dataset_tokens": 30322176, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 155.1170046, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.54803509526528e+17, "train_compute_hours": 14.635968173417194, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 84.18988%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 8.34266%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.36232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [QADI](https://alt.qcri.org/resources/qadi), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.09533%: [eBible](https://ebible.org/find/)\n* 0.00981%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "OSCAR 2021/09", "Glot500", "ADD", "AraBench", "DART", "Habibi", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "QADI", "Tatoeba", "TeDDi", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://alt.qcri.org/resources/qadi", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "uzn_cyrl_full": {"language_name": "Northern Uzbek", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "uzn", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["uzb"], "language_code_individuals": [], "language_byte_premium": 2.010315146, "dataset_raw_mb": 420.00245977242935, "dataset_scaled_mb": 208.9236907, "dataset_tokens": 44960768, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 201.03151459999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.29459346325504e+17, "train_compute_hours": 21.694338198047653, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "wln_latn_full": {"language_name": "Walloon", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "wln", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.218380386, "dataset_raw_mb": 124.66932397765693, "dataset_scaled_mb": 102.3238107, "dataset_tokens": 29091328, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.8380386, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.485053263872e+17, "train_compute_hours": 14.04050358569891, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.85998%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 12.46237%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 5.48451%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.19218%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mwl_latn_full": {"language_name": "Mirandese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mwl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241381732, "dataset_raw_mb": 10.082868045258616, "dataset_scaled_mb": 8.122294525, "dataset_tokens": 2293760, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.122294525, "tokenizer_training_raw_mb": 10.082868045258616, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.17058830336e+16, "train_compute_hours": 1.1067380322676366, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 63.81709%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 36.18263%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "che_cyrl_full": {"language_name": "Chechen", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "che", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.831603287, "dataset_raw_mb": 157.72367697505499, "dataset_scaled_mb": 86.11235746, "dataset_tokens": 23590400, "train_batch_size": 8, "tokenizer_training_scaled_mb": 86.11235746, "tokenizer_training_raw_mb": 157.72367697505499, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20376977850368e+17, "train_compute_hours": 11.381096087671157, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.55150%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 24.11809%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.84904%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.36799%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.11311%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00027%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tyv_cyrl_full": {"language_name": "Tuvinian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "tyv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.860256075, "dataset_raw_mb": 127.23054500464379, "dataset_scaled_mb": 68.39410268, "dataset_tokens": 15576576, "train_batch_size": 8, "tokenizer_training_scaled_mb": 68.39410268, "tokenizer_training_raw_mb": 127.23054500464379, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.948895551488e+16, "train_compute_hours": 7.515319430497746, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 49.97941%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 35.15487%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.54558%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.31541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00397%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00076%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_cyrl_full": {"language_name": "Azerbaijani", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "aze", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.818803281, "dataset_raw_mb": 31.13153799274141, "dataset_scaled_mb": 17.1164954, "dataset_tokens": 3627008, "train_batch_size": 8, "tokenizer_training_scaled_mb": 17.1164954, "tokenizer_training_raw_mb": 31.13153799274141, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8500521033728e+16, "train_compute_hours": 1.7491401704615566, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ngu_latn_full": {"language_name": "Guerrero Nahuatl", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ngu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.435018628, "dataset_raw_mb": 8.599322000697855, "dataset_scaled_mb": 5.992481096, "dataset_tokens": 1508864, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.992481096, "tokenizer_training_raw_mb": 8.599322000697855, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7697401970688000.0, "train_compute_hours": 0.7277543681377746, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 80.79780%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.69637%: [eBible](https://ebible.org/find/)\n* 9.49744%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/)\n* 0.00839%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/"]}, "sco_latn_full": {"language_name": "Scots", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sco", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.193629502, "dataset_raw_mb": 51.295032249459325, "dataset_scaled_mb": 42.97399835, "dataset_tokens": 12578304, "train_batch_size": 8, "tokenizer_training_scaled_mb": 42.97399835, "tokenizer_training_raw_mb": 51.295032249459325, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.418769412096e+16, "train_compute_hours": 6.068654716890764, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 95.84087%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.15532%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00381%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lin_latn_full": {"language_name": "Lingala", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.139208008, "dataset_raw_mb": 53.91824601809518, "dataset_scaled_mb": 47.32958831, "dataset_tokens": 13213184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 47.32958831, "tokenizer_training_raw_mb": 53.91824601809518, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.742771531776e+16, "train_compute_hours": 6.374983993679128, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 49.58207%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.44683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.44468%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Lingala_Song_Lyrics](https://github.com/espoirMur/songs_lyrics_webscrap), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.78539%: [eBible](https://ebible.org/find/)\n* 1.74103%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "CC100", "Wortschatz Leipzig Data", "Lingala_Song_Lyrics", "MoT", "TICO", "Wikipedia Hugging Face", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/espoirMur/songs_lyrics_webscrap", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "kaa_latn_full": {"language_name": "Kara-Kalpak", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kaa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22736123, "dataset_raw_mb": 202.78035402645045, "dataset_scaled_mb": 165.2165223, "dataset_tokens": 38767104, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.736123, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.97850849214464e+17, "train_compute_hours": 18.705898471185687, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 97.52670%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.65677%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81649%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "hsb_latn_full": {"language_name": "Upper Sorbian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hsb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.124658236, "dataset_raw_mb": 11.590298082043772, "dataset_scaled_mb": 10.30561793, "dataset_tokens": 2503680, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.30561793, "tokenizer_training_raw_mb": 11.590298082043772, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.2762025426944e+16, "train_compute_hours": 1.2065914949110692, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 52.92169%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.06296%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.86926%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14609%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hil_latn_full": {"language_name": "Hiligaynon", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.351057958, "dataset_raw_mb": 43.400313008199284, "dataset_scaled_mb": 32.12320593, "dataset_tokens": 9034752, "train_batch_size": 8, "tokenizer_training_scaled_mb": 32.12320593, "tokenizer_training_raw_mb": 43.400313008199284, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.6101843542016e+16, "train_compute_hours": 4.3587197530633315, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 99.96778%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.01707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/)\n* 0.01288%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00227%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/"]}, "jav_latn_full": {"language_name": "Javanese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "jav", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.146845795, "dataset_raw_mb": 533.9468941720454, "dataset_scaled_mb": 465.5786301, "dataset_tokens": 115332096, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.6845795, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.88606289477632e+17, "train_compute_hours": 55.65004918697612, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 46.56951%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 38.81475%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.78232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 6.69194%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.05217%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.08930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kon_latn_full": {"language_name": "Kongo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["kng", "kwy", "ldi"], "language_byte_premium": 1.230994587, "dataset_raw_mb": 13.93176800518069, "dataset_scaled_mb": 11.31748925, "dataset_tokens": 3549184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.31748925, "tokenizer_training_raw_mb": 13.93176800518069, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.8103357145088e+16, "train_compute_hours": 1.7115901300810474, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 80.64648%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.58706%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.42232%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.34414%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tet_latn_full": {"language_name": "Tetum", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tet", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.400532559, "dataset_raw_mb": 134.48996400045752, "dataset_scaled_mb": 96.02773112, "dataset_tokens": 28032512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 96.02773112, "tokenizer_training_raw_mb": 134.48996400045752, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4305608105984e+17, "train_compute_hours": 13.525302209293965, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 98.26624%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.01934%: [eBible](https://ebible.org/find/)\n* 0.71376%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00066%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "zsm_latn_full": {"language_name": "Standard Malay", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "zsm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.143822408, "dataset_raw_mb": 983.1374726227026, "dataset_scaled_mb": 859.5193325, "dataset_tokens": 185929728, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.38224079999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.48883059376128e+17, "train_compute_hours": 89.71258015919756, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 91.23035%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.76965%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "pag_latn_full": {"language_name": "Pangasinan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.043908443, "dataset_raw_mb": 38.02948176394745, "dataset_scaled_mb": 36.42990151, "dataset_tokens": 10441728, "train_batch_size": 8, "tokenizer_training_scaled_mb": 36.42990151, "tokenizer_training_raw_mb": 38.02948176394745, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.3272742068224e+16, "train_compute_hours": 5.036695613722997, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 96.89102%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.15082%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.95815%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "xal_cyrl_full": {"language_name": "Kalmyk", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "xal", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.723396561, "dataset_raw_mb": 10.435124999740967, "dataset_scaled_mb": 6.054976107, "dataset_tokens": 1474048, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.054976107, "tokenizer_training_raw_mb": 10.435124999740967, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7519723388928000.0, "train_compute_hours": 0.7109556658622836, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.88105%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 21.64138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.15243%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23428%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09086%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09", "Tatoeba"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://tatoeba.org/en/"]}, "uig_arab_full": {"language_name": "Uighur", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "uig", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.308625158, "dataset_raw_mb": 916.9999639737616, "dataset_scaled_mb": 397.2060864, "dataset_tokens": 104039936, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 230.86251579999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.30952463908864e+17, "train_compute_hours": 50.1991420422926, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 65.05923%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.33662%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.64713%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.70495%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 2.67723%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.57484%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "WikiMatrix", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "ava_cyrl_full": {"language_name": "Avaric", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "ava", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.938794023, "dataset_raw_mb": 59.5800369996377, "dataset_scaled_mb": 30.73046249, "dataset_tokens": 8009728, "train_batch_size": 8, "tokenizer_training_scaled_mb": 30.73046249, "tokenizer_training_raw_mb": 59.5800369996377, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.0867118972928e+16, "train_compute_hours": 3.8638003392586477, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.59506%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.75473%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 6.23003%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.42018%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "knc_latn_full": {"language_name": "Central Kanuri", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "knc", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kau"], "language_code_individuals": [], "language_byte_premium": 1.176891381, "dataset_raw_mb": 11.847892189577422, "dataset_scaled_mb": 10.06710762, "dataset_tokens": 3433472, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.06710762, "tokenizer_training_raw_mb": 11.847892189577422, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.7507088728064e+16, "train_compute_hours": 1.6552156615624147, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "bem_latn_full": {"language_name": "Bemba", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bem", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.155237492, "dataset_raw_mb": 40.84214773014419, "dataset_scaled_mb": 35.35389737, "dataset_tokens": 10177024, "train_batch_size": 8, "tokenizer_training_scaled_mb": 35.35389737, "tokenizer_training_raw_mb": 40.84214773014419, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.1929439731712e+16, "train_compute_hours": 4.9096924837254985, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 99.99219%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.00781%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "glv_latn_full": {"language_name": "Manx", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "glv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.224310206, "dataset_raw_mb": 8.111667002953375, "dataset_scaled_mb": 6.625499782, "dataset_tokens": 1939968, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.625499782, "tokenizer_training_raw_mb": 8.111667002953375, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9892777623552000.0, "train_compute_hours": 0.9353171571358255, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 42.26375%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 41.05658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 16.67753%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00213%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "kur_latn_full": {"language_name": "Kurdish", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kur", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.288322619, "dataset_raw_mb": 883.1881046509786, "dataset_scaled_mb": 685.5333374, "dataset_tokens": 189872128, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.8322619, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.6897014562816e+17, "train_compute_hours": 91.61172285938969, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 71.61672%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.40823%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.33884%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.36208%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.27236%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00178%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "BLOOM", "CCNet", "Earthlings", "OSCAR", "Tatoeba", "TICO", "W2C", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "ekk_latn_full": {"language_name": "Standard Estonian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ekk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["est"], "language_code_individuals": [], "language_byte_premium": 0.993591004, "dataset_raw_mb": 61.01578290803244, "dataset_scaled_mb": 61.40935522, "dataset_tokens": 12375552, "train_batch_size": 8, "tokenizer_training_scaled_mb": 61.40935522, "tokenizer_training_raw_mb": 61.01578290803244, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.3143571161088e+16, "train_compute_hours": 5.969937637048321, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.24897%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.75103%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/"]}, "bik_latn_full": {"language_name": "Bikol", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bik", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bcl", "bln", "bto", "cts", "fbl", "lbl", "rbl", "ubl"], "language_byte_premium": 1.272069912, "dataset_raw_mb": 24.49377000074888, "dataset_scaled_mb": 19.25505019, "dataset_tokens": 5440512, "train_batch_size": 8, "tokenizer_training_scaled_mb": 19.25505019, "tokenizer_training_raw_mb": 24.49377000074888, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.7749736382464e+16, "train_compute_hours": 2.6236114397965964, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 43.21685%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 29.79360%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.98955%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "pnb_arab_full": {"language_name": "Western Panjabi", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "pnb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["lah"], "language_code_individuals": [], "language_byte_premium": 1.414617177, "dataset_raw_mb": 171.98620223460927, "dataset_scaled_mb": 121.5779117, "dataset_tokens": 30110208, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 141.4617177, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.53639976108032e+17, "train_compute_hours": 14.525961377486663, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 51.15181%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 41.73099%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.11696%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00023%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tir_ethi_full": {"language_name": "Tigrinya", "language_script": "Ge'ez", "dataset_category": "full", "language_iso6393": "tir", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.762985532, "dataset_raw_mb": 446.00015798561907, "dataset_scaled_mb": 252.9800443, "dataset_tokens": 56515072, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.29855320000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.88450203222016e+17, "train_compute_hours": 27.27165557735424, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "legacy-datasets/wikipedia", "csebuetnlp/xlsum"], "dataset_readme_str": "* 50.52196%: [Tigrinya Language Modeling Dataset](https://zenodo.org/record/5139094)\n* 20.99445%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 13.46805%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.08565%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 4.86709%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.06280%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Tigrinya Language Modeling Dataset", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "HornMT", "Wortschatz Leipzig Data", "MoT", "Parallel Corpora for Ethiopian Languages", "TICO", "Wikipedia Hugging Face", "XLSum", "Wikipedia 2023/08"], "dataset_links": ["https://zenodo.org/record/5139094", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://dumps.wikimedia.org/"]}, "kal_latn_full": {"language_name": "Kalaallisut", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.341175059, "dataset_raw_mb": 188.3504929914857, "dataset_scaled_mb": 140.4369189, "dataset_tokens": 30082048, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.1175059, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.53518997897216e+17, "train_compute_hours": 14.514523437554969, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.53226%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.83885%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.54939%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.07923%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00028%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "lvs_latn_full": {"language_name": "Standard Latvian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lvs", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["lav"], "language_code_individuals": [], "language_byte_premium": 1.206980036, "dataset_raw_mb": 42.74751001761776, "dataset_scaled_mb": 35.41691556, "dataset_tokens": 8333312, "train_batch_size": 8, "tokenizer_training_scaled_mb": 35.41691556, "tokenizer_training_raw_mb": 42.74751001761776, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.2518484615168e+16, "train_compute_hours": 4.019929454524975, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "cfm_latn_full": {"language_name": "Falam Chin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cfm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323887964, "dataset_raw_mb": 18.648145992140606, "dataset_scaled_mb": 14.08589435, "dataset_tokens": 4315648, "train_batch_size": 8, "tokenizer_training_scaled_mb": 14.08589435, "tokenizer_training_raw_mb": 18.648145992140606, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.2016727908352e+16, "train_compute_hours": 2.0815815476987347, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mai_deva_full": {"language_name": "Maithili", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "mai", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.38980971, "dataset_raw_mb": 99.73484899094552, "dataset_scaled_mb": 41.73338512, "dataset_tokens": 11159040, "train_batch_size": 8, "tokenizer_training_scaled_mb": 41.73338512, "tokenizer_training_raw_mb": 99.73484899094552, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.6935272480768e+16, "train_compute_hours": 5.38297121636352, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 67.00767%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 12.01673%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.36867%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.75652%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.83462%: [eBible](https://ebible.org/find/)\n* 0.01578%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fur_latn_full": {"language_name": "Friulian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.067269167, "dataset_raw_mb": 22.112805251670075, "dataset_scaled_mb": 20.71905189, "dataset_tokens": 5487616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 20.71905189, "tokenizer_training_raw_mb": 22.112805251670075, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.7999531565056e+16, "train_compute_hours": 2.647228438878022, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 72.56517%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 15.82188%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.61295%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "ssw_latn_full": {"language_name": "Swati", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ssw", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.141426619, "dataset_raw_mb": 23.933373993007812, "dataset_scaled_mb": 20.967948, "dataset_tokens": 5566976, "train_batch_size": 8, "tokenizer_training_scaled_mb": 20.967948, "tokenizer_training_raw_mb": 23.933373993007812, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.839329865728e+16, "train_compute_hours": 2.684457327597382, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.30477%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.07695%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 14.60799%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.01028%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/"]}, "bam_latn_full": {"language_name": "Bambara", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.256878532, "dataset_raw_mb": 16.1348379929441, "dataset_scaled_mb": 12.83722936, "dataset_tokens": 4511744, "train_batch_size": 8, "tokenizer_training_scaled_mb": 12.83722936, "tokenizer_training_raw_mb": 16.1348379929441, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.3015647346688e+16, "train_compute_hours": 2.176024840050502, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 67.90603%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.26277%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.19633%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.75328%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.88159%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pcm_latn_full": {"language_name": "Nigerian Pidgin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pcm", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.954503897, "dataset_raw_mb": 23.497950372502547, "dataset_scaled_mb": 24.61797217, "dataset_tokens": 5281280, "train_batch_size": 8, "tokenizer_training_scaled_mb": 24.61797217, "tokenizer_training_raw_mb": 23.497950372502547, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.6938947207168e+16, "train_compute_hours": 2.546955008677702, "dataset_hugging_face": ["castorini/afriberta-corpus", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 48.40160%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 48.06214%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 3.19131%: [eBible](https://ebible.org/find/)\n* 0.34495%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["AfriBERTa", "Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/castorini/afriberta-corpus", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "sag_latn_full": {"language_name": "Sango", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sag", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.162464677, "dataset_raw_mb": 18.450196012565485, "dataset_scaled_mb": 15.87161862, "dataset_tokens": 4929024, "train_batch_size": 8, "tokenizer_training_scaled_mb": 15.87161862, "tokenizer_training_raw_mb": 18.450196012565485, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.5141258027008e+16, "train_compute_hours": 2.376991668008029, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 60.53835%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.27653%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93802%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.24653%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00058%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Tatoeba", "TeDDi", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://github.com/MorphDiv/TeDDi_sample", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "mgh_latn_full": {"language_name": "Makhuwa-Meetto", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mgh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113355123, "dataset_raw_mb": 6.4195049997083675, "dataset_scaled_mb": 5.765909607, "dataset_tokens": 1251328, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.765909607, "tokenizer_training_raw_mb": 6.4195049997083675, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6385977262080000.0, "train_compute_hours": 0.603765122960291, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 85.38638%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 14.61362%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "yid_hebr_full": {"language_name": "Yiddish", "language_script": "Hebrew", "dataset_category": "full", "language_iso6393": "yid", "language_iso15924": "hebr", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ydd", "yih"], "language_byte_premium": 1.546105069, "dataset_raw_mb": 689.617690916504, "dataset_scaled_mb": 446.0354634, "dataset_tokens": 85695488, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 154.6105069, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.37368371019776e+17, "train_compute_hours": 41.35119144186974, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.13900%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.11646%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.12670%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 8.66901%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.94883%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "gom_latn_full": {"language_name": "Goan Konkani", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "gom", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.2133745, "dataset_raw_mb": 12.594551000358859, "dataset_scaled_mb": 10.37977228, "dataset_tokens": 2806784, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.37977228, "tokenizer_training_raw_mb": 12.594551000358859, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4308874256384e+16, "train_compute_hours": 1.3528390206035783, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43524%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 44.56476%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "xho_latn_full": {"language_name": "Xhosa", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "xho", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.198781752, "dataset_raw_mb": 572.2490950788933, "dataset_scaled_mb": 477.3588638, "dataset_tokens": 127885824, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.87817519999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.5268920549376e+17, "train_compute_hours": 61.70879761031913, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 41.02036%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.08328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.57971%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536)\n* 0.21721%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09945%: [Lacuna Project: IsiXhosa](https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "CORP.NCHLT", "Mburisano_Covid", "Wikipedia 2023/08", "Lacuna Project: IsiXhosa"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://repo.sadilar.org/handle/20.500.12185/7", "https://repo.sadilar.org/handle/20.500.12185/536", "https://dumps.wikimedia.org/", "https://github.com/Chiamakac/lacuna_pos_ner/tree/main/language_corpus/xho"]}, "rue_cyrl_full": {"language_name": "Rusyn", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "rue", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.560953869, "dataset_raw_mb": 7.857343758896957, "dataset_scaled_mb": 5.033680953, "dataset_tokens": 1160704, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.033680953, "tokenizer_training_raw_mb": 7.857343758896957, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5920616153088000.0, "train_compute_hours": 0.5597673453828655, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 51.53918%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 42.05551%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.38884%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.01646%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "BLOOM", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "szl_latn_full": {"language_name": "Silesian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "szl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066563024, "dataset_raw_mb": 28.84150320968713, "dataset_scaled_mb": 27.04153675, "dataset_tokens": 7593472, "train_batch_size": 8, "tokenizer_training_scaled_mb": 27.04153675, "tokenizer_training_raw_mb": 28.84150320968713, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.874830188544e+16, "train_compute_hours": 3.6634758146234185, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 39.46050%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 34.03961%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 26.49989%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "tdx_latn_full": {"language_name": "Tandroy-Mahafaly Malagasy", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tdx", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["mlg"], "language_code_individuals": [], "language_byte_premium": 1.000930264, "dataset_raw_mb": 5.238290999581099, "dataset_scaled_mb": 5.233422535, "dataset_tokens": 1303552, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.233422535, "tokenizer_training_raw_mb": 5.238290999581099, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "43798", "model_parameters": "120052224", "train_compute_flops": 6647791878144000.0, "train_compute_hours": 0.6285185048427055, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 71.99496%: [eBible](https://ebible.org/find/)\n* 28.00504%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["eBible", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "mdf_cyrl_full": {"language_name": "Moksha", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "mdf", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.712267834, "dataset_raw_mb": 10.558169997344478, "dataset_scaled_mb": 6.166190702, "dataset_tokens": 1302016, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.166190702, "tokenizer_training_raw_mb": 10.558169997344478, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6641782161408000.0, "train_compute_hours": 0.627950313442211, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 79.42795%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.39365%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.17561%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 0.00279%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Languages of Russia", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download", "https://tatoeba.org/en/"]}, "lub_latn_full": {"language_name": "Luba-Katanga", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lub", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.296565957, "dataset_raw_mb": 11.168526997579475, "dataset_scaled_mb": 8.613928923, "dataset_tokens": 2269184, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.613928923, "tokenizer_training_raw_mb": 11.168526997579475, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.158046285824e+16, "train_compute_hours": 1.0948801247790547, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "khk_cyrl_full": {"language_name": "Halh Mongolian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "khk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["mon"], "language_code_individuals": [], "language_byte_premium": 1.801810967, "dataset_raw_mb": 195.04850403714494, "dataset_scaled_mb": 108.2513691, "dataset_tokens": 23605760, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 180.1810967, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.20445436362752e+17, "train_compute_hours": 11.387568528842008, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "umb_latn_full": {"language_name": "Umbundu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "umb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.167261104, "dataset_raw_mb": 18.814533951412507, "dataset_scaled_mb": 16.11853071, "dataset_tokens": 4743168, "train_batch_size": 8, "tokenizer_training_scaled_mb": 16.11853071, "tokenizer_training_raw_mb": 18.814533951412507, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.420609384448e+16, "train_compute_hours": 2.288576145296291, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 99.99900%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "aze_arab_full": {"language_name": "Azerbaijani", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "aze", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.198662364, "dataset_raw_mb": 320.3526641751087, "dataset_scaled_mb": 267.2584656, "dataset_tokens": 56526848, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 119.8662364, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.88485216354304e+17, "train_compute_hours": 27.27496590986147, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.47341%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 22.81190%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.00426%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 5.71043%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tzo_latn_full": {"language_name": "Tzotzil", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tzo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.487549999, "dataset_raw_mb": 13.4203790004673, "dataset_scaled_mb": 9.021800282, "dataset_tokens": 3463680, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.021800282, "tokenizer_training_raw_mb": 13.4203790004673, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "45078", "model_parameters": "121625088", "train_compute_flops": 1.7668828495872e+16, "train_compute_hours": 1.6705074214278983, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 50.76388%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.89747%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "bua_cyrl_full": {"language_name": "Buriat", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "bua", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bxm", "bxr", "bxu"], "language_byte_premium": 1.701900789, "dataset_raw_mb": 66.54405000940844, "dataset_scaled_mb": 39.09984086, "dataset_tokens": 8951808, "train_batch_size": 8, "tokenizer_training_scaled_mb": 39.09984086, "tokenizer_training_raw_mb": 66.54405000940844, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.5674892361728e+16, "train_compute_hours": 4.318353459654284, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 89.97116%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 8.41374%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.21634%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.38769%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.00722%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00386%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Wortschatz Leipzig Data", "Tatoeba", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "zho_hant_full": {"language_name": "Chinese", "language_script": "Han Traditional", "dataset_category": "full", "language_iso6393": "zho", "language_iso15924": "hant", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.989382544, "dataset_raw_mb": 175.44088803377926, "dataset_scaled_mb": 177.3236137, "dataset_tokens": 42692096, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 98.9382544, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.17931664457728e+17, "train_compute_hours": 20.604448276003378, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 36.42318%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.54996%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 29.93702%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.08984%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "CCNet", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "pms_latn_full": {"language_name": "Piemontese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pms", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22629916, "dataset_raw_mb": 20.54417067268712, "dataset_scaled_mb": 16.75298438, "dataset_tokens": 5307904, "train_batch_size": 8, "tokenizer_training_scaled_mb": 16.75298438, "tokenizer_training_raw_mb": 20.54417067268712, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.708422557696e+16, "train_compute_hours": 2.560690418185309, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 55.69647%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.63180%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 6.63122%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04052%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lug_latn_full": {"language_name": "Ganda", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.217417428, "dataset_raw_mb": 161.21605903250577, "dataset_scaled_mb": 132.4246354, "dataset_tokens": 37459968, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.74174280000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.91196786327552e+17, "train_compute_hours": 18.076787070968553, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 40.49929%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 33.20499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Makerere MT Corpus](https://zenodo.org/record/5089560#.Y00i3uxBw-S), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.27645%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.29489%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.85655%: [eBible](https://ebible.org/find/)\n* 0.49599%: [Makerere Radio Corpus](https://zenodo.org/record/5855017)\n* 0.37185%: [Makerere Parallel Corpus](https://zenodo.org/record/4764039)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "BLOOM", "CC100", "Wortschatz Leipzig Data", "Makerere MT Corpus", "TICO", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible", "Makerere Radio Corpus", "Makerere Parallel Corpus"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://wortschatz.uni-leipzig.de/en/download", "https://zenodo.org/record/5089560#.Y00i3uxBw-S", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://zenodo.org/record/5855017", "https://zenodo.org/record/4764039"]}, "wuu_hani_full": {"language_name": "Wu Chinese", "language_script": "Han", "dataset_category": "full", "language_iso6393": "wuu", "language_iso15924": "hani", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 17.169653997999998, "dataset_scaled_mb": 24.52807714, "dataset_tokens": 4112384, "train_batch_size": 8, "tokenizer_training_scaled_mb": 24.52807714, "tokenizer_training_raw_mb": 17.169653997999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.098697601024e+16, "train_compute_hours": 1.9842231864226911, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 62.28265%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 37.41809%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29927%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/"]}, "sun_latn_full": {"language_name": "Sundanese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.096980852, "dataset_raw_mb": 634.0155098163293, "dataset_scaled_mb": 577.9640626, "dataset_tokens": 142266368, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.6980852, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.26053214486528e+17, "train_compute_hours": 68.6450311878172, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.54837%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 32.02343%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 26.03343%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 2.55888%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.81131%: [IndoNLP](https://huggingface.co/indonlp)\n* 0.02459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "MC4", "OSCAR", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "IndoNLP", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/indonlp", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lij_latn_full": {"language_name": "Ligurian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lij", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.14375901, "dataset_raw_mb": 31.902135338017153, "dataset_scaled_mb": 27.89235762, "dataset_tokens": 8498176, "train_batch_size": 8, "tokenizer_training_scaled_mb": 27.89235762, "tokenizer_training_raw_mb": 31.902135338017153, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.3363503046656e+16, "train_compute_hours": 4.099822106229295, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 58.41627%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 21.25415%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 20.32958%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "apc_arab_full": {"language_name": "Levantine Arabic", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "apc", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.473448613, "dataset_raw_mb": 10.396692998998436, "dataset_scaled_mb": 7.056026866, "dataset_tokens": 1687040, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.056026866, "tokenizer_training_raw_mb": 10.396692998998436, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8606698242048000.0, "train_compute_hours": 0.8137241974299928, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.99983%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [ADD](https://github.com/drelhaj/ArabicDialects), [AraBench](https://alt.qcri.org/resources1/mt/arabench/), [DART](http://qufaculty.qu.edu.qa/telsayed/datasets/), [Habibi](http://ucrel-web.lancaster.ac.uk/habibi/), [QADI](https://alt.qcri.org/resources/qadi), [Shami](https://github.com/GU-CLASP/shami-corpus/tree/master/Data), [Tatoeba](https://tatoeba.org/en/)\n* 0.00017%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "ADD", "AraBench", "DART", "Habibi", "QADI", "Shami", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/drelhaj/ArabicDialects", "https://alt.qcri.org/resources1/mt/arabench/", "http://qufaculty.qu.edu.qa/telsayed/datasets/", "http://ucrel-web.lancaster.ac.uk/habibi/", "https://alt.qcri.org/resources/qadi", "https://github.com/GU-CLASP/shami-corpus/tree/master/Data", "https://tatoeba.org/en/"]}, "cak_latn_full": {"language_name": "Kaqchikel", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cak", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.82231949, "dataset_raw_mb": 15.502002360767888, "dataset_scaled_mb": 8.506742339, "dataset_tokens": 4157952, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.506742339, "tokenizer_training_raw_mb": 15.502002360767888, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "32127", "model_parameters": "110615040", "train_compute_flops": 2.1211948449792e+16, "train_compute_hours": 2.0054933079803345, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.21517%: [eBible](https://ebible.org/find/)\n* 39.78483%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/)\n", "dataset_names": ["eBible", "Glot500", "BLOOM", "Earthlings"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/"]}, "tam_latn_full": {"language_name": "Tamil", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.267529359, "dataset_raw_mb": 11.411485001324408, "dataset_scaled_mb": 9.002935451, "dataset_tokens": 2260992, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.002935451, "tokenizer_training_raw_mb": 11.411485001324408, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.153865613312e+16, "train_compute_hours": 1.0909274889495273, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "crh_cyrl_full": {"language_name": "Crimean Tatar", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "crh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.89497331, "dataset_raw_mb": 11.09620399719018, "dataset_scaled_mb": 5.855599094, "dataset_tokens": 1265664, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.855599094, "tokenizer_training_raw_mb": 11.09620399719018, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6459139031040000.0, "train_compute_hours": 0.6106822356619637, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 89.82288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.17712%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "diq_latn_full": {"language_name": "Dimli", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "diq", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["zza"], "language_code_individuals": [], "language_byte_premium": 0.9584231474, "dataset_raw_mb": 36.402077567975084, "dataset_scaled_mb": 37.98121703, "dataset_tokens": 9935872, "train_batch_size": 8, "tokenizer_training_scaled_mb": 37.98121703, "tokenizer_training_raw_mb": 36.402077567975084, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.0700583305216e+16, "train_compute_hours": 4.793509694311331, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.10048%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 30.14203%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 17.75749%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "meo_latn_full": {"language_name": "Kedah Malay", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "meo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.276200961, "dataset_raw_mb": 6.946411999458977, "dataset_scaled_mb": 5.443039311, "dataset_tokens": 1670656, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.443039311, "tokenizer_training_raw_mb": 6.946411999458977, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8523084791808000.0, "train_compute_hours": 0.8058189257709383, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ilo_latn_full": {"language_name": "Iloko", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ilo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.07650844, "dataset_raw_mb": 104.89965701308478, "dataset_scaled_mb": 97.44434239, "dataset_tokens": 25450496, "train_batch_size": 8, "tokenizer_training_scaled_mb": 97.44434239, "tokenizer_training_raw_mb": 104.89965701308478, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.29873636753408e+17, "train_compute_hours": 12.278962020322211, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 41.64934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.63745%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.20790%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 7.12547%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.86976%: [eBible](https://ebible.org/find/)\n* 0.51008%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kha_latn_full": {"language_name": "Khasi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kha", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.303114193, "dataset_raw_mb": 26.790172004064324, "dataset_scaled_mb": 20.55857587, "dataset_tokens": 6209536, "train_batch_size": 8, "tokenizer_training_scaled_mb": 20.55857587, "tokenizer_training_raw_mb": 26.790172004064324, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.168949764096e+16, "train_compute_hours": 2.996097958781673, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 99.86112%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.13888%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://tatoeba.org/en/"]}, "mhr_cyrl_full": {"language_name": "Eastern Mari", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "mhr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["chm"], "language_code_individuals": [], "language_byte_premium": 1.810192209, "dataset_raw_mb": 50.42615217249291, "dataset_scaled_mb": 27.85679439, "dataset_tokens": 6580224, "train_batch_size": 8, "tokenizer_training_scaled_mb": 27.85679439, "tokenizer_training_raw_mb": 50.42615217249291, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.3571845439488e+16, "train_compute_hours": 3.1740653870061384, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 85.57926%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 7.02298%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.37537%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.95016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.07223%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nap_latn_full": {"language_name": "Neapolitan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.23163545, "dataset_raw_mb": 8.290540459929792, "dataset_scaled_mb": 6.731326595, "dataset_tokens": 2123776, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.731326595, "tokenizer_training_raw_mb": 8.290540459929792, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.083839348736e+16, "train_compute_hours": 1.0247208388049456, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 62.19815%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.80185%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "smo_latn_full": {"language_name": "Samoan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "smo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.178001518, "dataset_raw_mb": 370.9924411060397, "dataset_scaled_mb": 314.9337547, "dataset_tokens": 101910016, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.80015180000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.20082715377664e+17, "train_compute_hours": 49.17145672661551, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 43.50048%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.24768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [MC4](https://huggingface.co/datasets/allenai/c4), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.13951%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.11208%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00025%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "MC4", "Tatoeba", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/allenai/c4", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "krc_cyrl_full": {"language_name": "Karachay-Balkar", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "krc", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.865302377, "dataset_raw_mb": 39.217601003435874, "dataset_scaled_mb": 21.02479549, "dataset_tokens": 4627456, "train_batch_size": 8, "tokenizer_training_scaled_mb": 21.02479549, "tokenizer_training_raw_mb": 39.217601003435874, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.3609825427456e+16, "train_compute_hours": 2.2322016767776582, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.46882%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.41364%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 11.40588%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.24320%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.46452%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00394%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nzi_latn_full": {"language_name": "Nzima", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nzi", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.418370521, "dataset_raw_mb": 7.1919050019505075, "dataset_scaled_mb": 5.070540381, "dataset_tokens": 1514496, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.070540381, "tokenizer_training_raw_mb": 7.1919050019505075, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "22996", "model_parameters": "104323584", "train_compute_flops": 7724315049984000.0, "train_compute_hours": 0.7302988774530328, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ban_latn_full": {"language_name": "Balinese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ban", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.269543636, "dataset_raw_mb": 40.419334274221626, "dataset_scaled_mb": 31.83768807, "dataset_tokens": 9161216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 31.83768807, "tokenizer_training_raw_mb": 40.419334274221626, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 4.6745405816832e+16, "train_compute_hours": 4.419565640864117, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 70.01948%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.06831%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 11.61178%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md)\n* 0.30043%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/indonlp"]}, "gla_latn_full": {"language_name": "Scottish Gaelic", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "gla", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9936126744, "dataset_raw_mb": 555.2695050026751, "dataset_scaled_mb": 558.8389916, "dataset_tokens": 123736064, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.36126744, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.31486402265088e+17, "train_compute_hours": 59.70416894142651, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.49478%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 21.15694%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.70049%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 0.49333%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.15447%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "syr_syrc_full": {"language_name": "Syriac", "language_script": "Syriac", "dataset_category": "full", "language_iso6393": "syr", "language_iso15924": "syrc", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aii", "cld"], "language_byte_premium": 1.410191627, "dataset_raw_mb": 17.155997002197342, "dataset_scaled_mb": 12.16572037, "dataset_tokens": 2641408, "train_batch_size": 8, "tokenizer_training_scaled_mb": 12.16572037, "tokenizer_training_raw_mb": 17.155997002197342, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.34761365504e+16, "train_compute_hours": 1.2741074556741818, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "yua_latn_full": {"language_name": "Yucateco", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "yua", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.241346656, "dataset_raw_mb": 14.158160999971756, "dataset_scaled_mb": 11.40548527, "dataset_tokens": 3645440, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.40548527, "tokenizer_training_raw_mb": 14.158160999971756, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.86039926784e+16, "train_compute_hours": 1.7589229441396366, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "chm_cyrl_full": {"language_name": "Mari (Russia)", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "chm", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["mhr", "mrj"], "language_byte_premium": 1.756255296, "dataset_raw_mb": 86.81655499348494, "dataset_scaled_mb": 49.43276481, "dataset_tokens": 11290624, "train_batch_size": 8, "tokenizer_training_scaled_mb": 49.43276481, "tokenizer_training_raw_mb": 86.81655499348494, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.7610712383488e+16, "train_compute_hours": 5.4468309889843205, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 73.38471%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 12.77683%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.02224%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.09681%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.65747%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.06194%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Languages of Russia", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "khm_khmr_full": {"language_name": "Central Khmer", "language_script": "Khmer", "dataset_category": "full", "language_iso6393": "khm", "language_iso15924": "khmr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.903484241, "dataset_raw_mb": 2897.847870542993, "dataset_scaled_mb": 742.3746816, "dataset_tokens": 235559424, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 390.3484241, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.202177987444736e+18, "train_compute_hours": 113.66046426750232, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 55.59600%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.62172%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.83408%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.36741%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/)\n* 1.58079%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "Earthlings", "OSCAR", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "cnh_latn_full": {"language_name": "Hakha Chin", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cnh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.323849924, "dataset_raw_mb": 43.94965299541276, "dataset_scaled_mb": 33.19836501, "dataset_tokens": 10364928, "train_batch_size": 8, "tokenizer_training_scaled_mb": 33.19836501, "tokenizer_training_raw_mb": 43.94965299541276, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.2886552444928e+16, "train_compute_hours": 5.000183140247739, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "sat_olck_full": {"language_name": "Santali", "language_script": "Ol Chiki", "dataset_category": "full", "language_iso6393": "sat", "language_iso15924": "olck", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.803663545, "dataset_raw_mb": 23.789809929155048, "dataset_scaled_mb": 8.485258501, "dataset_tokens": 2224128, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.485258501, "tokenizer_training_raw_mb": 23.789809929155048, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.135052587008e+16, "train_compute_hours": 1.0731406277166546, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 52.13518%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 41.86472%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.00009%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "BLOOM", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb"]}, "tiv_latn_full": {"language_name": "Tiv", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tiv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.309214276, "dataset_raw_mb": 8.278914999206341, "dataset_scaled_mb": 6.323575255, "dataset_tokens": 2119168, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.323575255, "tokenizer_training_raw_mb": 8.278914999206341, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "18144", "model_parameters": "99604992", "train_compute_flops": 1.0812002992128e+16, "train_compute_hours": 1.0222257374375565, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "bug_latn_full": {"language_name": "Buginese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bug", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.22786025, "dataset_raw_mb": 13.163489175400642, "dataset_scaled_mb": 10.72067377, "dataset_tokens": 3269632, "train_batch_size": 8, "tokenizer_training_scaled_mb": 10.72067377, "tokenizer_training_raw_mb": 13.163489175400642, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.6670954225664e+16, "train_compute_hours": 1.5761629449718693, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 93.76215%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.23785%: [IndoNLP](https://huggingface.co/indonlp)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "IndoNLP"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/indonlp"]}, "gle_latn_full": {"language_name": "Irish", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "gle", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.975875656, "dataset_raw_mb": 1929.847526488957, "dataset_scaled_mb": 976.7049463, "dataset_tokens": 404823040, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 197.5875656, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.065991154794496e+18, "train_compute_hours": 195.33007281693418, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.77543%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.32257%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.85422%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.21381%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.83397%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "nnb_latn_full": {"language_name": "Nande", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nnb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.307887294, "dataset_raw_mb": 8.489565003116374, "dataset_scaled_mb": 6.491052434, "dataset_tokens": 1764352, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.491052434, "tokenizer_training_raw_mb": 8.489565003116374, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8999420166144000.0, "train_compute_hours": 0.8508542702536146, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 94.01204%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "que_latn_full": {"language_name": "Quechua", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "que", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["qub", "qud", "quf", "qug", "quh", "quk", "qul", "qup", "qur", "qus", "quw", "qux", "quy", "quz", "qva", "qvc", "qve", "qvh", "qvi", "qvj", "qvl", "qvm", "qvn", "qvo", "qvp", "qvs", "qvw", "qvz", "qwa", "qwc", "qwh", "qws", "qxa", "qxc", "qxh", "qxl", "qxn", "qxo", "qxp", "qxr", "qxt", "qxu", "qxw"], "language_byte_premium": 1.21478134, "dataset_raw_mb": 169.32224301813037, "dataset_scaled_mb": 139.3849555, "dataset_tokens": 40595968, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 121.47813400000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.07152584261632e+17, "train_compute_hours": 19.585335239281573, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "Llamacha/monolingual-quechua-iic", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.54127%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 16.31458%: [AmericasNLP (excluding AmericasNLI)](https://turing.iimas.unam.mx/americasnlp/)\n* 7.98999%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [Quechua-IIC](https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.28328%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.76909%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09735%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00445%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "AmericasNLP (excluding AmericasNLI)", "Glot500", "BLOOM", "CC100", "Earthlings", "OSCAR", "Quechua-IIC", "Tatoeba", "W2C", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://turing.iimas.unam.mx/americasnlp/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://huggingface.co/datasets/Llamacha/monolingual-quechua-iic", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "new_deva_full": {"language_name": "Newari", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "new", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.561603163, "dataset_raw_mb": 31.86245298976429, "dataset_scaled_mb": 12.43848128, "dataset_tokens": 2927616, "train_batch_size": 8, "tokenizer_training_scaled_mb": 12.43848128, "tokenizer_training_raw_mb": 31.86245298976429, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4934929965056e+16, "train_compute_hours": 1.4120297421507493, "dataset_hugging_face": ["cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 47.70102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 24.62528%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 20.39344%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.65085%: [eBible](https://ebible.org/find/)\n* 3.62941%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "BLOOM", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "eBible", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bod_tibt_full": {"language_name": "Tibetan", "language_script": "Tibetan", "dataset_category": "full", "language_iso6393": "bod", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.616914857, "dataset_raw_mb": 345.275151931216, "dataset_scaled_mb": 131.9397729, "dataset_tokens": 23463424, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 261.6914857, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.19741515628544e+17, "train_compute_hours": 11.321016023062343, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 35.45938%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 27.32632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 23.94564%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.69915%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.63626%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.93326%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "BLOOM", "Earthlings", "Wortschatz Leipzig Data", "MoT", "OSCAR", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "pap_latn_full": {"language_name": "Papiamento", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pap", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.001675271, "dataset_raw_mb": 255.93321691254036, "dataset_scaled_mb": 255.5051765, "dataset_tokens": 60037632, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 100.16752710000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.06427095023616e+17, "train_compute_hours": 28.971288984050968, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 68.24074%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.46139%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.28720%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.01066%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "war_latn_full": {"language_name": "Waray", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "war", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.093149717, "dataset_raw_mb": 191.57923195539183, "dataset_scaled_mb": 175.2543398, "dataset_tokens": 48998912, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.3149717, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.50109255614464e+17, "train_compute_hours": 23.646693258094782, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.84724%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 25.81890%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.17011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.16375%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ori_orya_full": {"language_name": "Odia", "language_script": "Oriya", "dataset_category": "full", "language_iso6393": "ori", "language_iso15924": "orya", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ory", "spv"], "language_byte_premium": 2.595114712, "dataset_raw_mb": 2010.0556683675431, "dataset_scaled_mb": 774.553687, "dataset_tokens": 165528576, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 259.5114712, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.44751391031296e+17, "train_compute_hours": 79.8674042429589, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.28209%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 26.83885%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 18.83892%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.35162%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 1.43853%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.24999%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Workshop on NER for South and South East Asian Languages", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kab_latn_full": {"language_name": "Kabyle", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kab", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.028599141, "dataset_raw_mb": 46.485058790695575, "dataset_scaled_mb": 45.19258955, "dataset_tokens": 14035456, "train_batch_size": 8, "tokenizer_training_scaled_mb": 45.19258955, "tokenizer_training_raw_mb": 46.485058790695575, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.1620407263232e+16, "train_compute_hours": 6.771383959432844, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 73.42841%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 20.37499%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.19660%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "sme_latn_full": {"language_name": "Northern Sami", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sme", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.267947826, "dataset_raw_mb": 84.4986890147659, "dataset_scaled_mb": 66.64208675, "dataset_tokens": 15802880, "train_batch_size": 8, "tokenizer_training_scaled_mb": 66.64208675, "tokenizer_training_raw_mb": 84.4986890147659, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.063080169472e+16, "train_compute_hours": 7.6232757965917095, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 75.06677%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 20.99484%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.97487%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 1.96230%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00123%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nhe_latn_full": {"language_name": "Eastern Huasteca Nahuatl", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nhe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.492247034, "dataset_raw_mb": 7.491709000285515, "dataset_scaled_mb": 5.020421438, "dataset_tokens": 1268224, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.020421438, "tokenizer_training_raw_mb": 7.491709000285515, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "42595", "model_parameters": "118479360", "train_compute_flops": 6464626163712000.0, "train_compute_hours": 0.6112010191145891, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 56.31133%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 43.68867%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "inh_cyrl_full": {"language_name": "Ingush", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "inh", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.700418497, "dataset_raw_mb": 16.300086745085817, "dataset_scaled_mb": 9.585926508, "dataset_tokens": 2764800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.585926508, "tokenizer_training_raw_mb": 16.300086745085817, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4109769728e+16, "train_compute_hours": 1.3340145924654547, "dataset_hugging_face": [], "dataset_readme_str": "* 89.48455%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 10.51545%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Languages of Russia", "Wikipedia 2023/08"], "dataset_links": ["http://web-corpora.net/wsgi3/minorlangs/download", "https://dumps.wikimedia.org/"]}, "asm_beng_full": {"language_name": "Assamese", "language_script": "Bengali", "dataset_category": "full", "language_iso6393": "asm", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.526329303, "dataset_raw_mb": 881.37819506382, "dataset_scaled_mb": 348.8770027, "dataset_tokens": 77216256, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 252.63293029999997, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.94054774751232e+17, "train_compute_hours": 37.25608779466194, "dataset_hugging_face": ["cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.77996%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CC100](https://huggingface.co/datasets/statmt/cc100), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 25.83555%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.99529%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.32777%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.47875%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.58268%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CC100", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://huggingface.co/datasets/statmt/cc100", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "wol_latn_full": {"language_name": "Wolof", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "wol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.078647211, "dataset_raw_mb": 40.25085500544504, "dataset_scaled_mb": 37.31605162, "dataset_tokens": 12005888, "train_batch_size": 8, "tokenizer_training_scaled_mb": 37.31605162, "tokenizer_training_raw_mb": 40.25085500544504, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.12533846016e+16, "train_compute_hours": 5.791229089605818, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 63.24103%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 17.08456%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CC100](https://huggingface.co/datasets/statmt/cc100), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 8.36785%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 6.80354%: [eBible](https://ebible.org/find/)\n* 4.50302%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "Glot500", "CC100", "Earthlings", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/statmt/cc100", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/"]}, "bew_latn_full": {"language_name": "Betawi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bew", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.300925888, "dataset_raw_mb": 15.400584993804127, "dataset_scaled_mb": 11.83817244, "dataset_tokens": 3186176, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.83817244, "tokenizer_training_raw_mb": 15.400584993804127, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.625628377088e+16, "train_compute_hours": 1.5369577383377455, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 51.56687%: [IndoNLP](https://huggingface.co/indonlp)\n* 46.73817%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.69496%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["IndoNLP", "MADLAD-400 (CommonCrawl)", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/indonlp", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "iso_latn_full": {"language_name": "Isoko", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "iso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.483217547, "dataset_raw_mb": 12.028280003441777, "dataset_scaled_mb": 8.109585831, "dataset_tokens": 2638336, "train_batch_size": 4, "tokenizer_training_scaled_mb": 8.109585831, "tokenizer_training_raw_mb": 12.028280003441777, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "29138", "model_parameters": "109042176", "train_compute_flops": 1.3456800940032e+16, "train_compute_hours": 1.2722793616030255, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "gom_deva_full": {"language_name": "Goan Konkani", "language_script": "Devanagari", "dataset_category": "full", "language_iso6393": "gom", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": ["kok"], "language_code_individuals": [], "language_byte_premium": 1.736906273, "dataset_raw_mb": 19.970069000784232, "dataset_scaled_mb": 11.49749374, "dataset_tokens": 2219520, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.49749374, "tokenizer_training_raw_mb": 19.970069000784232, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.132309020672e+16, "train_compute_hours": 1.0705467104535273, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 34.12288%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 32.53101%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.87562%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 3.47037%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "sna_latn_full": {"language_name": "Shona", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "sna", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.119177764, "dataset_raw_mb": 680.587553180553, "dataset_scaled_mb": 608.1138985, "dataset_tokens": 151712256, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.91777640000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.74219003789312e+17, "train_compute_hours": 73.1988876309895, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 45.75185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 27.93918%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.33727%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.07291%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.53601%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.36278%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kur_arab_full": {"language_name": "Kurdish", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "kur", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ckb", "kmr", "sdh"], "language_byte_premium": 1.569891339, "dataset_raw_mb": 1416.651357585092, "dataset_scaled_mb": 902.3881605, "dataset_tokens": 196483584, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 156.9891339, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.002759908622336e+18, "train_compute_hours": 94.80639136065723, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 43.19057%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.23880%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Bianet](https://opus.nlpl.eu/Bianet.php), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.93282%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 15.05661%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.32587%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.25533%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Bianet", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "Tatoeba", "TICO", "W2C", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://opus.nlpl.eu/Bianet.php", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nno_latn_full": {"language_name": "Norwegian Nynorsk", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nno", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 1.033289113, "dataset_raw_mb": 515.5439190492193, "dataset_scaled_mb": 498.9348214, "dataset_tokens": 116016128, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.32891129999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.92071544406016e+17, "train_compute_hours": 55.977673289296064, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 70.90768%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 21.53541%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.55180%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00511%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mkw_cyrl_full": {"language_name": "Kituba", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "mkw", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.808563089, "dataset_raw_mb": 16.946573755636074, "dataset_scaled_mb": 9.370186674, "dataset_tokens": 2266112, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.370186674, "tokenizer_training_raw_mb": 16.946573755636074, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1560082079744e+16, "train_compute_hours": 1.0929532148121601, "dataset_hugging_face": [], "dataset_readme_str": "* 100.00000%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Wortschatz Leipzig Data"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download"]}, "luo_latn_full": {"language_name": "Luo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "luo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035745154, "dataset_raw_mb": 35.729216827985354, "dataset_scaled_mb": 34.49614675, "dataset_tokens": 9859072, "train_batch_size": 8, "tokenizer_training_scaled_mb": 34.49614675, "tokenizer_training_raw_mb": 35.729216827985354, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.031439368192e+16, "train_compute_hours": 4.756997220836073, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 88.95975%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.45137%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://ebible.org/find/"]}, "yue_hant_full": {"language_name": "Yue Chinese", "language_script": "Han Traditional", "dataset_category": "full", "language_iso6393": "yue", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.862461402, "dataset_raw_mb": 67.63317599595338, "dataset_scaled_mb": 78.41878586, "dataset_tokens": 16084992, "train_batch_size": 8, "tokenizer_training_scaled_mb": 78.41878586, "tokenizer_training_raw_mb": 67.63317599595338, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.208750477312e+16, "train_compute_hours": 7.761000451276801, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 52.60663%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 47.24904%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.14433%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "scn_latn_full": {"language_name": "Sicilian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "scn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.036272745, "dataset_raw_mb": 117.93086782074273, "dataset_scaled_mb": 113.8029234, "dataset_tokens": 32010752, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.6272745, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.63339136335872e+17, "train_compute_hours": 15.442972889936991, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.07639%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.21878%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.70271%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00212%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dyu_latn_full": {"language_name": "Dyula", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "dyu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.15445855, "dataset_raw_mb": 13.787629841633787, "dataset_scaled_mb": 11.94294056, "dataset_tokens": 3849216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.94294056, "tokenizer_training_raw_mb": 13.787629841633787, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.9638186541056e+16, "train_compute_hours": 1.8567012729725674, "dataset_hugging_face": ["breakend/nllb-multi-domain", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 68.41765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb) and [NLLB Multi-Domain](https://huggingface.co/datasets/breakend/nllb-multi-domain)\n* 31.58235%: [eBible](https://ebible.org/find/)\n* 0.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "NLLB Multi-Domain", "eBible", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/breakend/nllb-multi-domain", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lzh_hant_full": {"language_name": "Literary Chinese", "language_script": "Han Traditional", "dataset_category": "full", "language_iso6393": "lzh", "language_iso15924": "hant", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 0.7, "dataset_raw_mb": 10.642293998, "dataset_scaled_mb": 15.20327714, "dataset_tokens": 2767872, "train_batch_size": 8, "tokenizer_training_scaled_mb": 15.20327714, "tokenizer_training_raw_mb": 10.642293998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.4119698825216e+16, "train_compute_hours": 1.3349533434749674, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 53.82278%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 46.05529%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.12193%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Wikipedia 2023/08", "Glot500", "Tatoeba", "Wikipedia Hugging Face"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "pam_latn_full": {"language_name": "Pampanga", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "pam", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.18679287, "dataset_raw_mb": 45.2681856428521, "dataset_scaled_mb": 38.14329087, "dataset_tokens": 11270656, "train_batch_size": 8, "tokenizer_training_scaled_mb": 38.14329087, "tokenizer_training_raw_mb": 45.2681856428521, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.7510637535232e+16, "train_compute_hours": 5.437369366967389, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 91.78710%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 5.56188%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.58011%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 0.06645%: [Tatoeba](https://tatoeba.org/en/)\n* 0.00446%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tsn_latn_full": {"language_name": "Tswana", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tsn", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173940296, "dataset_raw_mb": 119.5667309470279, "dataset_scaled_mb": 101.8507767, "dataset_tokens": 31488512, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.39402960000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.60689635131392e+17, "train_compute_hours": 15.1924745942407, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "sil-ai/bloom-lm", "statmt/cc100", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 54.15070%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.72794%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CC100](https://huggingface.co/datasets/statmt/cc100), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 18.23011%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 1.14046%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.74997%: [eBible](https://ebible.org/find/)\n* 0.00083%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CC100", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://huggingface.co/datasets/statmt/cc100", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "fuv_latn_full": {"language_name": "Nigerian Fulfulde", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "fuv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["ful"], "language_code_individuals": [], "language_byte_premium": 1.110831498, "dataset_raw_mb": 23.579617841974912, "dataset_scaled_mb": 21.22699787, "dataset_tokens": 6159872, "train_batch_size": 8, "tokenizer_training_scaled_mb": 21.22699787, "tokenizer_training_raw_mb": 23.579617841974912, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.143212498944e+16, "train_compute_hours": 2.971764544456146, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "zap_latn_full": {"language_name": "Zapotec", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "zap", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["zaa", "zab", "zac", "zad", "zae", "zaf", "zai", "zam", "zao", "zaq", "zar", "zas", "zat", "zav", "zaw", "zax", "zca", "zcd", "zoo", "zpa", "zpb", "zpc", "zpd", "zpe", "zpf", "zpg", "zph", "zpi", "zpj", "zpk", "zpl", "zpm", "zpn", "zpo", "zpp", "zpq", "zpr", "zps", "zpt", "zpu", "zpv", "zpw", "zpx", "zpy", "zpz", "zsr", "zte", "ztg", "ztl", "ztm", "ztn", "ztp", "ztq", "zts", "ztt", "ztu", "ztx", "zty"], "language_byte_premium": 1.075680556, "dataset_raw_mb": 10.303848999865751, "dataset_scaled_mb": 9.578911641, "dataset_tokens": 2395136, "train_batch_size": 4, "tokenizer_training_scaled_mb": 9.578911641, "tokenizer_training_raw_mb": 10.303848999865751, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "49237", "model_parameters": "124770816", "train_compute_flops": 1.2218538000384e+16, "train_compute_hours": 1.1552072291272146, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 91.84155%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 8.15845%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/"]}, "dzo_tibt_full": {"language_name": "Dzongkha", "language_script": "Tibetan", "dataset_category": "full", "language_iso6393": "dzo", "language_iso15924": "tibt", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 3.261842241, "dataset_raw_mb": 25.10073899747828, "dataset_scaled_mb": 7.69526456, "dataset_tokens": 2019328, "train_batch_size": 4, "tokenizer_training_scaled_mb": 7.69526456, "tokenizer_training_raw_mb": 25.10073899747828, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.030535774208e+16, "train_compute_hours": 0.9743247319784728, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 74.77296%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.41761%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 9.58626%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.22317%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/nllb"]}, "ibo_latn_full": {"language_name": "Igbo", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ibo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.345020968, "dataset_raw_mb": 522.2893158534258, "dataset_scaled_mb": 388.3131403, "dataset_tokens": 119706112, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.5020968, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.10920106426368e+17, "train_compute_hours": 57.75971915303843, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "allenai/MADLAD-400", "allenai/nllb"], "dataset_readme_str": "* 31.78330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 31.30256%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 26.52382%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.93791%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 4.92177%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.53064%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "Wortschatz Leipzig Data", "MC4", "Wikipedia Hugging Face", "XLSum", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "oci_latn_full": {"language_name": "Occitan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "oci", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.014733462, "dataset_raw_mb": 380.91526390133873, "dataset_scaled_mb": 375.3845499, "dataset_tokens": 99783680, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.4733462, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.09257125199872e+17, "train_compute_hours": 48.147946382533355, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 33.78812%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.07771%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 25.38373%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.02047%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.72997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "xmf_geor_full": {"language_name": "Mingrelian", "language_script": "Georgian", "dataset_category": "full", "language_iso6393": "xmf", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.509423461, "dataset_raw_mb": 13.64831070895189, "dataset_scaled_mb": 5.438823268, "dataset_tokens": 1367040, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.438823268, "tokenizer_training_raw_mb": 13.64831070895189, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6971793997824000.0, "train_compute_hours": 0.6591514325215418, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.93889%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 37.36833%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 13.69277%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "crh_latn_full": {"language_name": "Crimean Tatar", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "crh", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.312089034, "dataset_raw_mb": 61.92819658458904, "dataset_scaled_mb": 47.19816642, "dataset_tokens": 12994560, "train_batch_size": 8, "tokenizer_training_scaled_mb": 47.19816642, "tokenizer_training_raw_mb": 61.92819658458904, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.6306511208448e+16, "train_compute_hours": 6.268979241525993, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 86.04959%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.27361%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 6.67679%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "NLLB_seed", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "san_latn_full": {"language_name": "Sanskrit", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "san", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cls", "vsn"], "language_byte_premium": 0.966275057, "dataset_raw_mb": 5.531503419894275, "dataset_scaled_mb": 5.724564015, "dataset_tokens": 1164800, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.724564015, "tokenizer_training_raw_mb": 5.531503419894275, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5941519515648000.0, "train_compute_hours": 0.5617436632976291, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 98.91275%: [eBible](https://ebible.org/find/)\n* 1.08725%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["eBible", "Glot500", "CCNet", "Wortschatz Leipzig Data"], "dataset_links": ["https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download"]}, "ceb_latn_full": {"language_name": "Cebuano", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ceb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.113380333, "dataset_raw_mb": 601.4572919331524, "dataset_scaled_mb": 540.2082955, "dataset_tokens": 140301312, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.3380333, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7.16033187643392e+17, "train_compute_hours": 67.69768319537525, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 36.46583%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 33.35484%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.15251%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 4.56324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.46359%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "mon_latn_full": {"language_name": "Mongolian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mon", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.181843271, "dataset_raw_mb": 58.15538167410059, "dataset_scaled_mb": 49.20735524, "dataset_tokens": 12692480, "train_batch_size": 8, "tokenizer_training_scaled_mb": 49.20735524, "tokenizer_training_raw_mb": 58.15538167410059, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 6.4768546308096e+16, "train_compute_hours": 6.123571650947259, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "bre_latn_full": {"language_name": "Breton", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "bre", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01282485, "dataset_raw_mb": 165.2034709660341, "dataset_scaled_mb": 163.1115893, "dataset_tokens": 43437056, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.28248500000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.21683295453184e+17, "train_compute_hours": 20.95914793375558, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.39389%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.93949%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.16716%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.31819%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.13530%: [eBible](https://ebible.org/find/)\n* 0.04596%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "kmb_latn_full": {"language_name": "Kimbundu", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kmb", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.132798568, "dataset_raw_mb": 12.5633449983503, "dataset_scaled_mb": 11.0905375, "dataset_tokens": 3359744, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.0905375, "tokenizer_training_raw_mb": 12.5633449983503, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.7130828201984e+16, "train_compute_hours": 1.6196419390966692, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400"], "dataset_readme_str": "* 68.73144%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 31.26856%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kin_latn_full": {"language_name": "Kinyarwanda", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "kin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.133982717, "dataset_raw_mb": 919.6194702534611, "dataset_scaled_mb": 810.9642735, "dataset_tokens": 193561088, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 113.39827170000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.87811652763648e+17, "train_compute_hours": 93.39310171583583, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 61.08100%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 27.90107%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.87658%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [KinyaSMT](https://github.com/pniyongabo/kinyarwandaSMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MoT](https://github.com/bltlab/mot), [TICO](https://tico-19.github.io/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.34254%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.79880%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "BLOOM", "KinyaSMT", "Wortschatz Leipzig Data", "MoT", "TICO", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/pniyongabo/kinyarwandaSMT", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/bltlab/mot", "https://tico-19.github.io/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "alt_cyrl_full": {"language_name": "Southern Altai", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "alt", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.856580455, "dataset_raw_mb": 21.631481002646055, "dataset_scaled_mb": 11.65124891, "dataset_tokens": 2694144, "train_batch_size": 8, "tokenizer_training_scaled_mb": 11.65124891, "tokenizer_training_raw_mb": 21.631481002646055, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.3743438299136e+16, "train_compute_hours": 1.2993796210092219, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 58.22529%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 34.73797%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "mal_latn_full": {"language_name": "Malayalam", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "mal", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.265435969, "dataset_raw_mb": 8.072882002176662, "dataset_scaled_mb": 6.379526266, "dataset_tokens": 1556480, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.379526266, "tokenizer_training_raw_mb": 8.072882002176662, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 7943277772800000.0, "train_compute_hours": 0.7510008076101818, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "aka_latn_full": {"language_name": "Akan", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "aka", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["fat", "twi"], "language_byte_premium": 1.574942782, "dataset_raw_mb": 77.96816701371849, "dataset_scaled_mb": 49.50539658, "dataset_tokens": 22551040, "train_batch_size": 8, "tokenizer_training_scaled_mb": 49.50539658, "tokenizer_training_raw_mb": 77.96816701371849, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.15078498025472e+17, "train_compute_hours": 10.880148904226445, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 55.43728%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 24.29722%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 10.29716%: [eBible](https://ebible.org/find/)\n* 5.04766%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.84113%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Akuapem](https://zenodo.org/record/4432117#.Y00gXOxBw-Q), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.07955%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "eBible", "Wikipedia 2023/08", "Glot500", "Akuapem", "BLOOM", "Wortschatz Leipzig Data", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/4432117#.Y00gXOxBw-Q", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "ton_latn_full": {"language_name": "Tonga (Tonga Islands)", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ton", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.270386963, "dataset_raw_mb": 22.180197000172868, "dataset_scaled_mb": 17.45940225, "dataset_tokens": 6237184, "train_batch_size": 8, "tokenizer_training_scaled_mb": 17.45940225, "tokenizer_training_raw_mb": 22.180197000172868, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.1824846913536e+16, "train_compute_hours": 3.0088946172797675, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 83.09550%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.64313%: [eBible](https://ebible.org/find/)\n* 1.76795%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 1.49248%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00095%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "eBible", "Glot500", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://ebible.org/find/", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "pbt_arab_full": {"language_name": "Southern Pashto", "language_script": "Arabic", "dataset_category": "full", "language_iso6393": "pbt", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["pus"], "language_code_individuals": [], "language_byte_premium": 1.736020956, "dataset_raw_mb": 390.7873254083407, "dataset_scaled_mb": 225.105189, "dataset_tokens": 60608000, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 173.60209559999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.09321165570048e+17, "train_compute_hours": 29.244910199349995, "dataset_hugging_face": ["allenai/nllb"], "dataset_readme_str": "* 100.00000%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb"]}, "uzb_cyrl_full": {"language_name": "Uzbek", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "uzb", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.980686782, "dataset_raw_mb": 1040.872067975403, "dataset_scaled_mb": 525.5106852, "dataset_tokens": 110868992, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 198.0686782, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.65796801544192e+17, "train_compute_hours": 53.49351578235998, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "TIL", "W2C"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9"]}, "kbd_cyrl_full": {"language_name": "Kabardian", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "kbd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.783468787, "dataset_raw_mb": 62.23185300958343, "dataset_scaled_mb": 34.89371581, "dataset_tokens": 9802752, "train_batch_size": 8, "tokenizer_training_scaled_mb": 34.89371581, "tokenizer_training_raw_mb": 62.23185300958343, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 5.0011817508864e+16, "train_compute_hours": 4.728390019019869, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 59.32058%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.38457%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 3.04549%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 2.24936%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "nan_latn_full": {"language_name": "Min Nan Chinese", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "nan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["chi", "zho"], "language_code_individuals": [], "language_byte_premium": 1.14871192, "dataset_raw_mb": 50.97702901653165, "dataset_scaled_mb": 44.37755727, "dataset_tokens": 16624128, "train_batch_size": 8, "tokenizer_training_scaled_mb": 44.37755727, "tokenizer_training_raw_mb": 50.97702901653165, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8.4831332401152e+16, "train_compute_hours": 8.020416881563463, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/MADLAD-400"], "dataset_readme_str": "* 49.98250%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 42.06803%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 7.94947%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "Wikipedia Hugging Face", "Wikipedia 2023/08", "MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/allenai/MADLAD-400"]}, "hin_latn_full": {"language_name": "Hindi", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "hin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.255306654, "dataset_raw_mb": 165.5281900003518, "dataset_scaled_mb": 131.862752, "dataset_tokens": 37683712, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 125.5306654, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.92313287180288e+17, "train_compute_hours": 18.182347151590868, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500"], "dataset_readme_str": "* 74.45561%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 25.54439%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [IITB](https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "IITB", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://www.cfilt.iitb.ac.in/~moses/iitb_en_hi_parallel/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "myv_cyrl_full": {"language_name": "Erzya", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "myv", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.773695539, "dataset_raw_mb": 29.81217399776407, "dataset_scaled_mb": 16.80794327, "dataset_tokens": 3851776, "train_batch_size": 8, "tokenizer_training_scaled_mb": 16.80794327, "tokenizer_training_raw_mb": 29.81217399776407, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.9643673673728e+16, "train_compute_hours": 1.8572200564251928, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 40.71457%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 32.46832%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n* 13.56506%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 13.24591%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00459%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.00155%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Languages of Russia", "Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "http://web-corpora.net/wsgi3/minorlangs/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ido_latn_full": {"language_name": "Ido", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ido", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.17576321, "dataset_raw_mb": 30.78313939289929, "dataset_scaled_mb": 26.18141062, "dataset_tokens": 7369216, "train_batch_size": 8, "tokenizer_training_scaled_mb": 26.18141062, "tokenizer_training_raw_mb": 30.78313939289929, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.759077818368e+16, "train_compute_hours": 3.554037210093382, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 59.56311%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 39.86649%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.29534%: [Tatoeba](https://tatoeba.org/en/)\n* 0.27506%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tso_latn_full": {"language_name": "Tsonga", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "tso", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.207440889, "dataset_raw_mb": 86.75316499964093, "dataset_scaled_mb": 71.84878845, "dataset_tokens": 21684224, "train_batch_size": 8, "tokenizer_training_scaled_mb": 71.84878845, "tokenizer_training_raw_mb": 86.75316499964093, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1066240139264e+17, "train_compute_hours": 10.462627040758692, "dataset_hugging_face": ["allenai/nllb", "allenai/MADLAD-400", "cis-lmu/Glot500", "sil-ai/bloom-lm", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 57.07934%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 29.98123%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 12.39404%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AUTSHUMATO](https://autshumato.sourceforge.net/), [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CORP.NCHLT](https://repo.sadilar.org/handle/20.500.12185/7), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [Tatoeba](https://tatoeba.org/en/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 0.54534%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00004%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "MADLAD-400 (CommonCrawl)", "Glot500", "AUTSHUMATO", "BLOOM", "CORP.NCHLT", "Wortschatz Leipzig Data", "Mburisano_Covid", "Tatoeba", "Wikipedia Hugging Face", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://autshumato.sourceforge.net/", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://repo.sadilar.org/handle/20.500.12185/7", "https://wortschatz.uni-leipzig.de/en/download", "https://repo.sadilar.org/handle/20.500.12185/536", "https://tatoeba.org/en/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/"]}, "bew_cyrl_full": {"language_name": "Betawi", "language_script": "Cyrillic", "dataset_category": "full", "language_iso6393": "bew", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.740538562, "dataset_raw_mb": 40.93866302412365, "dataset_scaled_mb": 23.52068717, "dataset_tokens": 5288960, "train_batch_size": 8, "tokenizer_training_scaled_mb": 23.52068717, "tokenizer_training_raw_mb": 40.93866302412365, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 2.6976311967744e+16, "train_compute_hours": 2.550487676950342, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 52.29436%: [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n* 47.70564%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download)\n", "dataset_names": ["Wortschatz Leipzig Data", "Glot500", "Wortschatz Leipzig Data"], "dataset_links": ["https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download"]}, "lmo_latn_full": {"language_name": "Lombard", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "lmo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9431924587, "dataset_raw_mb": 116.23843958865749, "dataset_scaled_mb": 123.2393649, "dataset_tokens": 35603456, "train_batch_size": 32, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 94.31924587, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.81696730628096e+17, "train_compute_hours": 17.17859998665635, "dataset_hugging_face": ["allenai/nllb", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 44.48817%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 37.05952%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.79259%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.65971%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "Glot500", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "vol_latn_full": {"language_name": "Volap\u00fck", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "vol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.129514541, "dataset_raw_mb": 24.165513414084398, "dataset_scaled_mb": 21.39460143, "dataset_tokens": 6030336, "train_batch_size": 8, "tokenizer_training_scaled_mb": 21.39460143, "tokenizer_training_raw_mb": 24.165513414084398, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.0759820591104e+16, "train_compute_hours": 2.90820121952256, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.27628%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 22.97146%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 4.60410%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.14817%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "rus_latn_full": {"language_name": "Russian", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "rus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.182252535, "dataset_raw_mb": 31.472203014300938, "dataset_scaled_mb": 26.62054179, "dataset_tokens": 7373824, "train_batch_size": 8, "tokenizer_training_scaled_mb": 26.62054179, "tokenizer_training_raw_mb": 31.472203014300938, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 3.7616123510784e+16, "train_compute_hours": 3.556433495565033, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "ang_latn_full": {"language_name": "Old English", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "ang", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.289057339, "dataset_raw_mb": 7.049783000480736, "dataset_scaled_mb": 5.468944466, "dataset_tokens": 1671168, "train_batch_size": 4, "tokenizer_training_scaled_mb": 5.468944466, "tokenizer_training_raw_mb": 7.049783000480736, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 8528571924480000.0, "train_compute_hours": 0.8063377092235637, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 92.13059%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 7.77618%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.09323%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "cor_latn_full": {"language_name": "Cornish", "language_script": "Latin", "dataset_category": "full", "language_iso6393": "cor", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.215014217, "dataset_raw_mb": 7.667927996819269, "dataset_scaled_mb": 6.310978003, "dataset_tokens": 1936896, "train_batch_size": 4, "tokenizer_training_scaled_mb": 6.310978003, "tokenizer_training_raw_mb": 7.667927996819269, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9881803358208000.0, "train_compute_hours": 0.9342795902305746, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 71.08004%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.11609%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.80387%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Wikipedia 2023/08", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://dumps.wikimedia.org/", "https://tatoeba.org/en/"]}, "isl_latn_1000mb": {"language_name": "Icelandic", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "isl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.154319145, "dataset_raw_mb": 1154.3198364371679, "dataset_scaled_mb": 1000.000599, "dataset_tokens": 236872704, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 115.4319145, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.208963741515776e+18, "train_compute_hours": 114.30202647058246, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 80.12926%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Ndc without informant codes](http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 9.11564%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 8.86357%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 1.85697%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03456%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Ndc without informant codes", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "http://tekstlab.uio.no/nota/scandiasyn/dialect_data_collection.html", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "srp_cyrl_1000mb": {"language_name": "Serbian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "srp", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.424878645, "dataset_raw_mb": 1424.879603943328, "dataset_scaled_mb": 1000.000673, "dataset_tokens": 184423424, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 142.4878645, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.41195847794688e+17, "train_compute_hours": 88.98578924604324, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaa_cyrl_1000mb": {"language_name": "Kara-Kalpak", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "kaa", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.920886208, "dataset_raw_mb": 1920.8897213008745, "dataset_scaled_mb": 1000.001829, "dataset_tokens": 212100608, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 192.0886208, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.082542817673216e+18, "train_compute_hours": 102.34950276183135, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia"], "dataset_readme_str": "* 99.51383%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 0.48617%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Wikipedia Hugging Face"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/legacy-datasets/wikipedia"]}, "uzb_latn_1000mb": {"language_name": "Uzbek", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "uzb", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["uzn", "uzs"], "language_byte_premium": 1.229496706, "dataset_raw_mb": 1229.4989781099125, "dataset_scaled_mb": 1000.001848, "dataset_tokens": 261058560, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 122.94967059999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.332409594281984e+18, "train_compute_hours": 125.97327073211486, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 76.42069%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 15.51914%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.42511%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.63507%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bul_cyrl_1000mb": {"language_name": "Bulgarian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "bul", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.812315686, "dataset_raw_mb": 1812.3176741103075, "dataset_scaled_mb": 1000.001097, "dataset_tokens": 224346112, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 181.2315686, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.14500258758656e+18, "train_compute_hours": 108.25479009909296, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lit_latn_1000mb": {"language_name": "Lithuanian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "lit", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.030036841, "dataset_raw_mb": 1030.0376464888097, "dataset_scaled_mb": 1000.000782, "dataset_tokens": 201228800, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.0036841, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.026942747475968e+18, "train_compute_hours": 97.09276885227335, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kor_hang_1000mb": {"language_name": "Korean", "language_script": "Hangul", "dataset_category": "1000mb", "language_iso6393": "kor", "language_iso15924": "hang", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.293311204, "dataset_raw_mb": 1293.3145109967488, "dataset_scaled_mb": 1000.002557, "dataset_tokens": 227021824, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 129.3311204, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.158590818418688e+18, "train_compute_hours": 109.53949555958506, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hye_armn_1000mb": {"language_name": "Armenian", "language_script": "Armenian", "dataset_category": "1000mb", "language_iso6393": "hye", "language_iso15924": "armn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.724226492, "dataset_raw_mb": 1724.229302489182, "dataset_scaled_mb": 1000.00163, "dataset_tokens": 203630592, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 172.42264920000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.039250386059264e+18, "train_compute_hours": 98.25640013651224, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 52.58958%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 24.27630%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 23.13412%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "pes_arab_1000mb": {"language_name": "Iranian Persian", "language_script": "Arabic", "dataset_category": "1000mb", "language_iso6393": "pes", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["fas", "per"], "language_code_individuals": [], "language_byte_premium": 1.59732627, "dataset_raw_mb": 1597.3291212273919, "dataset_scaled_mb": 1000.001785, "dataset_tokens": 215946240, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 159.73262699999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.102068126056448e+18, "train_compute_hours": 104.19553191806418, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "dan_latn_1000mb": {"language_name": "Danish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "dan", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.021065783, "dataset_raw_mb": 1021.065820779434, "dataset_scaled_mb": 1000.000037, "dataset_tokens": 208085504, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.10657830000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.061968683073536e+18, "train_compute_hours": 100.40431185422523, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hrv_latn_1000mb": {"language_name": "Croatian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "hrv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.989672823, "dataset_raw_mb": 989.6735810893824, "dataset_scaled_mb": 1000.000766, "dataset_tokens": 219422208, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 98.96728230000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.119823964798976e+18, "train_compute_hours": 105.87426576281229, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 58.01986%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 18.11618%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 17.57132%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.27830%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.96930%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.04505%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "pus_arab_1000mb": {"language_name": "Pushto", "language_script": "Arabic", "dataset_category": "1000mb", "language_iso6393": "pus", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pbt", "pbu", "pst"], "language_byte_premium": 1.585665259, "dataset_raw_mb": 1585.6677675224398, "dataset_scaled_mb": 1000.001582, "dataset_tokens": 237871616, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 158.5665259, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.213977935609856e+18, "train_compute_hours": 114.7760957303864, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "sil-ai/bloom-lm"], "dataset_readme_str": "* 60.56138%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.95008%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.68420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.73176%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [BLOOM](https://huggingface.co/datasets/sil-ai/bloom-lm), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [NLLB_seed](https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/)\n* 4.07251%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.00007%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "BLOOM", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "NLLB_seed", "OSCAR", "Tatoeba", "TICO", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/sil-ai/bloom-lm", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://github.com/facebookresearch/flores/blob/main/nllb_seed/README.md", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://dumps.wikimedia.org/"]}, "rus_cyrl_1000mb": {"language_name": "Russian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "rus", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.822744411, "dataset_raw_mb": 1822.7460168378261, "dataset_scaled_mb": 1000.000881, "dataset_tokens": 220467712, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 182.2744411, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.125142825402368e+18, "train_compute_hours": 106.37713985622389, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "srp_latn_1000mb": {"language_name": "Serbian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "srp", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.826257686, "dataset_raw_mb": 826.2583833614871, "dataset_scaled_mb": 1000.000844, "dataset_tokens": 207482368, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 82.6257686, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.058990999076864e+18, "train_compute_hours": 100.12278536726716, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 98.93765%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.75254%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30981%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "hun_latn_1000mb": {"language_name": "Hungarian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "hun", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.01995535, "dataset_raw_mb": 1019.9575806423505, "dataset_scaled_mb": 1000.002187, "dataset_tokens": 191089664, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 101.995535, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.7528296112128e+17, "train_compute_hours": 92.2085708696483, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bos_cyrl_1000mb": {"language_name": "Bosnian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "bos", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 1.148940187, "dataset_raw_mb": 1148.9425618593666, "dataset_scaled_mb": 1000.002067, "dataset_tokens": 232501760, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 114.8940187, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.186590349983744e+18, "train_compute_hours": 112.18672399846308, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "lat_latn_1000mb": {"language_name": "Latin", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "lat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.87538342, "dataset_raw_mb": 875.3854780264204, "dataset_scaled_mb": 1000.002351, "dataset_tokens": 188774912, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 87.538342, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.63370396090368e+17, "train_compute_hours": 91.08229199399844, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.83255%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.81393%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 4.54910%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.52148%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.21200%: [eBible](https://ebible.org/find/)\n* 0.07094%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "Tatoeba", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "zho_hans_1000mb": {"language_name": "Chinese", "language_script": "Han Simplified", "dataset_category": "1000mb", "language_iso6393": "zho", "language_iso15924": "hans", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["cdo", "cjy", "cmn", "cnp", "cpx", "csp", "czh", "czo", "gan", "hak", "hsn", "lzh", "mnp", "nan", "wuu", "yue"], "language_byte_premium": 0.9359663918, "dataset_raw_mb": 935.9689179732915, "dataset_scaled_mb": 1000.002699, "dataset_tokens": 206204416, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 93.59663918, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.052469149958144e+18, "train_compute_hours": 99.5061741778609, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ell_grek_1000mb": {"language_name": "Modern Greek", "language_script": "Greek", "dataset_category": "1000mb", "language_iso6393": "ell", "language_iso15924": "grek", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.967262375, "dataset_raw_mb": 1967.2633763365488, "dataset_scaled_mb": 1000.000509, "dataset_tokens": 238704128, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 196.7262375, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.218293434810368e+18, "train_compute_hours": 115.18410656388934, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hau_latn_1000mb": {"language_name": "Hausa", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "hau", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.176529442, "dataset_raw_mb": 1176.531139731985, "dataset_scaled_mb": 1000.001443, "dataset_tokens": 277416448, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.65294420000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.41575469760512e+18, "train_compute_hours": 133.85317140993862, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 31.25887%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 29.98045%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 27.12632%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [TICO](https://tico-19.github.io/)\n* 8.31691%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 3.07782%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.23963%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "TeDDi", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://github.com/MorphDiv/TeDDi_sample", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "tur_latn_1000mb": {"language_name": "Turkish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "tur", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.044489573, "dataset_raw_mb": 1044.4909172580803, "dataset_scaled_mb": 1000.001287, "dataset_tokens": 186848768, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 104.44895729999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.53573251350528e+17, "train_compute_hours": 90.15601649132266, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fra_latn_1000mb": {"language_name": "French", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "fra", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.173979035, "dataset_raw_mb": 1173.98088401698, "dataset_scaled_mb": 1000.001575, "dataset_tokens": 251415552, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.39790349999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.28314768490496e+18, "train_compute_hours": 121.31578111828713, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nor_latn_1000mb": {"language_name": "Norwegian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "nor", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["nno", "nob"], "language_byte_premium": 1.125315993, "dataset_raw_mb": 1125.3168955034264, "dataset_scaled_mb": 1000.000802, "dataset_tokens": 255482880, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 112.53159930000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.303938169307136e+18, "train_compute_hours": 123.28142691631105, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 99.92843%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.07157%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "tgl_latn_1000mb": {"language_name": "Tagalog", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "tgl", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.117593432, "dataset_raw_mb": 1117.594764171371, "dataset_scaled_mb": 1000.001192, "dataset_tokens": 245370880, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.7593432, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.252232656846848e+18, "train_compute_hours": 118.39290573824746, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 50.29659%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.79155%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.61906%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29280%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TICO", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "deu_latn_1000mb": {"language_name": "German", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "deu", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.053648018, "dataset_raw_mb": 1053.6491317059551, "dataset_scaled_mb": 1000.001057, "dataset_tokens": 210817024, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.36480180000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.075975503740928e+18, "train_compute_hours": 101.72859308096048, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tat_cyrl_1000mb": {"language_name": "Tatar", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "tat", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.854541191, "dataset_raw_mb": 1854.5429880504141, "dataset_scaled_mb": 1000.000969, "dataset_tokens": 232933888, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 185.4541191, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.18882910011392e+18, "train_compute_hours": 112.39838764713427, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 42.95952%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 28.38275%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 12.13842%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.01257%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.69512%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.81162%: [Languages of Russia](http://web-corpora.net/wsgi3/minorlangs/download)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "Languages of Russia"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "http://web-corpora.net/wsgi3/minorlangs/download"]}, "slk_latn_1000mb": {"language_name": "Slovak", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "slk", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.041506091, "dataset_raw_mb": 1041.5074564144852, "dataset_scaled_mb": 1000.001311, "dataset_tokens": 211206144, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 104.1506091, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.07794460049408e+18, "train_compute_hours": 101.9147622285312, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tgk_cyrl_1000mb": {"language_name": "Tajik", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "tgk", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.746052186, "dataset_raw_mb": 1746.0534518878349, "dataset_scaled_mb": 1000.000725, "dataset_tokens": 216990208, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 174.6052186, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.107378886606848e+18, "train_compute_hours": 104.69764018828381, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 74.68005%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.31925%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.47599%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.49619%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.02853%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "nld_latn_1000mb": {"language_name": "Dutch", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "nld", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.051605721, "dataset_raw_mb": 1051.6076254579607, "dataset_scaled_mb": 1000.001811, "dataset_tokens": 216978432, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.16057210000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.107419386871808e+18, "train_compute_hours": 104.70146930424367, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "jpn_jpan_1000mb": {"language_name": "Japanese", "language_script": "Japanese", "dataset_category": "1000mb", "language_iso6393": "jpn", "language_iso15924": "jpan", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.321973899, "dataset_raw_mb": 1321.9753703569497, "dataset_scaled_mb": 1000.001113, "dataset_tokens": 219063296, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 132.19738990000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.117992307654656e+18, "train_compute_hours": 105.70109090553112, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fas_arab_1000mb": {"language_name": "Persian", "language_script": "Arabic", "dataset_category": "1000mb", "language_iso6393": "fas", "language_iso15924": "arab", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["pes", "prs"], "language_byte_premium": 1.590775443, "dataset_raw_mb": 1590.7765120010977, "dataset_scaled_mb": 1000.000672, "dataset_tokens": 244359680, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 159.0775443, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.247088861904896e+18, "train_compute_hours": 117.90658330737199, "dataset_hugging_face": ["cis-lmu/Glot500"], "dataset_readme_str": "* 100.00000%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["Glot500", "CCNet", "TICO", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "vie_latn_1000mb": {"language_name": "Vietnamese", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "vie", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.349331266, "dataset_raw_mb": 1349.3315722981974, "dataset_scaled_mb": 1000.000227, "dataset_tokens": 262306304, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 134.9331266, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.338627560767488e+18, "train_compute_hours": 126.56115119983524, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kan_knda_1000mb": {"language_name": "Kannada", "language_script": "Kannada", "dataset_category": "1000mb", "language_iso6393": "kan", "language_iso15924": "knda", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.641985282, "dataset_raw_mb": 2641.989818288729, "dataset_scaled_mb": 1000.001717, "dataset_tokens": 212683264, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 264.1985282, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.08548287561728e+18, "train_compute_hours": 102.62747187654284, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 49.18479%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TeDDi](https://github.com/MorphDiv/TeDDi_sample), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 40.78353%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 3.59325%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.16622%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 3.00209%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.27013%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "TeDDi", "W2C", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://github.com/MorphDiv/TeDDi_sample", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "kat_geor_1000mb": {"language_name": "Georgian", "language_script": "Georgian", "dataset_category": "1000mb", "language_iso6393": "kat", "language_iso15924": "geor", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 4.338956454, "dataset_raw_mb": 4338.963674023539, "dataset_scaled_mb": 1000.001664, "dataset_tokens": 354762752, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 433.8956454, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.81056565149696e+18, "train_compute_hours": 171.18075250516713, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500", "allenai/nllb"], "dataset_readme_str": "* 60.04062%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.58420%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 7.78020%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 7.59499%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb"]}, "afr_latn_1000mb": {"language_name": "Afrikaans", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "afr", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.03722598, "dataset_raw_mb": 1037.2278345600523, "dataset_scaled_mb": 1000.001788, "dataset_tokens": 239682048, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.722598, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.22326738993152e+18, "train_compute_hours": 115.65437141170736, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 66.06252%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 16.34080%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Mburisano_Covid](https://repo.sadilar.org/handle/20.500.12185/536), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 13.07209%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 2.39061%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 2.13398%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfroMAFT", "CCNet", "Earthlings", "Mburisano_Covid", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://repo.sadilar.org/handle/20.500.12185/536", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "som_latn_1000mb": {"language_name": "Somali", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "som", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.42230286, "dataset_raw_mb": 1422.3035085701042, "dataset_scaled_mb": 1000.000456, "dataset_tokens": 302652928, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 142.230286, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.544580292018176e+18, "train_compute_hours": 146.03304579080938, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "castorini/afriberta-corpus"], "dataset_readme_str": "* 38.68291%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 30.77395%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 22.30160%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [TICO](https://tico-19.github.io/)\n* 7.64665%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 0.41754%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.17735%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "Wortschatz Leipzig Data", "TICO", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://wortschatz.uni-leipzig.de/en/download", "https://tico-19.github.io/", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "sin_sinh_1000mb": {"language_name": "Sinhala", "language_script": "Sinhala", "dataset_category": "1000mb", "language_iso6393": "sin", "language_iso15924": "sinh", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.446318518, "dataset_raw_mb": 2446.3232711968803, "dataset_scaled_mb": 1000.001943, "dataset_tokens": 233098752, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 244.6318518, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.189687183147008e+18, "train_compute_hours": 112.47951549753532, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 53.49015%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.08102%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.71980%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 8.70904%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "amh_ethi_1000mb": {"language_name": "Amharic", "language_script": "Ge'ez", "dataset_category": "1000mb", "language_iso6393": "amh", "language_iso15924": "ethi", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.720917143, "dataset_raw_mb": 1720.9187004300145, "dataset_scaled_mb": 1000.000905, "dataset_tokens": 211767808, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 172.0917143, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.080827696775168e+18, "train_compute_hours": 102.18734587692498, "dataset_hugging_face": ["cis-lmu/Glot500", "castorini/afriberta-corpus", "allenai/MADLAD-400", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 35.99036%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [HornMT](https://github.com/asmelashteka/HornMT), [OSCAR](https://oscar-project.org/), [Parallel Corpora for Ethiopian Languages](https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 33.89884%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.09942%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 6.48807%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.48971%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.03361%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "HornMT", "OSCAR", "Parallel Corpora for Ethiopian Languages", "TICO", "W2C", "MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://github.com/asmelashteka/HornMT", "https://oscar-project.org/", "https://github.com/AAUThematic4LT/Parallel-Corpora-for-Ethiopian-Languages", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "pol_latn_1000mb": {"language_name": "Polish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "pol", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.077373477, "dataset_raw_mb": 1077.3759301794073, "dataset_scaled_mb": 1000.002277, "dataset_tokens": 216235008, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 107.73734770000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.103508890320896e+18, "train_compute_hours": 104.33174963033926, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ukr_cyrl_1000mb": {"language_name": "Ukrainian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "ukr", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.751370357, "dataset_raw_mb": 1751.3748825410025, "dataset_scaled_mb": 1000.002584, "dataset_tokens": 215392768, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 175.1370357, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.099260281880576e+18, "train_compute_hours": 103.93006301416357, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tha_thai_1000mb": {"language_name": "Thai", "language_script": "Thai", "dataset_category": "1000mb", "language_iso6393": "tha", "language_iso15924": "thai", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.741647235, "dataset_raw_mb": 2741.6488717633993, "dataset_scaled_mb": 1000.000597, "dataset_tokens": 205872640, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 274.1647235, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.050759254900736e+18, "train_compute_hours": 99.34451137243323, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 99.89966%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.09995%: [eBible](https://ebible.org/find/)\n* 0.00039%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["OSCAR 2021/09", "eBible", "Tatoeba"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/", "https://tatoeba.org/en/"]}, "mlt_latn_1000mb": {"language_name": "Maltese", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "mlt", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.08846667, "dataset_raw_mb": 1088.4678390132037, "dataset_scaled_mb": 1000.001074, "dataset_tokens": 283158528, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 108.84666700000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.445110073524224e+18, "train_compute_hours": 136.628588769563, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 94.92627%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MaCoCu](https://macocu.eu/), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 4.23828%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 0.42133%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.41412%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "Wortschatz Leipzig Data", "MaCoCu", "MC4", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://wortschatz.uni-leipzig.de/en/download", "https://macocu.eu/", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "cym_latn_1000mb": {"language_name": "Welsh", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "cym", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.026556848, "dataset_raw_mb": 1026.5579505220549, "dataset_scaled_mb": 1000.001074, "dataset_tokens": 236230144, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.6556848, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.205584190373888e+18, "train_compute_hours": 113.98250527171305, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 68.76968%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 9.88536%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 7.87431%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 7.28932%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 6.18133%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "eus_latn_1000mb": {"language_name": "Basque", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "eus", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059583723, "dataset_raw_mb": 1059.584048292203, "dataset_scaled_mb": 1000.000307, "dataset_tokens": 209921536, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.95837230000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.071338615341056e+18, "train_compute_hours": 101.29019635951803, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 69.69288%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 22.18968%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 8.11744%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ron_latn_1000mb": {"language_name": "Romanian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "ron", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.115120928, "dataset_raw_mb": 1115.1224746727273, "dataset_scaled_mb": 1000.001387, "dataset_tokens": 230580224, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 111.5120928, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.176850950782976e+18, "train_compute_hours": 111.26590807402683, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "urd_arab_1000mb": {"language_name": "Urdu", "language_script": "Arabic", "dataset_category": "1000mb", "language_iso6393": "urd", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.707950617, "dataset_raw_mb": 1707.9509927491358, "dataset_scaled_mb": 1000.00022, "dataset_tokens": 247899648, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 170.79506170000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.265154592997376e+18, "train_compute_hours": 119.61461606520646, "dataset_hugging_face": ["allenai/nllb", "oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 37.89138%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 35.70994%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 26.24866%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Workshop on NER for South and South East Asian Languages](https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5)\n* 0.15002%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Glot500", "Anuvaad", "CCNet", "Earthlings", "OSCAR", "TICO", "W2C", "Workshop on NER for South and South East Asian Languages", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://ltrc.iiit.ac.in/ner-ssea-08/index.cgi?topic=5", "https://ebible.org/find/"]}, "bel_cyrl_1000mb": {"language_name": "Belarusian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "bel", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.007901403, "dataset_raw_mb": 2007.9021378919135, "dataset_scaled_mb": 1000.000366, "dataset_tokens": 254138368, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 200.7901403, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.297059917856768e+18, "train_compute_hours": 122.63111950645808, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.85869%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 19.04732%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 14.98527%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.10871%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "fin_latn_1000mb": {"language_name": "Finnish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "fin", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.05886439, "dataset_raw_mb": 1058.8647658968587, "dataset_scaled_mb": 1000.000355, "dataset_tokens": 186050560, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.88643900000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.49616767401984e+17, "train_compute_hours": 89.78194891800577, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "msa_latn_1000mb": {"language_name": "Malay", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "msa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["bjn", "btj", "bve", "bvu", "coa", "dup", "hji", "ind", "jak", "jax", "kvb", "kvr", "kxd", "lce", "lcf", "liw", "max", "meo", "mfa", "mfb", "min", "mqg", "msi", "mui", "orn", "ors", "pel", "pse", "tmw", "urk", "vkk", "vkt", "xmm", "zlm", "zmi", "zsm"], "language_byte_premium": 1.285705434, "dataset_raw_mb": 1285.7065127068593, "dataset_scaled_mb": 1000.000839, "dataset_tokens": 236371456, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.5705434, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.206355524452352e+18, "train_compute_hours": 114.05543140276784, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 82.84912%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.03127%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 5.11781%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 1.00181%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "OSCAR", "TICO", "W2C", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ces_latn_1000mb": {"language_name": "Czech", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "ces", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.035849214, "dataset_raw_mb": 1035.850137977499, "dataset_scaled_mb": 1000.000892, "dataset_tokens": 206113280, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 103.5849214, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05186896216064e+18, "train_compute_hours": 99.44942914973325, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "aze_latn_1000mb": {"language_name": "Azerbaijani", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "aze", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["azb", "azj"], "language_byte_premium": 1.29878361, "dataset_raw_mb": 1298.7862387380267, "dataset_scaled_mb": 1000.002024, "dataset_tokens": 233091584, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 129.878361, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.189600434192384e+18, "train_compute_hours": 112.47131377818904, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.37652%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 18.62348%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "MC4", "OSCAR", "TIL", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "XLSum", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "tam_taml_1000mb": {"language_name": "Tamil", "language_script": "Tamil", "dataset_category": "1000mb", "language_iso6393": "tam", "language_iso15924": "taml", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.729099655, "dataset_raw_mb": 2729.1003427331134, "dataset_scaled_mb": 1000.000252, "dataset_tokens": 200523264, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 272.9099655, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.023409295327232e+18, "train_compute_hours": 96.75869701275649, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 36.08815%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 33.39717%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 19.80765%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 10.41716%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.28987%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "AI4Bharat", "Anuvaad", "CCNet", "OSCAR", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "nob_latn_1000mb": {"language_name": "Norwegian Bokm\u00e5l", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "nob", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["nor"], "language_code_individuals": [], "language_byte_premium": 0.9976190383, "dataset_raw_mb": 997.6203581499876, "dataset_scaled_mb": 1000.001323, "dataset_tokens": 205949952, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.76190383, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.051035179286528e+18, "train_compute_hours": 99.37059876890811, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mar_deva_1000mb": {"language_name": "Marathi", "language_script": "Devanagari", "dataset_category": "1000mb", "language_iso6393": "mar", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.479356527, "dataset_raw_mb": 2479.3579154396552, "dataset_scaled_mb": 1000.00056, "dataset_tokens": 206630400, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 247.93565270000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.054643099664384e+18, "train_compute_hours": 99.71171124099631, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/MADLAD-400", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 48.53905%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Hindialect](https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 40.24253%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 11.13450%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.08391%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Hindialect", "Indiccorp", "OSCAR", "TICO", "W2C", "WikiMatrix", "MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11234/1-4839", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "mal_mlym_1000mb": {"language_name": "Malayalam", "language_script": "Malayalam", "dataset_category": "1000mb", "language_iso6393": "mal", "language_iso15924": "mlym", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.884868527, "dataset_raw_mb": 2884.873480319261, "dataset_scaled_mb": 1000.001717, "dataset_tokens": 244708864, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 288.4868527, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.248822011953152e+18, "train_compute_hours": 118.07044476647984, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 79.43378%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 20.26204%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.30417%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "cat_latn_1000mb": {"language_name": "Catalan", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "cat", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.092622079, "dataset_raw_mb": 1092.6231956597649, "dataset_scaled_mb": 1000.001022, "dataset_tokens": 238915072, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.2622079, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.219286344531968e+18, "train_compute_hours": 115.27798166484062, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "sqi_latn_1000mb": {"language_name": "Albanian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "sqi", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["aae", "aat", "aln", "als"], "language_byte_premium": 1.335619892, "dataset_raw_mb": 1335.619941417936, "dataset_scaled_mb": 1000.000037, "dataset_tokens": 274664448, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 133.5619892, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.401709989593088e+18, "train_compute_hours": 132.5253081069829, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109", "cis-lmu/Glot500"], "dataset_readme_str": "* 62.65815%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 37.34185%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n", "dataset_names": ["OSCAR 2021/09", "Glot500", "CCNet", "OSCAR", "W2C", "WikiMatrix"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix"]}, "pan_guru_1000mb": {"language_name": "Panjabi", "language_script": "Gurmukhi", "dataset_category": "1000mb", "language_iso6393": "pan", "language_iso15924": "guru", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.220967698, "dataset_raw_mb": 2220.9708961934853, "dataset_scaled_mb": 1000.00144, "dataset_tokens": 215775232, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 222.0967698, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.1011630104576e+18, "train_compute_hours": 104.10995735235491, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.26136%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [OSCAR](https://oscar-project.org/), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia)\n* 11.38636%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.73324%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 2.48906%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.12998%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "OSCAR", "Wikipedia Hugging Face", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://oscar-project.org/", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "epo_latn_1000mb": {"language_name": "Esperanto", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "epo", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9954937917, "dataset_raw_mb": 995.4945711716388, "dataset_scaled_mb": 1000.000783, "dataset_tokens": 231384576, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 99.54937917, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.180872235155456e+18, "train_compute_hours": 111.64610223287949, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/nllb", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 38.12373%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 33.22117%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 14.49198%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 14.01395%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.14917%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "OSCAR 2021/09", "Wikipedia 2023/08", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/", "https://ebible.org/find/"]}, "mon_cyrl_1000mb": {"language_name": "Mongolian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "mon", "language_iso15924": "cyrl", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["khk", "mvf"], "language_byte_premium": 1.783772557, "dataset_raw_mb": 1783.7747332025197, "dataset_scaled_mb": 1000.00122, "dataset_tokens": 205737472, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 178.3772557, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.050052721246208e+18, "train_compute_hours": 99.27771182691421, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 63.59654%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 24.77330%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9)\n* 11.63016%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "swe_latn_1000mb": {"language_name": "Swedish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "swe", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.020981292, "dataset_raw_mb": 1020.9813665316344, "dataset_scaled_mb": 1000.000073, "dataset_tokens": 206359552, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 102.09812920000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.05321069674496e+18, "train_compute_hours": 99.57628405588713, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "fil_latn_1000mb": {"language_name": "Filipino", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "fil", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.328161865, "dataset_raw_mb": 1328.1638452893408, "dataset_scaled_mb": 1000.001491, "dataset_tokens": 274955776, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 132.8161865, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.403181847609344e+18, "train_compute_hours": 132.66446559215618, "dataset_hugging_face": ["allenai/MADLAD-400"], "dataset_readme_str": "* 100.00000%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400"]}, "kir_cyrl_1000mb": {"language_name": "Kirghiz", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "kir", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.963770708, "dataset_raw_mb": 1963.7723222195218, "dataset_scaled_mb": 1000.000822, "dataset_tokens": 223066112, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 196.37707079999998, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.138453564096512e+18, "train_compute_hours": 107.63560969639751, "dataset_hugging_face": ["allenai/MADLAD-400", "allenai/nllb", "cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 65.41839%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 13.62164%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 9.53594%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt)\n* 8.32078%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 3.10325%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "OSCAR 2021/09", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://dumps.wikimedia.org/"]}, "eng_latn_1000mb": {"language_name": "English", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "eng", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.0, "dataset_raw_mb": 1000.002308, "dataset_scaled_mb": 1000.002308, "dataset_tokens": 213977088, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 100.0, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.092001066647552e+18, "train_compute_hours": 103.24373721031401, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "kaz_cyrl_1000mb": {"language_name": "Kazakh", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "kaz", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.764638392, "dataset_raw_mb": 1764.641061897887, "dataset_scaled_mb": 1000.001513, "dataset_tokens": 199970304, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 176.4638392, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.020620786761728e+18, "train_compute_hours": 96.49505620292702, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109", "allenai/nllb"], "dataset_readme_str": "* 68.35707%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [TIL](https://github.com/turkic-interlingua/til-mt), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 26.84524%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 4.79769%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "TIL", "W2C", "WikiMatrix", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://github.com/turkic-interlingua/til-mt", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb"]}, "heb_hebr_1000mb": {"language_name": "Hebrew", "language_script": "Hebrew", "dataset_category": "1000mb", "language_iso6393": "heb", "language_iso15924": "hebr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.355477183, "dataset_raw_mb": 1355.480640822294, "dataset_scaled_mb": 1000.002551, "dataset_tokens": 192904704, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 135.5477183, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.84445688807424e+17, "train_compute_hours": 93.074865123611, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ind_latn_1000mb": {"language_name": "Indonesian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "ind", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["may", "msa"], "language_code_individuals": [], "language_byte_premium": 1.178746238, "dataset_raw_mb": 1178.7487805556354, "dataset_scaled_mb": 1000.002157, "dataset_tokens": 210432000, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 117.8746238, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.074044033040384e+18, "train_compute_hours": 101.54598130563632, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "lav_latn_1000mb": {"language_name": "Latvian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "lav", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ltg", "lvs"], "language_byte_premium": 1.288843709, "dataset_raw_mb": 1288.8455842675965, "dataset_scaled_mb": 1000.001455, "dataset_tokens": 243401728, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 128.8843709, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.242166120022016e+18, "train_compute_hours": 117.44116043844517, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "guj_gujr_1000mb": {"language_name": "Gujarati", "language_script": "Gujarati", "dataset_category": "1000mb", "language_iso6393": "guj", "language_iso15924": "gujr", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.162862919, "dataset_raw_mb": 2162.865241914775, "dataset_scaled_mb": 1000.001074, "dataset_tokens": 193794560, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 216.2862919, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.89020128411648e+17, "train_compute_hours": 93.5073575952831, "dataset_hugging_face": ["cis-lmu/Glot500", "allenai/c4", "legacy-datasets/wikipedia", "csebuetnlp/xlsum", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 86.12642%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [MC4](https://huggingface.co/datasets/allenai/c4), [OSCAR](https://oscar-project.org/), [Tatoeba](https://tatoeba.org/en/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [XLSum](https://huggingface.co/datasets/csebuetnlp/xlsum)\n* 13.58996%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.28351%: [eBible](https://ebible.org/find/)\n* 0.00012%: [Tatoeba](https://tatoeba.org/en/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "MC4", "OSCAR", "Tatoeba", "W2C", "Wikipedia Hugging Face", "XLSum", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://huggingface.co/datasets/allenai/c4", "https://oscar-project.org/", "https://tatoeba.org/en/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://huggingface.co/datasets/csebuetnlp/xlsum", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "arb_arab_1000mb": {"language_name": "Standard Arabic", "language_script": "Arabic", "dataset_category": "1000mb", "language_iso6393": "arb", "language_iso15924": "arab", "language_code_type": "individual", "language_code_macrolangs": ["ara"], "language_code_individuals": [], "language_byte_premium": 1.465017978, "dataset_raw_mb": 1465.020654587846, "dataset_scaled_mb": 1000.001827, "dataset_tokens": 196197376, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 146.50179780000002, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.001332731543552e+18, "train_compute_hours": 94.67145825502675, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "mkd_cyrl_1000mb": {"language_name": "Macedonian", "language_script": "Cyrillic", "dataset_category": "1000mb", "language_iso6393": "mkd", "language_iso15924": "cyrl", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.834986754, "dataset_raw_mb": 1834.9892440770252, "dataset_scaled_mb": 1000.001357, "dataset_tokens": 221346304, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 183.4986754, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.129593935167488e+18, "train_compute_hours": 106.79797205219887, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 72.44064%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 27.55936%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "bos_latn_1000mb": {"language_name": "Bosnian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "bos", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": ["hbs"], "language_code_individuals": [], "language_byte_premium": 0.9699882939, "dataset_raw_mb": 969.989776042113, "dataset_scaled_mb": 1000.001528, "dataset_tokens": 228266496, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.99882939, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.164942827716608e+18, "train_compute_hours": 110.14004916593386, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "allenai/nllb"], "dataset_readme_str": "* 70.89860%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 17.71791%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 11.38348%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n", "dataset_names": ["Glot500", "CCNet", "Wortschatz Leipzig Data", "OSCAR", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "NLLB (CommonCrawl and ParaCrawl)", "Wikipedia 2023/08"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/allenai/nllb", "https://dumps.wikimedia.org/"]}, "tel_telu_1000mb": {"language_name": "Telugu", "language_script": "Telugu", "dataset_category": "1000mb", "language_iso6393": "tel", "language_iso15924": "telu", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.619819498, "dataset_raw_mb": 2619.825059876794, "dataset_scaled_mb": 1000.002123, "dataset_tokens": 209365504, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 261.9819498, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.068517706563584e+18, "train_compute_hours": 101.02349225692068, "dataset_hugging_face": ["cis-lmu/Glot500", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 81.10762%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AI4Bharat](https://ai4bharat.org/), [Anuvaad](https://github.com/project-anuvaad/anuvaad-parallel-corpus), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Indiccorp](https://ai4bharat.iitm.ac.in/corpora), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 18.59426%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 0.29812%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["Glot500", "AI4Bharat", "Anuvaad", "CCNet", "Earthlings", "Indiccorp", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "OSCAR 2021/09", "eBible"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://ai4bharat.org/", "https://github.com/project-anuvaad/anuvaad-parallel-corpus", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://ai4bharat.iitm.ac.in/corpora", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://ebible.org/find/"]}, "ben_beng_1000mb": {"language_name": "Bengali", "language_script": "Bengali", "dataset_category": "1000mb", "language_iso6393": "ben", "language_iso15924": "beng", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.430759774, "dataset_raw_mb": 2430.761614085149, "dataset_scaled_mb": 1000.000757, "dataset_tokens": 194737152, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 243.07597740000003, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.93798637092864e+17, "train_compute_hours": 93.95914387059806, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "hin_deva_1000mb": {"language_name": "Hindi", "language_script": "Devanagari", "dataset_category": "1000mb", "language_iso6393": "hin", "language_iso15924": "deva", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 2.370151128, "dataset_raw_mb": 2370.156273598099, "dataset_scaled_mb": 1000.002171, "dataset_tokens": 228020736, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 237.0151128, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.163772239413248e+18, "train_compute_hours": 110.0293753627071, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "nep_deva_1000mb": {"language_name": "Nepali", "language_script": "Devanagari", "dataset_category": "1000mb", "language_iso6393": "nep", "language_iso15924": "deva", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["dty", "npi"], "language_byte_premium": 2.629966681, "dataset_raw_mb": 2629.970218305186, "dataset_scaled_mb": 1000.001345, "dataset_tokens": 215368192, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 262.9966681, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.09918502977536e+18, "train_compute_hours": 103.92294826967041, "dataset_hugging_face": ["allenai/MADLAD-400", "oscar-corpus/OSCAR-2109", "allenai/nllb", "cis-lmu/Glot500"], "dataset_readme_str": "* 53.66687%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 23.99086%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n* 16.12038%: [NLLB (CommonCrawl and ParaCrawl)](https://huggingface.co/datasets/allenai/nllb)\n* 5.97982%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Tatoeba](https://tatoeba.org/en/), [TICO](https://tico-19.github.io/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 0.24207%: [eBible](https://ebible.org/find/)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "OSCAR 2021/09", "NLLB (CommonCrawl and ParaCrawl)", "Glot500", "CCNet", "Earthlings", "Tatoeba", "TICO", "W2C", "WikiMatrix", "eBible"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109", "https://huggingface.co/datasets/allenai/nllb", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://tatoeba.org/en/", "https://tico-19.github.io/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://ebible.org/find/"]}, "slv_latn_1000mb": {"language_name": "Slovenian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "slv", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 0.9721500484, "dataset_raw_mb": 972.1524875244714, "dataset_scaled_mb": 1000.002509, "dataset_tokens": 198052864, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 97.21500483999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.010733496270848e+18, "train_compute_hours": 95.56025782924382, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "swa_latn_1000mb": {"language_name": "Swahili", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "swa", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["swc", "swh"], "language_byte_premium": 1.258733422, "dataset_raw_mb": 1258.7347738796952, "dataset_scaled_mb": 1000.001074, "dataset_tokens": 260033024, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 125.8733422, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.32712574681088e+18, "train_compute_hours": 125.47370697121049, "dataset_hugging_face": ["allenai/MADLAD-400", "cis-lmu/Glot500", "castorini/afriberta-corpus", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 57.90197%: [MADLAD-400 (CommonCrawl)](https://huggingface.co/datasets/allenai/MADLAD-400)\n* 35.61041%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus), [AfroMAFT](https://zenodo.org/record/6990611#.Y0-yU-xBw-Q), [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [Wortschatz Leipzig Data](https://wortschatz.uni-leipzig.de/en/download), [OSCAR](https://oscar-project.org/), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 5.07304%: [AfriBERTa](https://huggingface.co/datasets/castorini/afriberta-corpus)\n* 1.23078%: [Wikipedia 2023/08](https://dumps.wikimedia.org/)\n* 0.18380%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["MADLAD-400 (CommonCrawl)", "Glot500", "AfriBERTa", "AfroMAFT", "CCNet", "Earthlings", "Wortschatz Leipzig Data", "OSCAR", "W2C", "WikiMatrix", "Wikipedia 2023/08", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/allenai/MADLAD-400", "https://huggingface.co/datasets/cis-lmu/Glot500", "https://huggingface.co/datasets/castorini/afriberta-corpus", "https://zenodo.org/record/6990611#.Y0-yU-xBw-Q", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://wortschatz.uni-leipzig.de/en/download", "https://oscar-project.org/", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://dumps.wikimedia.org/", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "por_latn_1000mb": {"language_name": "Portuguese", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "por", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.097878434, "dataset_raw_mb": 1097.8807109998722, "dataset_scaled_mb": 1000.002074, "dataset_tokens": 225242112, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 109.78784340000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.149625366216704e+18, "train_compute_hours": 108.69185280594293, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "est_latn_1000mb": {"language_name": "Estonian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "est", "language_iso15924": "latn", "language_code_type": "macrolanguage", "language_code_macrolangs": [], "language_code_individuals": ["ekk", "vro"], "language_byte_premium": 0.9677856419, "dataset_raw_mb": 967.7871303543172, "dataset_scaled_mb": 1000.001538, "dataset_tokens": 189518336, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 96.77856419000001, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 9.67280631349248e+17, "train_compute_hours": 91.4519869639289, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "glg_latn_1000mb": {"language_name": "Galician", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "glg", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.059078607, "dataset_raw_mb": 1059.081016403831, "dataset_scaled_mb": 1000.002275, "dataset_tokens": 222080000, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 105.9078607, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.133404356870144e+18, "train_compute_hours": 107.15823010408636, "dataset_hugging_face": ["cis-lmu/Glot500", "legacy-datasets/wikipedia", "oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 80.77003%: [Glot500](https://huggingface.co/datasets/cis-lmu/Glot500), including [CCNet](https://github.com/facebookresearch/cc_net), [Earthlings](https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/), [OSCAR](https://oscar-project.org/), [SLI_GalWeb.1.0](https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz), [W2C](https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9), [Wikipedia Hugging Face](https://huggingface.co/datasets/legacy-datasets/wikipedia), [WikiMatrix](https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix)\n* 19.22997%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["Glot500", "CCNet", "Earthlings", "OSCAR", "SLI_GalWeb.1.0", "W2C", "Wikipedia Hugging Face", "WikiMatrix", "OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/cis-lmu/Glot500", "https://github.com/facebookresearch/cc_net", "https://publicdata.canterbury.ac.nz/Research/Geocorpus/CCGLU_v5.0/", "https://oscar-project.org/", "https://ilg.usc.gal/download/SLI_Galician_Corpora/SLI_GalWeb.1.0.tar.gz", "https://lindat.mff.cuni.cz/repository/xmlui/handle/11858/00-097C-0000-0022-6133-9", "https://huggingface.co/datasets/legacy-datasets/wikipedia", "https://github.com/facebookresearch/LASER/tree/main/tasks/WikiMatrix", "https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "spa_latn_1000mb": {"language_name": "Spanish", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "spa", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.083831966, "dataset_raw_mb": 1083.8335202150392, "dataset_scaled_mb": 1000.001434, "dataset_tokens": 221790720, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 108.38319659999999, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.13196150226944e+18, "train_compute_hours": 107.0218147600198, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}, "ita_latn_1000mb": {"language_name": "Italian", "language_script": "Latin", "dataset_category": "1000mb", "language_iso6393": "ita", "language_iso15924": "latn", "language_code_type": "individual", "language_code_macrolangs": [], "language_code_individuals": [], "language_byte_premium": 1.066886932, "dataset_raw_mb": 1066.8887734468447, "dataset_scaled_mb": 1000.001726, "dataset_tokens": 216099840, "train_batch_size": 64, "tokenizer_training_scaled_mb": 100.0, "tokenizer_training_raw_mb": 106.6886932, "train_epochs": 10, "train_learning_rate": 0.0001, "model_sequence_length": 512, "model_architecture": "gpt2", "tokenizer_vocab_size": "50000", "model_parameters": "124770816", "train_compute_flops": 1.102969060982784e+18, "train_compute_hours": 104.2807112201905, "dataset_hugging_face": ["oscar-corpus/OSCAR-2109"], "dataset_readme_str": "* 100.00000%: [OSCAR 2021/09](https://huggingface.co/datasets/oscar-corpus/OSCAR-2109)\n", "dataset_names": ["OSCAR 2021/09"], "dataset_links": ["https://huggingface.co/datasets/oscar-corpus/OSCAR-2109"]}}
